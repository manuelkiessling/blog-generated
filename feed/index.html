<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>The Log Book of Manuel Kiessling</title>
	<atom:link href="/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>Writings about Software Development and System Operation topics</description>
	<lastBuildDate>Sun, 25 Mar 2012 08:20:20 +0000</lastBuildDate>
	<generator>http://wordpress.org/?v=2.9.2</generator>
	<language>de</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
			<item>
		<title>Talk: PHP 5.4 &#8211; die wichtigsten Neuerungen im Überblick</title>
		<link>/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/</link>
		<comments>/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/#comments</comments>
		<pubDate>Sat, 24 Mar 2012 21:42:30 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Deutsche Artikel]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=600</guid>
		<description><![CDATA[Am 14. März 2012 fand das erste Treffen der Symfony User Group Berlin statt. Ich habe dort einen kurzen Talk mit dem Titel <em>PHP 5.4 - die wichtigsten Neuerungen im Überblick</em> gehalten. Folien und Videomitschnitt des Talks nach dem Klick.]]></description>
			<content:encoded><![CDATA[<p>
  Am 14. März 2012 fand das erste Treffen der Symfony User Group Berlin statt. Ich habe dort einen kurzen Talk mit dem Titel <em>PHP 5.4 &#8211; die wichtigsten Neuerungen im Überblick</em> gehalten. Die Folien des Talks gibt es <a href="/wp-content/uploads/2012/03/Manuel-Kiessling-PHP-5.4-Die-wichtigsten-Neuerungen-im-Überblick.pdf" title="Präsentation 'PHP 5.4 - die wichtigsten Neuerungen im Überblick' von Manuel Kiessling als PDF-Download">hier zum Download als PDF</a> und <a href="http://www.slideshare.net/manuelkiessling/php-54-die-wichtigsten-neuerungen" title="Präsentation 'PHP 5.4 - die wichtigsten Neuerungen im Überblick' von Manuel Kiessling online bei SlideShare">hier bei Slideshare</a>. Einen Videomitschnitt gibt es <a href="http://www.ustream.tv/recorded/21104071/highlight/251550" title="Videomitschnitt der Präsentation 'PHP 5.4 - die wichtigsten Neuerungen im Überblick' von Manuel Kiessling">bei Ustream</a>.
</p>

<div style="width:510px" id="__ss_12011306"> <strong style="display:block;margin:12px 0 4px"><a href="http://www.slideshare.net/manuelkiessling/php-54-die-wichtigsten-neuerungen" title="PHP 5.4: Die wichtigsten Neuerungen im Überblick" target="_blank">PHP 5.4: Die wichtigsten Neuerungen im Überblick</a></strong> <iframe src="http://www.slideshare.net/slideshow/embed_code/12011306?rel=0" width="510" height="426" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> <div style="padding:5px 0 12px"> View more <a href="http://www.slideshare.net/thecroaker/death-by-powerpoint" target="_blank">PowerPoint</a> from <a href="http://www.slideshare.net/manuelkiessling" target="_blank">Manuel Kiessling</a> </div> </div>

<iframe src="http://www.ustream.tv/embed/recorded/21104071/highlight/251550" width="608" height="368" scrolling="no" frameborder="0" style="border: 0px none transparent;"></iframe>]]></content:encoded>
			<wfw:commentRss>/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Object-orientation and inheritance in JavaScript: a comprehensive explanation</title>
		<link>/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/</link>
		<comments>/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/#comments</comments>
		<pubDate>Fri, 23 Mar 2012 09:43:26 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Other]]></category>

		<guid isPermaLink="false">/?p=568</guid>
		<description><![CDATA[<p>
Let’s talk about object-orientation and inheritance in JavaScript.
</p>
<p>
The good news is that it’s actually quite simple, but the bad news is that it works completely different than object-orientation in languages like C++, Java, Ruby, Python or PHP, making it not-quite-so simple to understand.
</p>
<p>
But fear not, we are going to take it step by step.
</p>]]></description>
			<content:encoded><![CDATA[<p>Let&#8217;s talk about object-orientation and inheritance in JavaScript.</p>
<p>The good news is that it&#8217;s actually quite simple, but the bad news is that it works completely different than object-orientation in languages like C++, Java, Ruby, Python or PHP, making it not-quite-so simple to understand.</p>
<p>But fear not, we are going to take it step by step.</p>
<h2 id="blueprints-versus-finger-pointing">Blueprints versus finger-pointing</h2>
<p>Let&#8217;s start by looking at how &quot;typical&quot; object-oriented languages actually create objects.</p>
<p>We are going to talk about an object called <em>myCar</em>. myCar is our bits-and-bytes representation of an incredibly simplified real world car. It could have attributes like <em>color</em> and <em>weight</em>, and methods like <em>drive</em> and <em>honk</em>.</p>
<p>In a &quot;real&quot; application, <em>myCar</em> could be used to represent the car in a game which is driven by the player of that game &#8211; but we are going to completely ignore the context of this object, because we are going to talk about the nature and usage of this object in a more abstract way.</p>
<p>If you would want to use this <em>myCar</em> object in, say, Java, you need to define the blueprint of this specific object first &#8211; this is what Java and most other object-oriented languages call a <em>class</em>.</p>
<p>If you want to create the object <em>myCar</em>, you tell Java to &quot;build a new object after the specification that is laid out in the class <em>Car</em>&quot;.</p>
<p>The newly built object shares certain aspects with its blueprint. If you call the method <em>honk</em> on your object, like so:</p>
<code>myCar.honk();</code>
<p>the Java interpreter will go to the class of <em>myCar</em> and look up which code it actually needs to execute, which is defined in the <em>honk</em> method of class <em>Car</em>.</p>
<p>Ok, nothing shockingly new here. Enter JavaScript.</p>
<h2 id="a-classless-society">A classless society</h2>
<p>JavaScript does not have classes. But as in other languages, we would like to tell the interpreter that it should built our <em>myCar</em> object following a certain pattern or schema or blueprint &#8211; it would be quite tedious to create every car object from scratch, &quot;manually&quot; giving it the attributes and methods it needs every time we build it.</p>
<p>If we were to create 30 car objects based on the <em>Car</em> class in Java, this object-class relationship provides us with 30 cars that are able to <em>drive</em> and <em>honk</em> without us having to write 30 <em>drive</em> and <em>honk</em> methods.</p>
<p>How is this achieved in JavaScript? Instead of an object-class relationship, there is an object-object relationship.</p>
<p>Where in Java our <em>myCar</em>, asked to honk, says &quot;go look at this class over there, which is my <em>blueprint</em>, to find the code you need&quot;, JavaScript says &quot;go look at that other object over there, which is my <em>prototype</em>, it has the code you are looking for&quot;.</p>
<p>Building objects via an object-object relationship is called <em>Prototype-based programming</em>, versus <em>Class-based programming</em> used in more traditional languages like Java.</p>
<p>Both are perfectly valid implementations of the object-oriented programming paradigm &#8211; it&#8217;s just two different approaches.</p>
<h2 id="creating-objects">Creating objects</h2>
<p>Let&#8217;s dive into code a bit, shall we? How could we set up our code in order to allow us to create our <em>myCar</em> object, ending up with an object that is a <em>Car</em> and can therefore <em>honk</em> and <em>drive</em>?</p>
<p>Well, in the most simple sense, we can create our object completely from scratch, or <em>ex nihilo</em> if you prefer the boaster expression.</p>
<p>It works like this:</p>
<code>var myCar = {}

myCar.honk = function() {
  console.log(&quot;honk honk&quot;);
}

myCar.drive = function() {
  console.log(&quot;vrooom...&quot;);
}</code>
<p>This gives us an object called <em>myCar</em> that is able to honk and drive:</p>
<code>myCar.honk()  // outputs &quot;honk honk&quot;
myCar.drive() // outputs &quot;vrooom...&quot;</code>
<p>However, if we were to create 30 cars this way, we would end up defining the honk and drive behaviour of every single one, something we said we want to avoid.</p>
<p>In real life, if we made a living out of creating, say, pencils, and we don&#8217;t want to create every pencil individually by hand, then we would consider building a pencil-making machine, and have this machine create the pencils for us.</p>
<p>After all, that&#8217;s what we implicitly do in a class-based language like Java &#8211; by defining a class <em>Car</em>, we get the car-maker for free:</p>
<code>Car myCar = new Car();</code>
<p>will built the <em>myCar</em> object for us based on the <em>Car</em> blueprint. Using the <em>new</em> keyword does all the magic for us.</p>
<p>JavaScript, however, leaves the responsibility of building an object creator to us. Furthermore, it gives us a lot of freedom regarding the way we actually build our objects.</p>
<p>In the most simple case, we can write a function which creates &quot;plain&quot; objects that are exactly like our &quot;ex nihilo&quot; object, and that don&#8217;t really share any behaviour &#8211; they just happen to roll out of the factory with the same behaviour copied onto every single one, if you want so.</p>
<p>Or, we can write a special kind of function that not only creates our objects, but also does some behind-the-scenes magic which links the created objects with their creator. This allows for a true sharing of behaviour: functions that are available on all created objects point to a single implementation. If this function implementation changes after objects have been created, which is possible in JavaScript, the behaviour of all objects sharing the function will change accordingly.</p>
<p>Let&#8217;s examine all possible ways of creating objects in detail.</p>
<h3 id="using-a-simple-function-to-create-plain-objects">Using a simple function to create plain objects</h3>
<p>In our first example, we created a plain <em>myCar</em> object out of thin air &#8211; we can simply wrap the creation code into a function, which gives us a very basic object creator:</p>
<code>function makeCar() {
  var newCar = {}
  newCar.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}</code>
<p>For the sake of brevity, the <em>drive</em> function has been omitted.</p>
<p>We can then use this function to mass-produce cars:</p>
<code>function makeCar() {
  var newCar = {}
  newCar.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
  return newCar;
}

myCar1 = makeCar();
myCar2 = makeCar();
myCar3 = makeCar();</code>
<p>One downside of this approach is efficiency: for every <em>myCar</em> object that is created, a new <em>honk</em> function is created and attached &#8211; creating 1,000 objects means that the JavaScript interpreter has to allocate memory for 1,000 functions, although they all implement the same behaviour. This results in an unnecessarily high memory footprint of the application.</p>
<p>Secondly, this approach deprives us of some interesting opportunities. These <em>myCar</em> objects don&#8217;t share anything &#8211; they were built by the same creator function, but are completely independent from each other.</p>
<p>It&#8217;s really like with real cars from a real car factory: They all look the same, but once they leave the assembly line, they are totally independent. If the manufacturer should decide that pushing the horn on already produced cars should result in a different type of honk, all cars would have to be returned to the factory and modified.</p>
<p>In the virtual universe of JavaScript, we are not bound to such limits. By creating objects in a more sophisticated way, we are able to magically change the behaviour of all created objects at once.</p>
<h3 id="using-a-constructor-to-create-objects">Using a constructor to create objects</h3>
<p>In JavaScript, the entities that create objects with shared behaviour are functions which are called in a special way. These special functions are called <em>constructors</em>.</p>
<p>Let&#8217;s create a constructor for cars. We are going to call this function <em>Car</em>, with a capital <em>C</em>, which is common practice to indicate that this function is a constructor.</p>
<p>Because we are going to encounter two new concepts that are both necessary for shared object behaviour to work, we are going to approach the final solution in two steps.</p>
<p>Step one is to recreate the previous solution (where a common function spilled out independent car objects), but this time using a constructor:</p>
<code>function Car() {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}</code>
<p>When this function is called using the <em>new</em> keyword, like so:</p>
<code>var myCar = new Car();</code>
<p>it implicitly returns a newly created object with the honk function attached.</p>
<p>Using <em>this</em> and <em>new</em> makes the explicit creation and return of the new object unnecessary &#8211; it is created and returned &quot;behind the scenes&quot; (i.e., the <em>new</em> keyword is what creates the new, &quot;invisible&quot; object, and secretly passes it to the <em>Car</em> function as its <em>this</em> variable).</p>
<p>You can think of the mechanism at work a bit like in this pseudo-code:</p>
<code>// Pseudo-code, for illustration only!

function Car(this) {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
  return this;
}

var newObject = {}
var myCar = Car(newObject);</code>
<p>As said, this is more or less like our previous solution &#8211; we don&#8217;t have to create every car object manually, but we still cannot modify the <em>honk</em> behaviour only once and have this change reflected in all created cars.</p>
<p>But we laid the first cornerstone for it. By using a constructor, all objects received a special property that links them to their constructor:</p>
<code>function Car() {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}

var myCar1 = new Car();
var myCar2 = new Car();

console.log(myCar1.constructor); // outputs [Function: Car]
console.log(myCar2.constructor); // outputs [Function: Car]</code>
<p>All created <em>myCars</em> are linked to the <em>Car</em> constructor. This is what actually makes them a class of related objects, and not just a bunch of objects that happen to have similar names and identical functions.</p>
<p>Now we have finally reached the moment to get back to the mysterious <em>prototype</em> we talked about in the introduction.</p>
<h3 id="using-prototyping-to-efficiently-share-behaviour-between-objects">Using prototyping to efficiently share behaviour between objects</h3>
<p>As stated there, while in class-based programming the class is the place to put functions that all objects will share, in prototype-based programming, the place to put these functions is the object which acts as the prototype for our objects at hand.</p>
<p>But where is the object that is the prototype of our <em>myCar</em> objects &#8211; we didn&#8217;t create one!</p>
<p>It has been implicitly created for us, and is assigned to the</p>
<code>Car.prototype</code>
<p>property (in case you wondered, JavaScript functions are objects that have properties, too).</p>
<p>Here is the key to sharing functions between objects: Whenever we call a function on an object, the JavaScript interpreter tries to find that function within the queried object. But if it doesn&#8217;t find the function within the object itself, it asks the object for the pointer to it&#8217;s prototype, then goes to the prototype, and asks for the function there. If it is found, it is then executed.</p>
<p>This means that we can create <em>myCar</em> objects without any functions, create the <em>honk</em> function in their prototype, and end up having <em>myCar</em> objects that know how to honk &#8211; because everytime the interpreter tries to execute the <em>honk</em> function on one of the <em>myCar</em> objects, it will be redirected to the prototype, and execute the <em>honk</em> function which is defined there.</p>
<p>Here is how this setup can be achieved:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;</code>
<p>Our constructor is now empty, because for our very simple cars, no additional setup is necessary.</p>
<p>Because both <em>myCars</em> are created through this constructor, their prototype points to <em>Car.prototype</em> &#8211; executing <em>myCar1.honk()</em> results in <em>Car.prototype.honk()</em> being executed.</p>
<p>Let&#8217;s see what this enables us to do. In JavaScript, objects can be changed at runtime. This holds true for prototypes, too. Which is why we can change the <em>honk</em> behaviour of all our cars even after they have been created:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;

Car.prototype.honk = function() {
  console.log(&quot;meep meep&quot;);
}

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;meep meep&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;meep meep&quot;</code>
<p>Of course, we can also add additional functions at runtime:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

Car.prototype.drive = function() {
  console.log(&quot;vrooom...&quot;);
}

myCar1.drive(); // executes Car.prototype.drive() and outputs &quot;vrooom...&quot;
myCar2.drive(); // executes Car.prototype.drive() and outputs &quot;vrooom...&quot;</code>
<p>But we could even decide to treat only one of our cars differently:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;

myCar2.honk = function() {
  console.log(&quot;meep meep&quot;);
}

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes myCar2.honk() and outputs &quot;meep meep&quot;</code>
<p>It&#8217;s important to understand what happens behind the scenes in this example. As we have seen, when calling a function on an object, the interpreter follows a certain path to find the actual location of that function.</p>
<p>While for <em>myCar1</em>, there still is no <em>honk</em> function within that object itself, that no longer holds true for <em>myCar2</em>. When the interpreter calls <em>myCar2.honk</em>, there now is a function within <em>myCar2</em> itself. Therefore, the interpreter no longer follows the path to the prototype of <em>myCar2</em>, and executes the function within <em>myCar2</em> instead.</p>
<p>That&#8217;s one of the major differences to class-based programming: while objects are relatively &quot;rigid&quot; e.g. in Java, where the structure of an object cannot be changed at runtime, in JavaScript, the prototype-based approach links objects of a certain class more loosely together, which allows to change the structure of objects at any time.</p>
<h2 id="object-orientation-prototyping-and-inheritance">Object-orientation, prototyping, and inheritance</h2>
<p>So far, we haven&#8217;t talked about inheritance in JavaScript, so let&#8217;s do this now.</p>
<p>It&#8217;s useful to share behaviour between a certain class of objects, but there are cases where we would like to share behaviour between different, but similar classes of objects.</p>
<p>Imagine our virtual world not only had cars, but also bikes. Both drive, but where a car has a horn, a bike has a bell.</p>
<p>Being able to <em>drive</em> makes both objects <em>vehicles</em>, but not sharing the <em>honk</em> and <em>ring</em> distinguishes them.</p>
<p>We could illustrate their shared and local behaviour as well as their relationship to each other as follows:</p>
<code>         Vehicle
         &gt; drive

            |
 ----------------------
 |                    |

Car                 Bike
&gt; honk              &gt; ring</code>
<p>Designing this relationship in a class-based language like Java is straightforward: We would define a class <em>Vehicle</em> with a method <em>drive</em>, and two classes <em>Car</em> and <em>Bike</em> which both <em>extend</em> the <em>Vehicle</em> class, and implement a <em>honk</em> and a <em>ring</em> class, respectively.</p>
<p>This would make the car as well as bike objects inherit the drive behaviour through the inheritance of their classes.</p>
<p>How does this work in JavaScript, where we don&#8217;t have classes, but prototypes?</p>
<p>Let&#8217;s look at an example first, and then dissect it. To keep the code short for now, let&#8217;s only start with a car that inherits from a vehicle:</p>
<code>function Vehicle() {}

Vehicle.prototype.drive = function () {
  console.log(&quot;vrooom...&quot;);
}


function Car() {}

Car.prototype = new Vehicle();

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}


var myCar = new Car();

myCar.honk();
myCar.drive();</code>
<p>In JavaScript, inheritance runs through a chain of prototypes.</p>
<p>The prototype of the <em>Car</em> constructor is set to a newly created <em>vehicle</em> object, which establishes the link structure that allows the interpreter to look for methods in parent objects.</p>
<p>The prototype of the <em>Vehicle</em> constructor has a function <em>drive</em>. Here is what happens when the <em>myCar</em> object is asked to <em>drive()</em>:</p>
<ul>
<li>The interpreter looks for a <em>drive</em> method within the <em>myCar</em> object, which does not exist</li>
<li>The interpreter then asks the <em>myCar</em> object for its prototype, which is the prototype of its constructor <em>Car</em></li>
<li>When looking at <em>Car.prototype</em>, the interpreter sees a <em>vehicle</em> object which has a function <em>honk</em> attached, but no <em>drive</em> function</li>
<li>Thus, the interpreter now asks this <em>vehicle</em> object for its prototype, which is the prototype of its constructor <em>Vehicle</em></li>
<li>When looking at <em>Vehicle.prototype</em>, the interpreter sees an object which has a <em>drive</em> function attached &#8211; the interpreter now knows which code implements the <em>myCar.drive()</em> behaviour, and executes it</li>
</ul>]]></content:encoded>
			<wfw:commentRss>/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Project: projectile, an HTML5 canvas game</title>
		<link>/2012/03/19/project-projectile-an-html5-canvas-game/</link>
		<comments>/2012/03/19/project-projectile-an-html5-canvas-game/#comments</comments>
		<pubDate>Mon, 19 Mar 2012 00:03:30 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Projects]]></category>

		<guid isPermaLink="false">/?p=512</guid>
		<description><![CDATA[Some days ago my son asked me how computer games are made. I couldn't really explain it very well in terms he understood (he's 5 years old), but I wanted to show it to him. Thus I started working with him on a 2D space shooter written in JavaScript, using the canvas element of HTML5.
<br />
<img src="/wp-content/uploads/2012/03/projectile_screenshot.png" alt="" title="projectile Screenshot" width="446" height="239" class="aligncenter size-full wp-image-513" />]]></description>
			<content:encoded><![CDATA[<p>
  Some days ago my son asked me how computer games are made. I couldn&#8217;t really explain it very well in terms he understood (he&#8217;s 5 years old), but I wanted to show it to him. Thus I started working with him on a 2D space shooter written in JavaScript, using the canvas element of HTML5. The result is playable at <em><a href="http://manuel.kiessling.net/projectile/">http://manuel.kiessling.net/projectile/</a></em>.
</p>
<p>
  <a href="http://manuel.kiessling.net/projectile/"><img src="/wp-content/uploads/2012/03/projectile_screenshot.png" alt="" title="projectile Screenshot" width="446" height="239" class="aligncenter size-full wp-image-513" /></a>
</p>
<p>
  The project source code is hosted at <a href="https://github.com/ManuelKiessling/projectile">https://github.com/ManuelKiessling/projectile</a>
</p>]]></content:encoded>
			<wfw:commentRss>/2012/03/19/project-projectile-an-html5-canvas-game/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Project: bivouac, an HTML5 web chat with filesharing</title>
		<link>/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/</link>
		<comments>/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/#comments</comments>
		<pubDate>Sun, 18 Mar 2012 13:33:36 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Projects]]></category>

		<guid isPermaLink="false">/?p=486</guid>
		<description><![CDATA[
Some weeks ago I started working on a new open source software, called bivouac.



bivouac provides an open source software package which allows to easily setup and run web-based group chats with dead-simple file-sharing (drag a file into the chat, and it&#8217;s immediately available as a download for all chat members).







Besides these &#8220;feature goals&#8221;, my secondary [...]]]></description>
			<content:encoded><![CDATA[<p>
Some weeks ago I started working on a new open source software, called <a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac"><em>bivouac</em></a>.
</p>

<p>
<a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac">bivouac</a> provides an open source software package which allows to easily setup and run web-based group chats with dead-simple file-sharing (drag a file into the chat, and it&#8217;s immediately available as a download for all chat members).
</p>

<p>
<img src="/wp-content/uploads/2012/03/bivouac_screenshots.png" alt="" title="bivouac - Screenshots" width="850" height="300" class="alignnone size-full wp-image-488" />
</p>

<p>
Besides these &#8220;feature goals&#8221;, my secondary goal is to learn how to architect JavaScript applications that are relatively complex, with domain-driven design and a strong separation of concerns in mind.
</p>

<p>
bivouac is written in JavaScript, for the Node.js platform. It&#8217;s hosted at <a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac">https://github.com/ManuelKiessling/bivouac</a>
</p>
]]></content:encoded>
			<wfw:commentRss>/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Deploying Symfony2 Apps via Scalarium: Improved Methodology</title>
		<link>/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/</link>
		<comments>/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/#comments</comments>
		<pubDate>Thu, 05 Jan 2012 08:04:19 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=453</guid>
		<description><![CDATA[Some weeks ago I wrote about deploying Symfony2 Applications to Amazon AWS with Scalarium. It turned out that the described methodology can be refined in several ways. Here's how.]]></description>
			<content:encoded><![CDATA[<p>
Some weeks ago I wrote about <a href="/wordpress/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/">deploying Symfony2 Applications to Amazon AWS with Scalarium</a>. It turned out that the described methodology can be improved in several ways. Here&#8217;s how.
</p>

<p>
First, let&#8217;s discuss what&#8217;s suboptimal with the previously described approach. The basic idea was to provide a custom Chef recipe which is executed on our instances whenever our Symfony2 application gets deployed. This recipe took care of
<ul>
<li>
executing the tasks which need to be done whenever the application is deployed, like installing the Symfony2 vendors or cleaning the application cache
</li>
<li>
configuring Apache to correctly serve the application
</li>
</ul>
</p>

<p>
Well, the problem is that these are really two very different tasks which shouldn&#8217;t be mixed together.
Updating the web server configuration every time you release a new version of your application simply doesn&#8217;t make sense.
</p>

<p>
Thanks to <a href="http://wiki.opscode.com/display/chef/Deploy+Resource">Chef deployment hooks</a>, we can separate these tasks. Whatever needs to be done upon application deployment can be provided within the application itself, making it much more self-contained. This way, application business logic and application deployment logic live in the same source tree.
</p>

<p>
On the other hand, system configuration steps which aren&#8217;t specific to deploying a new version of your application, but are specific to hosting your application in a given system context, shouldn&#8217;t be bundled with your application, but with your system context.
</p>

<p>
Thus, we are going to separate the deployment recipes and the system setup recipes. We will provide the deploy recipes from our application, and the system setup recipes through a custom Chef cookbook, just as we did in <a href="/wordpress/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/">the first version of this tutorial</a>.
</p>


<p>
Let&#8217;s look at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/b325adb91f421ca87eacf8f545339324bfccfc2a/symfony2/recipes/deploy.rb">the old version of our deploy.rb recipe</a>, and decide which parts are related to the deployment of our application, and which parts are related to hosting our application.
</p>

<p>
Well, it&#8217;s actually quite simple &#8211; everything from the beginning of the file through <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/b325adb91f421ca87eacf8f545339324bfccfc2a/symfony2/recipes/deploy.rb#L44">line 44</a> is stuff that needs to be done upon every single deployment, or else we wouldn&#8217;t end up with a working application.
</p>

<p>
Let&#8217;s move this part of the deployment recipe into our application. Where does it belong? When Scalarium&#8217;s Chef deploys our application, it looks for certain scripts in the <em>/deploy</em> directory of our application:
<ul>
<li>/deploy/before_migrate.rb</li>
<li>/deploy/before_symlink.rb</li>
<li>/deploy/before_restart.rb</li>
<li>/deploy/after_restart.rb</li>
</ul>
As their names imply, these scripts are triggered at certain points of the deployment lifecycle. They are closely related to the steps that are necessary when deploying Ruby on Rails applications, and thus not all of them are useful for us when deploying a Symfony2 application.
</p>

<p>
For the steps we want to execute with our recipe (installing the vendors, clearing the app cache, executing db migrations, installing the assets, and chowning app cache and log dirs), the <em>before_symlink.rb</em> is just fine &#8211; it hooks into the deployment process the moment before Chef, after downloading the application source code from Github, changes the symbolic link at <em>/srv/www/symfonyexample/current</em> to the newly downloaded release. At this moment, we have all the source code available, but it is not yet put into production, thus it&#8217;s the most sensible moment for additional setup steps.
</p>

<p>
Moving the deployment-specific parts of our recipe into this hook gives us a <em>before_symlink.rb</em> script as it&#8217;s available in our <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/blob/9bf49a35f352809c06c25b825c8b1f7386eaae2a/deploy/before_symlink.rb">ScalariumExampleSymfony2Application repository on GitHub</a>.
</p>

<p>
The rest of the original recipe is all about configuring Apache in a way that gives us a working vhost for serving our application &#8211; this configuration may change, but it isn&#8217;t related to any specific deployment. Thus, these steps should be done only when our Scalarium/AWS instance is set up, and not on every deployment.
</p>

<p>
And as with the deployment steps, there is a better way to set up Apache for our application, too. Turns out, we don&#8217;t need a recipe at all. The reason is that Scalarium implements a very convenient substitution logic &#8211; if our own cookbook provides a file with the same name at the same location as one from the Scalarium-provided cookbooks, the file from our own cookbook &#8220;wins&#8221; and is used instead.
</p>

<p>
The Scalarium file we are going to substitute is located at <a href="https://github.com/scalarium/cookbooks/blob/master/mod_php5_apache2/templates/default/web_app.conf.erb">/mod_php5_apache2/templates/default/web_app.conf.erb</a>. By providing this file <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/f5850713ef4663175ad326d709f8e03ad8737243/mod_php5_apache2/templates/default/web_app.conf.erb">in our own cookbook repository</a>, it&#8217;s used as the template for our Symfony2 application vhost &#8211; we don&#8217;t even need to define &#8220;symfony2::deploy&#8221; as a <em>Custom Recipe</em> anymore.
</p>

<p>
And that&#8217;s it. As described in my previous post on this topic, we need to configure our Scalarium cloud with the information on our custom cookbook and our application, but instead of manually hooking our custom recipe into the <em>configure</em> and <em>deploy</em> events in Scalarium, our newly created deploy hook now lives in the application itself, and is automatically triggered on every deployment, and our newly created <em>web_app.conf.erb</em> Apache vhost template is automatically used by Scalarium&#8217;s Chef when setting up new PHP application server instances.
</p>]]></content:encoded>
			<wfw:commentRss>/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Cocoa: What to do if outlineView: objectValue ForTableColumn: byItem never gets called</title>
		<link>/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/</link>
		<comments>/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/#comments</comments>
		<pubDate>Tue, 15 Nov 2011 21:10:21 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=441</guid>
		<description><![CDATA[
If you set up an OutlineView in Interface Builder and connect your Controller as its dataSource and delegate (and provide the methods there accordingly), you will notice that


- (id)outlineView:(NSOutlineView *)outlineView
  objectValueForTableColumn:(NSTableColumn *)tableColumn
  byItem:(id)item


never get&#8217;s called. The reason might very simple: in Interface Builder, in the attributes inspector for your OutlineView, you can define [...]]]></description>
			<content:encoded><![CDATA[<p>
If you set up an OutlineView in Interface Builder and connect your Controller as its dataSource and delegate (and provide the methods there accordingly), you will notice that
</p>
<p>
<code>- (id)outlineView:(NSOutlineView *)outlineView
  objectValueForTableColumn:(NSTableColumn *)tableColumn
  byItem:(id)item</code>
</p>
<p>
never get&#8217;s called. The reason might very simple: in Interface Builder, in the attributes inspector for your OutlineView, you can define the <em>Content Mode</em> as <em>View Based</em> or <em>Cell Based</em>.
</p>
<p>
If you simply want to display the content of NSStrings, choose <em>Cell Based</em>, and objectValueForTableColumn will be called.
</p>
<p>
Only took me 3 hours to find out.
</p>
]]></content:encoded>
			<wfw:commentRss>/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>How hosting MyHammer started in a devops way back in the day</title>
		<link>/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/</link>
		<comments>/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/#comments</comments>
		<pubDate>Mon, 07 Nov 2011 15:47:59 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=433</guid>
		<description><![CDATA[
This here is just me, bragging about myself. You have been warned.







This guy with the psychopathic look standing in a room full of rubbish in front of something that vaguely looks like computers is me standing in front of the first MyHammer server cluster, only days before the whole system went into production.



It was 2005, [...]]]></description>
			<content:encoded><![CDATA[<p>
This here is just me, bragging about myself. You have been warned.
</p>

<p>
<a href="/wp-content/uploads/2011/11/manuel_kiessling_myhammer_fai.jpg"><img src="/wp-content/uploads/2011/11/manuel_kiessling_myhammer_fai-300x225.jpg" alt="Manuel Kiessling in front of the very first MyHammer FAI server cluster" title="Manuel Kiessling in front of the very first MyHammer FAI server cluster" width="300" height="225" class="alignnone size-medium wp-image-434" /></a>
</p>

<p>
This guy with the psychopathic look standing in a room full of rubbish in front of something that vaguely looks like computers is me standing in front of the first MyHammer server cluster, only days before the whole system went into production.
</p>

<p>
It was 2005, and I had set up the whole system in a way I&#8217;m still quite proud of. Being the only systems administrator at the company at that time, I knew that I wouldn&#8217;t face happy times if I would set up the cluster in a traditional way, that is, machine by machine. Although it started small, the system was supposed to grow, especially regarding the web servers, and even I knew that while managing 13 servers individually was doable, it wasn&#8217;t fun, and managing several dozen machines just sounded like total nightmare.
</p>

<p>
Which is why I thought about better ways to setup and manage the whole cluster. I ended up with a central installation server running <a href="http://fai-project.org/">FAI &#8211; Fully Automatic Installation</a>, and hosting a <a href="http://cfengine.com/community">cfengine</a> server. This way, new machines could be automatically provisioned with the operating system (Debian GNU/Linux) they needed by simply booting from their network adaptor, and new as well as existing systems could easily be maintained, configured, and updated via cfengine. No need to ever log in on a specific machine to change configuration or install software. A centralized syslog server completed the picture.
</p>

<p>
cfengine was a major pain in the ass at times, and with Chef and Puppet there a way more sophisticated tools available today, but the overall system ran extremely well and could easily scale over time.
</p>

<p>
Years later, the term Devops was coined, and I couldn&#8217;t help congratulating myself for getting it right so long ago.
</p>]]></content:encoded>
			<wfw:commentRss>/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Deploying Symfony2 Applications to Amazon AWS with Scalarium</title>
		<link>/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/</link>
		<comments>/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/#comments</comments>
		<pubDate>Tue, 01 Nov 2011 10:18:46 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=391</guid>
		<description><![CDATA[This article describes how to use the cloud-based cluster-management platform Scalarium in order to automatically mass-deploy Symfony2 applications with a MySQL database backend onto clusters of Amazon EC2 virtual machines by creating a special Symfony2 environment, using a custom Chef recipe, and making use of Doctrine migrations.]]></description>
			<content:encoded><![CDATA[<h2 style="color: red">Important Note</h2>
<p>
The methodology explained here is <span style="color: red">outdated</span>, please read <a href="/wordpress/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/">Deploying Symfony2 Apps via Scalarium: Improved Methodology</a> for an updated version.
</p>

<h2>About</h2>
<p>
This article describes how to use the cloud-based cluster-management platform Scalarium in order to automatically mass-deploy Symfony2 applications with a MySQL database backend onto clusters of Amazon EC2 virtual machines by creating a special Symfony2 environment, using a custom Chef recipe, and making use of Doctrine migrations.
</p>

<h2>Target audience</h2>
<p>
Current or soon-to-be Scalarium customers, devops interested in deploying Symfony2 applications using centralized configuration management tools like Chef or Puppet, PHP developers.
</p>

<h2>Prerequisites</h2>
<p>
In order to get your Symfony2 application running on Amazon AWS, you need an AWS account, a Scalarium account (correctly set up with your AWS credentials), and a Github account to host your Symfony2 application and your custom Chef recipes. This tutorial assumes that you are already familiar with managing EC2 clusters with Scalarium. It&#8217;s tested to work with Symfony 2.0.4 on Ubuntu 11.04 instances.
</p>

<h2>Overview</h2>
<p>
<a href="/wordpress/wp-content/uploads/2011/11/Logical-overview.png"><img src="/wordpress/wp-content/uploads/2011/11/Logical-overview-300x225.png" alt="" /></a>
</p>

<p>
The above diagram is here to illustrate how the different parts are working together in order to deploy a running application. The virtual machines hosting our Symfony2 application and the database are EC2 instances running in the Amazon AWS cloud. We will use Scalarium to define the layout of our cluster (called a &#8220;cloud&#8221; in Scalarium), to configure the application which will be deployed onto the application servers within this cluster, and to configure the custom Chef recipes which need to be applied to our application server instances in order to finalize the deployment and setup of our application.
</p>

<p>
Looking at the diagram, we could say that Scalarium <em>manages</em>, Amazon <em>runs</em>, and Github <em>provides</em> &#8211; Scalarium uses what Github provides in order to run the application on Amazon.
</p>

<p>
Because Scalarium already provides the setup logic and recipes that allow us to set up basic Apache/PHP application servers and MySQL database servers on AWS, we will only need to provide what is needed in order to finalize the deployment and setup of our specific Symfony2 application &#8211; setting up a basic Ubuntu server, installing software packages like PHP, Apache, MySQL, and checking out our Symfony2 application code from Github is all done by Scalarium without the need to provide additional means.
</p>

<h2>The example application</h2>
<p>
It makes sense to follow this tutorial using your own Symfony2 application. However, an example application hosted at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application">Github.com → MyHammer → ScalariumExampleSymfony2Application</a> is used to illustrate the process &#8211; you are free to use this repository to deploy the application to your Scalarium cloud. The <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/commits/master">commit history</a> of this repository also shows step by step which changes need to be applied to a Symfony2 application to prepare it for running on a Scalarium cloud.
</p>

<h2>Setting up a Scalarium cloud for our application</h2>
<p>
Our very first step is to set up a new cloud in Scalarium which manages the EC2 instances, server roles, application configuration and custom Chef cookbooks needed to deploy and run our Symfony2 application.
</p>

<p>
In order to do so, click on <em>Add Cloud</em> in the upper right corner of the main Scalarium administration interface. Choose whatever name you like (I will stick with &#8220;Symfony&#8221;), define a region that makes sense for you (Europe in my case), and choose &#8220;Ubuntu 11.04&#8243; as the default operating system. Please choose &#8220;Role Dependent&#8221; as the <em>Hostname Theme</em>, because this will make it much easier to follow my explanations whenever I&#8217;m talking about specific machine instances.
</p>

<p>
Assuming that you have already set up Amazon AWS in your Scalarium account, please choose the AWS credentials and SSH keys accordingly.
</p>

<p>
You now have an empty cloud. Let&#8217;s define these three things in order to host our application:
<ul>
<li>An application</li>
<li>Custom Cookbooks</li>
<li>An application server and a database server role</li>
</ul>
</p>

<p>
Let&#8217;s start with our application. Later, we are going to make some modifications to our application that are necessary for hosting it with Scalarium, but we can already configure it into our cloud. Clicking on <em>Add Application</em> gives us a dialogue in which we can configure where EC2 instances will be able to pull our application from upon bootup or application deployment.
</p>

<p>
I&#8217;m going to choose &#8220;SymfonyExample&#8221; as the name. Please choose &#8220;PHP&#8221; as the <em>Application Type</em>. If you are going to use the sample application that resides on Github, please choose a <em>Repository Type</em> of &#8220;Git&#8221;, and enter the following <em>Repository URL</em>: &#8220;git://github.com/MyHammer/ScalariumExampleSymfony2Application.git&#8221;. All other parameters are not important for now &#8211; however, if you are configuring your own private repository, you will need to provide a deploy SSH key.
</p>

<p>
Although this is sufficient to deploy the code from Github onto our instances, it&#8217;s not enough to actually get a running application. Additional steps are necessary after the code has been pulled from Github, and those steps need to be performed using a custom Chef recipe. We are going to write this recipe later, but we can already configure the cookbook that contains the recipe into our cloud. On the cloud overview page, select <em>Manage Cookbooks</em> from the <em>Actions</em> dropdown.
</p>

<p>
In the new dialogue, check <em>Enable custom cookbooks</em>, and configure as follows:
<ul>
<li><em>Repository Type</em>: &#8220;Git&#8221;</li>
<li><em>Repository URL</em>: &#8220;git://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes.git&#8221;</li>
</ul>
<em>Deploy SSH Key</em> and <em>Branch / Revision</em> don&#8217;t need to be filled out.
</p>

<p>
As a last step, our cloud needs two machine roles: one for our application servers, and one for our database server. We will start with the application server role. On the cloud overview page, select <em>Add role</em>, and choose the <em>PHP Application Server</em> role.
</p>

<p>
Once you have added it, an additional configuration is necessary: In <em>Custom Recipes</em>, please add the recipe &#8220;symfony2::deploy&#8221; for the Scalarium actions <em>configure</em> and <em>deploy</em>. This makes sure that our recipe is run everytime our application is deployed to an instance, or whenever the cloud changes.
</p>

<p>
Last but not least, create an additional role <em>MySQL Master</em>. Additional configuration is not needed here. If you like, you can already add and boot an instance for this role.
</p>

<h2>Modifying the Symfony2 application</h2>
<p>
Our Symfony2 application doesn&#8217;t need to be re-programmed in any way in order to run via Scalarium &#8211; however, its configuration needs to be specifically modified. You can either modify your existing &#8220;prod&#8221; environment configuration, or you may want to create a new environment called &#8220;scalarium&#8221;, which is what I did for the example application.
</p>

<p>
The nice thing about Scalarium and Symfony2 is that the former provides a very nice PHP interface to the layout of the current cluster, and the latter provides an easy way to make use of this interface.
</p>

<p>
This allows us to dynamically configure our application&#8217;s database setup, avoiding the need to hard code e.g. the IP address of our MySQL master into the application. Here is a step by step description on how to achieve this:
</p>

<p>
First, create a new and empty file <em>/app/config/config_scalarium.yml</em>. This is going to be the main configuration file for the new &#8220;scalarium&#8221; environment. Fill the file with the following content:

<code>imports:
    - { resource: parameters_scalarium.php }
    - { resource: config.yml }
</code>

This makes our new environment use 99% of it&#8217;s settings based on those from the &#8220;prod&#8221; environment &#8211; however, an additional file is imported, <em>/app/config/parameters_scalarium.php</em> &#8211; this is where we are going to dynamically define our database settings using the power of Scalarium.
</p>

<p>
Let&#8217;s fill <em>/app/config/parameters_scalarium.php</em> with life:

<code>&lt;?php

include_once(__DIR__ . '/../../scalarium.php');

$scalarium = new Scalarium();
$container-&gt;setParameter('database_driver',  'pdo_mysql');
$container-&gt;setParameter('database_host',     $scalarium->db->host);
$container-&gt;setParameter('database_port',     '3306');
$container-&gt;setParameter('database_name',     $scalarium->db->database);
$container-&gt;setParameter('database_user',     $scalarium->db->username);
$container-&gt;setParameter('database_password', $scalarium->db->password);
$container-&gt;setParameter('database_path',     null);

unset($scalarium);</code>

What happens here is that the database parameter placeholders like <em>%database_host%</em>, which are used in <em>/app/config/config.yml</em> to configure the database for our application, are programmatically defined based on the current state of the Scalarium cloud. Leveraging the power of the <em>scalarium.php</em> script, which is available in the root directory of our application per default (a symlink is created whenever our application is deployed on an instance), this makes our database config dynamic and therefore always up to date.
</p>

<p>
<blockquote>
The creators of Scalarium point out that using <em>scalarium.php</em> is just a convenience. The full-stack alternative would be to write a Chef recipe that generates the configuration file and register this recipe on the <em>configure</em> and <em>deploy</em> events. Inside the Chef recipe Scalarium provides you with all necessary information like which hosts exist in the cloud and what their configuration is (e.g. IP addresses, roles, etc.). See <a href="http://support.scalarium.com/kb/custom-instance-setup">http://support.scalarium.com/kb/custom-instance-setup</a> for further information on how to use the Scalarium cloud structure information in your own recipes.
</blockquote>
</p>

<p>
One important thing to consider here is that the settings in <em>/app/config/parameters.ini</em> overwrite settings with the same name in <em>/app/config/parameters_scalarium.php</em> &#8211; we therefore need to remove all lines containing &#8220;%database&#8221; from <em>parameters.ini</em>!
</p>

<p>
The last step is to create a Frontend Controller for our new &#8220;scalarium&#8221; environment at <em>/web/app_scalarium.php</em>:

<code>&lt;?php

require_once __DIR__.'/../app/bootstrap.php.cache';
require_once __DIR__.'/../app/AppKernel.php';
//require_once __DIR__.'/../app/AppCache.php';

use Symfony\Component\HttpFoundation\Request;

$kernel = new AppKernel('scalarium', false);
$kernel-&gt;loadClassCache();
//$kernel = new AppCache($kernel);
$kernel-&gt;handle(Request::createFromGlobals())-&gt;send();
</code>
</p>

<p>
You can look at a visual before-and-after comparison of all these changes on Github:
<br />
<a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/compare/d2fad2a06b82471c173bd211a59223075b87ab98...0392bed4c1bf5e08b98a48aff3fd71760c938a93"><img src="/wp-content/uploads/2011/11/github_comparison_symfony2_scalarium_environment-249x300.png" width="249" height="300"/></a>

<h2>Putting the parts together</h2>
<p>
Our cloud now has everything in place it needs. Its roles are defined, our application servers know where to pull our application from, instances of role &#8220;PHP Application Server&#8221; are configured to use our custom Symfony2 deploy recipe, and our application has been modified to make use of the dynamic cloud configuration in a new environment called &#8220;scalarium&#8221;.
</p>

<p>
Let&#8217;s take a moment to look at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/master/symfony2/recipes/deploy.rb#L1">our custom Chef recipe</a>.
</p>

<p>
What this recipe does is it checks for each application that is going to be deployed if it&#8217;s actually a Symfony2 application, and if yes, it starts by updating the vendors (or installing them from scratch if none exist yet), clears the application&#8217;s cache, applies database migrations (if any), makes sure that cache and log files and directories are read and writable for those users that need it, installs and configures an Apache vhost file for the app, and disables the default vhost.
</p>

<p>
Ok, let&#8217;s deploy our application! All you need to do is to add an instance of role <em>PHP Application Server</em> and boot it up. Everything else happens automatically:
<ul>
<li>Scalarium requests an EC2 instance from Amazon AWS which is booted with a fresh Ubuntu 11.04 image</li>
<li>Scalarium sets this machine up with some basic packages like PHP, Apache etc., and executes its basic setup routines (which are Chef recipes, too)</li>
<li>Our Symfony2 application is pulled from Github and deployed to <em>/srv/www/symfonyexample/current</em></li>
<li>The instance executes our custom Chef recipe <em>symfony2::deploy</em> which installs the vendors, clears the app cache, applies the database migrations, installs the web assets, sets users and permissions on cache and log files, and configures an Apache vhost for our application</li>
</ul>
</p>

<p>
Once this is done, you can browse to the public IP of the booted application server instance, which will display the Symfony2 welcome page.
</p>
]]></content:encoded>
			<wfw:commentRss>/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Behaviour-driven node.js development with stubbing by combining Vows and node-gently</title>
		<link>/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/</link>
		<comments>/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/#comments</comments>
		<pubDate>Wed, 13 Apr 2011 18:35:16 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=374</guid>
		<description><![CDATA[
I&#8217;m about 3.5 hours into node.js development, I guess that qualifies me to give advice on it on this Internet thing.



Being the BDD fanatic that I am, I wanted to start off behaviour-driven right from the beginning, and Vows looked like a good choice.



However, I quickly came to the point where I needed to stub [...]]]></description>
			<content:encoded><![CDATA[<p>
I&#8217;m about 3.5 hours into node.js development, I guess that qualifies me to give advice on it on this Internet thing.
</p>

<p>
Being the BDD fanatic that I am, I wanted to start off behaviour-driven right from the beginning, and <a href="http://vowsjs.org/">Vows</a> looked like a good choice.
</p>

<p>
However, I quickly came to the point where I needed to stub out a dependency in one of my modules, and as far as I can see, Vows doesn&#8217;t provide mocking/stubbing. But <a href="https://github.com/felixge/node-gently">https://github.com/felixge/node-gently</a> does, and here is my approach at combining these two:
</p>

<p>
This is the Vows spec:
<code>var gently = global.GENTLY = new (require("gently"));

var vows = require("vows"),
    assert = require("assert");

var myModule = require("MyModule");

vows.describe("My Module").addBatch({
  "when calling its foo() method": {
    topic: myModule,
    "it triggers a console message": function (topic) {
      gently.expect(gently.hijacked.sys, "puts", function(str) {
        assert.equal(str, "Hello World");
      });
      topic.foo("Hello World");
    }
  }
}).export(module);
</code>
</p>

<p>
And this is the implementation of MyModule:
<code>if (global.GENTLY) require = GENTLY.hijack(require);

var sys = require("sys");

function foo(message) {
  sys.puts(message);
  return true;
}

exports.foo = foo;
</code>
</p>
<p>
No idea if this makes any sense in the long run &#8211; I will tell you when I&#8217;m about 14 hours or so into BDD node.js development&#8230;
</p>]]></content:encoded>
			<wfw:commentRss>/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>The only job application form that makes sense</title>
		<link>/2011/04/11/the-only-job-application-form-that-makes-sense/</link>
		<comments>/2011/04/11/the-only-job-application-form-that-makes-sense/#comments</comments>
		<pubDate>Mon, 11 Apr 2011 10:00:14 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Other]]></category>

		<guid isPermaLink="false">/?p=370</guid>
		<description><![CDATA[It just dawned on me that this is really the only job application form that makes sense...]]></description>
			<content:encoded><![CDATA[<p>
It just dawned on me that this is really the only job application form that makes sense, isn&#8217;t it?
</p>
<p>
<img src="http://manuel.kiessling.net/images/job_application_form.png" alt="Job Application Form" />
</p>]]></content:encoded>
			<wfw:commentRss>/2011/04/11/the-only-job-application-form-that-makes-sense/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Why developing without tests is like driving a car without brakes</title>
		<link>/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/</link>
		<comments>/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/#comments</comments>
		<pubDate>Thu, 07 Apr 2011 07:37:55 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=367</guid>
		<description><![CDATA[The following roots in something I heard from Jon Jagger at QCon London 2011 after his fantastic talk about Deliberate Practice. It was related to Test Driven Development. He asked "Why do cars have brakes?". It made us think "To stop!", but he said "No - to go faster".]]></description>
			<content:encoded><![CDATA[<blockquote>
<p>
The following roots in something I heard from <a href="http://twitter.com/JonJagger">Jon Jagger</a> at QCon London 2011 after his fantastic talk about <a href="http://qconlondon.com/london-2011/presentation/Deliberate+Practice">Deliberate Practice</a>. It was related to Test Driven Development. He asked &#8220;Why do cars have brakes?&#8221;. It made us think &#8220;To stop!&#8221;, but he said &#8220;No &#8211; to go faster&#8221;.
</p>
<p>
(Jon points out he didn&#8217;t invent it, he heard it from <a href="http://twitter.com/KevlinHenney">Kevlin Henney</a>).
</p>
<p>
I have been thinking about this ever since, and here is what I came up with.
</p>
</blockquote>

<p>
Imagine I would give you the keys to my car. I would tell you &#8220;here are the keys, you can drive wherever you want, including the highway, have fun!&#8221;
</p>

<p>
How fast would go? My car is not exactly a sports car, but it can do around 200 km/h. I guess we both agree that you would drive around 50 km/h within cities (the maximum allowed in Germany), and probably up to 200 km/h on the highway, as long as there is no limit.
</p>

<p>
Fine. Now image this: I would give you the keys to my car. I would tell you &#8220;here are the keys, you can drive wherever you want, including the highway, have fun! Oh, just one thing, <strong>the brakes don&#8217;t work</strong>.&#8221;
</p>

<p>
Now let&#8217;s forget for a moment that in reality, you probably wouldn&#8217;t start at all, if you <em>had</em> to drive, then how fast would you go? 10 km/h, maybe 20? Driving extremely cautious, always trying to look as far ahead as possible if you are going to need to halt? Yeah, I thought so.
</p>

<p>
But why is that? The brakes don&#8217;t have anything to do with the speed of my car &#8211; it&#8217;s still capable of doing 200 km/h just fine!
</p>

<p>
It&#8217;s because the ability to stop is what enables you to go real fast. With only a bit of exaggaration you could say that having a brake allows for a very &#8220;iterative&#8221; way of driving &#8211; no cars within the next 300 meters, let&#8217;s accelerate a bit &#8211; oh, there&#8217;s a car coming over from the right, let&#8217;s brake a bit &#8211; ok, now I can accelerate again &#8211; ah, there is a signal that suddenly turned red, no problem, I will stop here.
</p>

<p>
For me, this metaphor is the best I could find by now to explain to myself (and in the future, to others), why I <em>really</em> want to develop test-driven, and why it actually makes me faster, not slower, although I&#8217;m doing more.
</p>

<p>
Just as the brake doesn&#8217;t directly influence your driving speed, but does so indirectly, your tests won&#8217;t influence your coding speed directly, but indirectly. It&#8217;s because once they are in place, they allow you to iterate over your code and refactor it at what I, from my own experience, can only describe as the speed of light compared to conventional programming.
</p>

<p>
With tests in place, it&#8217;s like: Mh, what if I would split this rather long method into two? &#8211; ok, works; What if I put a bit more of dependency injection into this class? &#8211; ah, now this test here fails, no problem, I will have it back to green within minutes, I know exactly where to go to fix this; Hey, I could give this method here a better name &#8211; ok, still green; There&#8217;s this performance bottleneck deep inside this one class that is heavily used by a lot of other classes, let&#8217;s see if I can fix this &#8211; my tests will tell me if I accidently changed behaviour.
</p>

<p>
Compare this to conventional programming: You will never know for sure what breaks somewhere else if you change something. If you want to find out, you need huge amounts of manpower to have your webpage or UI tested for regression. What really happens is that you slow down to a near halt: because you don&#8217;t know what&#8217;s around the next corner when developing, and you know there is nothing that will immediately stop you and save you from harm if you take that next corner, you will drive, err, code so cautious, you won&#8217;t make any real progress.
</p>

<blockquote>
On which Jon commented: &#8220;Yes. As the pragmatic programmers say, paraphrasing &#8211; <em>you don&#8217;t know why it&#8217;s broken because you didn&#8217;t know why it worked in the first place.</em>&#8221;
</blockquote>
]]></content:encoded>
			<wfw:commentRss>/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Visualization: Why working iterative works</title>
		<link>/2011/03/10/visualization-why-working-iterative-works/</link>
		<comments>/2011/03/10/visualization-why-working-iterative-works/#comments</comments>
		<pubDate>Thu, 10 Mar 2011 16:48:18 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=299</guid>
		<description><![CDATA[
I&#8217;m really into visualizations. More often then not I can only really &#8220;get&#8221; something (a complex system, an abstract idea, a process etc.) when I see it visualized. You could call this the transformation of gut feelings into images.



So, I had this (rather obvious) gut feeling that working iterative in software projects makes a lot [...]]]></description>
			<content:encoded><![CDATA[<p>
I&#8217;m really into visualizations. More often then not I can only really &#8220;get&#8221; something (a complex system, an abstract idea, a process etc.) when I see it visualized. You could call this the transformation of gut feelings into images.
</p>

<p>
So, I had this (rather obvious) gut feeling that working iterative in software projects makes a lot of sense, I&#8217;ve heard all the arguments and explanations and examples and stuff like that, and I probably already &#8220;got it&#8221;, but I thought it could make sense to clearly work out why exactly it makes sense, by visualizing it. Surely it&#8217;s no rocket surgery what happens here, but I kind of like it and would like to share it.
</p>

<p>
So here&#8217;s my approach:
</p>

<p>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Start.png" width="450" height="800" border="0" style="border: none" align="center" />
<br/>
<br />
The customer tells a product manager what he wants him to build. Let&#8217;s see how a waterfall approach leads to the product team failing at this:
<br />
<br />
</p>

<p>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Waterfall.png" width="450" height="800" border="0" style="border: none" align="center" />
<br />
The reason why what is finally delivered isn&#8217;t what the customer expected is that the project goes through the hands of different people and different stages, and every time the project is given to another person or team, the amount of misunderstanding grows. That&#8217;s only natural because we cannot copy ideas from one brain to another in a 1:1 manner.
</p>

<p>
Here is why an iterative approach makes sense:
<br />
<br/>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Iterations.png" width="450" height="800" border="0" style="border: none" align="center" />
<br />
The project really starts the same: There is a certain amount of misunderstanding, and the team does things wrong. But due to the regular feedback from the customer, this wrong direction can be corrected. It might then move into another direction which is still a bit wrong, but then comes the next correction, and finally everything is on track.
</p>
]]></content:encoded>
			<wfw:commentRss>/2011/03/10/visualization-why-working-iterative-works/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Interview mit mir auf dem O&#8217;Reilly Blog</title>
		<link>/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/</link>
		<comments>/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/#comments</comments>
		<pubDate>Mon, 31 Jan 2011 12:26:43 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Other]]></category>

		<guid isPermaLink="false">/?p=289</guid>
		<description><![CDATA[<p>
Das Blog von O'Reilly Deutschland hat in seiner Reihe "Karriere(n) in der IT" ein <a href="http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/">kurzes Interview</a> mit mir geführt.
</p>]]></description>
			<content:encoded><![CDATA[<p>
Das Blog von O&#8217;Reilly Deutschland hat in seiner Reihe &#8220;Karriere(n) in der IT&#8221; ein kurzes Interview mit mir geführt:
</p>
<p>
<a href="http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/">http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/</a>
</p>]]></content:encoded>
			<wfw:commentRss>/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Empfehlung: Literatur für Manager</title>
		<link>/2011/01/21/literatur-fuer-manager/</link>
		<comments>/2011/01/21/literatur-fuer-manager/#comments</comments>
		<pubDate>Fri, 21 Jan 2011 13:52:04 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Recommendations]]></category>

		<guid isPermaLink="false">/?p=272</guid>
		<description><![CDATA[
Ich werde oft gefragt, wie ich so ein hervorragender Manager geworden bin. Haha, quatsch, kein Mensch fragt mich das. Ich stelle mir nur manchmal vor, dass es so wäre. Dann weine ich mich langsam in den Schlaf&#8230; Wo war ich? Ach ja.



Also, ich wurde nach Literatur zum Thema Management bzw. Mitarbeiterführung gefragt. Spontan fiel mir [...]]]></description>
			<content:encoded><![CDATA[<p>
Ich werde oft gefragt, wie ich so ein hervorragender Manager geworden bin. Haha, quatsch, kein Mensch fragt mich das. Ich stelle mir nur manchmal vor, dass es so wäre. Dann weine ich mich langsam in den Schlaf&#8230; Wo war ich? Ach ja.
</p>

<p>
Also, ich wurde nach Literatur zum Thema Management bzw. Mitarbeiterführung gefragt. Spontan fiel mir dazu erst mal nicht viel ein, weil ich keinen Kanon dazu im Hinterkopf habe, und weil Führungsqualitäten sowieso nur bedingt anlesbar sind. Dann sind mir aber doch ein paar Quellen eingefallen die mir vieles klarer gemacht haben und auf deren Erkenntnisse ich in der Praxis immer wieder zurückgreifen konnte:
</p>

<p>
<strong>Samy Molcho &#8211; Alles über Körpersprache</strong>

<p>
Kein Buch über Management im engeren Sinne. Ist aber eine gute Anleitung für den Umgang mit Menschen, also das, was man in 99,9% der Zeit als Manager tun muss. Hat mir übrigens vor allem geholfen, mich selbst einzuschätzen, der Aspekt &#8220;ich lese jetzt die Körpersprache anderer Menschen und kann praktisch ihre Gedanken lesen&#8221; ist denke ich überbewertet. Übrigens das vielleicht wichtigste Buch als Vorbereitung auf ein Bewerbungsgespräch.
</p>

<p>
Link zu Amazon: <a href="http://www.amazon.de/gp/product/3442390478?ie=UTF8&#038;tag=thelogbooofma-21&#038;linkCode=as2&#038;camp=1638&#038;creative=6742&#038;creativeASIN=3442390478">Alles über Körpersprache: sich selbst und andere besser verstehen</a><img src="http://www.assoc-amazon.de/e/ir?t=thelogbooofma-21&#038;l=as2&#038;o=3&#038;a=3442390478" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
</p>
</p>


<p>
<strong>Project Management lessons from NASA</strong>

<p>
<a href="http://www.mariosalexandrou.com/blog/project-management-lessons-from-nasa/">http://www.mariosalexandrou.com/blog/project-management-lessons-from-nasa/</a>
</p>

<p>
Jeder einzelne Satz ist Gold wert und sollte in Bezug auf die eigene Situation reflektiert werden. Das ist schon eine Zusammenfassung, müsste ich es noch weiter runterdestillieren, wären das meine wichtigsten Punkte:

<ul>
<li>Most managers succeed on the strength and skill of their staff.</li>
<li>Sometimes the best thing to do is nothing. It is also occasionally the best help you can give. Just listening is all that is needed on many occasions. You may be the boss but, if you constantly have to solve someone&#8217;s problems, you are working for him.</li>
<li>Never assume someone knows something or has done something unless you have asked them. </li>
<li>Never ask management to make a decision that you can make. Assume you have the authority to make decisions unless you know there is a document that states unequivocally that you cannot.</li>
</ul>
</p>
</p>





<p>
<strong>Dale Carnegie &#8211; Wie man Freunde gewinnt: Die Kunst, beliebt und einflussreich zu werden</strong>

<p>
In Teilen etwas angestaubtes, aber sehr lehrreiches Buch, das vor allem sehr menschlich und berührend vermittelt, wie man andere Menschen behandeln sollte. Ich konnte für meine Arbeit als Manager unglaublich viel davon ableiten.
</p>

<p>
Link zu Amazon: <a href="http://www.amazon.de/gp/product/3596170699?ie=UTF8&#038;tag=thelogbooofma-21&#038;linkCode=as2&#038;camp=1638&#038;creative=19454&#038;creativeASIN=3596170699">Wie man Freunde gewinnt: Die Kunst, beliebt und einflussreich zu werden</a><img src="http://www.assoc-amazon.de/e/ir?t=thelogbooofma-21&#038;l=as2&#038;o=3&#038;a=3596170699" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
</p>
</p>


<p>
<strong>Michael Lopp &#8211; Managing Nerds</strong>

<p>
Zu guter Letzt noch ein lesenswerter Aufsatz für alle, die nicht nur irgendwen, sondern ein Team von Nerds managen dürfen: <a href="http://www.randsinrepose.com/archives/2011/01/17/managing_nerds.html">http://www.randsinrepose.com/archives/2011/01/17/managing_nerds.html</a>
</p>

</p>]]></content:encoded>
			<wfw:commentRss>/2011/01/21/literatur-fuer-manager/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Neues Projekt: Platform Health Viewer</title>
		<link>/2011/01/11/neues-projekt-platform-health-viewer/</link>
		<comments>/2011/01/11/neues-projekt-platform-health-viewer/#comments</comments>
		<pubDate>Tue, 11 Jan 2011 08:16:53 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=247</guid>
		<description><![CDATA[
Platform Health Viewer (kurz PHV) ist mein aktuelles Ruby on Rails Hobbyprojekt.


Sobald es einen stabilen Zustand erreicht, wird dieses Tool das Sammeln und Visualisieren verschiedener statistischer Daten, wie sie typischerweise von Internetplattformen erzeugt werden, schnell und leichtgewichtig ermöglichen. Beispiele für diese Daten sind Dinge wie die CPU Last einzelner Systeme, Benutzerlogins, Anzahl der Seitenaufrufe usw.


Die [...]]]></description>
			<content:encoded><![CDATA[<p>
Platform Health Viewer (kurz PHV) ist mein aktuelles Ruby on Rails Hobbyprojekt.
</p>
<p>
Sobald es einen stabilen Zustand erreicht, wird dieses Tool das Sammeln und Visualisieren verschiedener statistischer Daten, wie sie typischerweise von Internetplattformen erzeugt werden, schnell und leichtgewichtig ermöglichen. Beispiele für diese Daten sind Dinge wie die CPU Last einzelner Systeme, Benutzerlogins, Anzahl der Seitenaufrufe usw.
</p>
<p>
Die Applikation basiert in erster Linie auf <a href="http://rubyonrails.org/">Rails</a>, der HTTP Server für die Datenanlieferung ist in <a href="http://nodejs.org/">node.js</a> geschrieben, die Weboberfläche nutzt sehr ausgiebig <a href="http://jquery.com/">jQuery</a> und für die Erzeugung von SVG Graphen die <a href="http://raphaeljs.com/">Raphaël</a> Bibliothek. Massendaten werden in SQL gespeichert, weitere Daten liegen in einer <a href="http://couchdb.apache.org/">CouchDB</a>.
</p>
<p>
Der Projektcode ist auf Github abrufbar unter <a href="https://github.com/ManuelKiessling/PlatformHealthViewer">https://github.com/ManuelKiessling/PlatformHealthViewer</a>.
</p>
<p>
Das folgende Video ist eine kurze Einführung zur aktuellen Alphaversion des Projekts. Es enthält außerdem eine lustige Sprecherstimme und eine sehr kreative Interpretation der englischen Grammatik.
</p>

<object width="640" height="385"><param name="movie" value="http://www.youtube.com/v/HI6SRqz_3D0?fs=1&amp;hl=de_DE"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/HI6SRqz_3D0?fs=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="640" height="385"></embed></object>

<br />
<p />
<p>

Im Folgenden das ins Deutsche übersetzte Transskript des Videos:

<blockquote>
Hi. Platform Health Viewer &#8211; oder PHV &#8211; ist mein aktuellen Hobbyprojekt.
<p/>
Ich brauche eine einfache und leichtgewichtige Möglichkeit, die verschiedenen Kennzahlen der Webseite, für die ich verantwortlich bin, zu sammeln und zu visualisieren. Sachen wie die CPU Perfomance wichtiger Systeme, Userlogins, HTTP Anfragen.
<p/>
Deshalb habe ich angefangen mit Ruby on Rails, jQuery, CouchDB und node.js zu experimentieren, und hier ist eine frühe Alphaversion, die ich demonstrieren möchte.
<p/>
Mein Hauptziel ist es, den Prozess von der Einspeisung der Daten in das System bis hin zu ihrer grafischen Visualisierung so einfach wie möglich zu gestalten.
<p/>
Um Daten in das System zu bekommen, benötigt man lediglich einen HTTP Aufruf, was es sehr einfach macht, die Daten der unterschiedlichsten Quellen zu sammeln.
<p/>
Probieren wir ein Beispiel aus. I möchte die CPU Performance meiner lokalen Maschine visualisieren.
<p/>
Ich werde diese Daten bekommen, indem ich einen Standard Unix Befehl verwende: sar.
<p/>
Dies ist ein wichtiger Aspekt meines Ansatzes: Für den Platform Health Viewer spielt es überhaupt keine Rolle, woher die Daten stammen &#8211; man ist bei den Mitteln der Datenbeschafung völlig frei. Dadurch kann man wirklich alles in das System übermitteln, angefangen bei allgemeinen Daten wie der CPU Last bis hin zu äußerst individuellen Sachen wir den Logins auf der eigenen Webseite.
<p/>
Ok, so bekomme ich den CPU &#8220;usr&#8221; Wert auf meiner OS X Kommandozeile:
<p/>
sar 1 1| grep Average| cut -b 14-15
<p/>
Fein, das wird&#8217;s tun.
<p/>
Wie liefern wir diese Werte in das System? Es reicht ein simpler HTTP Post Aufruf mithilfe von curl:
<p/>
curl &#8211;data &#8220;event[value]=`sar 1 1| grep Average| cut -b 14-15`&#038;event[source]=macbook&#038;event[name]=cpu_usr_percentage&#8221; http://localhost:3000/queue_event
<p/>
Wie man sieht, die Nutzdaten des Aufrufs bestehen aus lediglich drei Parametern: Der Quelle des Events, dem Namen des Events, und seinem Wert.
<p/>
Noch mal: Man ist an dieser Stelle komplett flexibel, man ist nicht gezwungen Eventnamen und -quellen zuvor im PHV zu konfigurieren &#8211; man definiert diese einfach im Moment der Dateneinlieferung, das das System akzeptiert diese. Wir werden gleich sehen, wie man mit den verschiedenen Events, die in das System geschrieben werden, sinnvoll umgeht.
<p/>
Ok, ich verwende jetzt ein kleines Hilfsskript das ich geschrieben habe, um die CPU sys, idle, usr und nice Werte in das System zu liefern:
<p/>
cat script/agents/macosx/cpu_overview_percent.sh
<p/>
Wie man sieht, geschieht alles unter der Verwendung normaler Unixbefehle.
<p/>
Starten wir also das Skript:
<p/>
bash ./script/agents/macosx/cpu_overview_percent.sh http://localhost:3000/ macbook
<p/>
Ich übergebe hier nur zwei Parameter, die URL zum Platform Health Viewer (der für diese Demonstration auf demselben Host läuft), und den Namen meiner Eventquelle, die ich &#8220;macbook&#8221; nenne. Eventnamen und -werten kommen direkt aus dem Hilfsskript.
<p/>
Man sieht, wie das Skript alle vier CPU Kennzahlen in das System liefert. Schauen wir uns diese Daten innerhalb des Platform Health Viewer an.
<p/>
Nun, das Dashboard ist nach wie vor leer, da wir noch keinerlei Visualisierungen definiert haben. Aber der &#8220;Tageditor&#8221; zeigt ebenfalls noch keinerlei Events an. Das liegt daran, dass die in das System eingelieferten Events noch nicht zu sogenannten Eventtypen normalisiert wurden.
<p/>
Dies ist bewusst ein zusätzlicher Schrit, denn es erlaubt dem System, eingehende Events so schnell wie möglich in die Datenbank speichern zu können, ohne sie in Hinblick auf Name und Quelle normalisieren zu müssen. Diese Normalisierung erledigt ein Rake Task:
<p/>
rake queue:convert
<p/>
Dieser Task liest die Events aus der Anlieferungs-Queue, erzeugt bei Bedarf neue Eventtypen, oder verbindet die Events mit bereits existierenden Eventtypen, falls diese bereits existieren. Abschliessend wird die Anlieferungsqueue geleert.
<p/>
Zurück im Tageditor, können wir die vier Eventtypen nun sehen.
<p/>
Ein Eventtyp ist die Kombination einer Eventquelle und eines Eventnamen, also ist &#8220;macbook &#8211; cpu_idle_percentage&#8221; ein Eventtyp.
<p/>
Schauen wir nun, wie wir den Tageditor verwenden können um etwas sinnvolles zu basteln. Das Gruppieren von einem oder mehreren Eventtypen unter einem Tag macht unsere eingelieferten Daten visualisierbar. Ich bin übrigens nicht ganz glücklich mit dem Begriff &#8220;Tag&#8221;, vielleicht fällt mir da noch etwas besseres ein.
<p/>
Wie auch immer, erzeugen wir nun einen einfachen Tag den wir benutzen können, um genau einen Wert zu visualisieren.
<p/>
Ich werde diesen Tag &#8220;macbook_cpu_usr&#8221; nennen. In diesen laufen dann alle Events, deren Quelle &#8220;macbook&#8221; und deren Name &#8220;cpu_usr_percentage&#8221; lautet. Ich könnte diese Parameter auch in die Textbox eintippen, aber es ist einfacher sie schlicht per Drag&#038;Drop in das Formular zu ziehen.
<p/>
Ok, fügen wir diesen Tag also hinzu.
<p/>
Wir haben nun also einen ersten Tag, und um zu sehen, ob er wie erwartet funktioniert, kann ich die Werte der zugeordneten Events in einer Vorschau kontrollieren.
<p/>
Liefern wir jetzt ein paar weitere Werte in das System und schauen, ob sie dann hier angezeigt werden.
<p/>
Ok, ich starte also mein Hilfsskript erneut um neue Werte in den Server zu liefern, und starte dann wiederum den Rake Task um die Werte zu normalisieren.
<p/>
Ein erneuter Klick auf &#8220;Show latest events&#8221; zeigt diese neuen Werte nun an.
<p/>
Ich starte die Datenanlieferung jetzt in einer Schleife, um viele Werte zu erhalten.
<p/>
Ok, wir haben nach wie vor keine Datenvisualisierung, also gehen wir das jetzt an. Ich wechsle in&#8217;s Dashboard und füge einen Frame hinzu, dies ist ein Container der unsere Graphen enthalten wird.
<p/>
Ein Frame ist die Visualisierung aller Werte die mit einem Tag verbunden sind, also muss ich den Namen des Tags angeben, das ich mit diesem Frame visualisieren möchte.
<p/>
&#8220;Add frame&#8221;, und los geht&#8217;s. Ein einfacher Liniengraph, der einen meiner CPU Werte repräsentiert. Der Graph ist im Übrigen ein SVG, erzeugt mit Raphael, einer fantastischen JavaScript Bibliothek.
<p/>
Und dank jQuery kann ich diesen Graphen frei bewegen und skalieren.
<p/>
Erzeugen wir nun einen Graphen für alle meine CPU Werte. Zurück im Tageditor ziehe ich nun alle meine Eventtypen zusammen.
<p/>
Ich kann übrigens Tags erzeugen, indem ich Eventquellen und -namen mit bereits existierenden Tags kombiniere, wie man hier sieht.
<p/>
Ich überprüfe alle Werte die meinem neuen Tag zugeordnet sind, und dort sieht man alle CPU Werte, die mein Skript gesammelt hat.
<p/>
Wieder zurück im Dashboard gehe ich her und erzeuge einen weiteren Frame, für meinen neuen Tag. Wie man sieht, enthält dieser Frame vier Liniengraphen und zeigt mir so eine hübsche Übersicht meiner kompletten CPU Performance. Natürlich wird hier eine Legende benötigt, etwas das bisher noch nicht implementiert ist.
<p/>
Nun, das ist es, das ist der aktuelle Stand des Projekts. Ich würde mich sehr über Feedback freuen, der Code kann auf Github geforkt werden oder schreibt mir eine Mail.
<p/>
Danke für&#8217;s Interesse.
</blockquote>

</p>]]></content:encoded>
			<wfw:commentRss>/2011/01/11/neues-projekt-platform-health-viewer/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>New project: Platform Health Viewer</title>
		<link>/2011/01/11/platform-health-viewer/</link>
		<comments>/2011/01/11/platform-health-viewer/#comments</comments>
		<pubDate>Tue, 11 Jan 2011 07:39:48 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">/?p=233</guid>
		<description><![CDATA[
Platform Health Viewer is my current Ruby on Rails pet project.


Once stable, it will allow users to easily collect and visualize different types of statistical data which is typically produced by internet platforms, like CPU performance, user logins, HTTP requests etc.


The main application is build on Rails, the server used for data collection is written [...]]]></description>
			<content:encoded><![CDATA[<p>
Platform Health Viewer is my current Ruby on Rails pet project.
</p>
<p>
Once stable, it will allow users to easily collect and visualize different types of statistical data which is typically produced by internet platforms, like CPU performance, user logins, HTTP requests etc.
</p>
<p>
The main application is build on <a href="http://rubyonrails.org/">Rails</a>, the server used for data collection is written in <a href="http://nodejs.org/">node.js</a>, the web interface makes heavy use of <a href="http://jquery.com/">jQuery</a> and uses <a href="http://raphaeljs.com/">Raphaël</a> to create SVG graphs. Mass data is saved in a SQL db, other data is stored using <a href="http://couchdb.apache.org/">CouchDB</a>.
</p>
<p>
The project&#8217;s code is hosted on Github at <a href="https://github.com/ManuelKiessling/PlatformHealthViewer">https://github.com/ManuelKiessling/PlatformHealthViewer</a>.
</p>
<p>
This video is a short introduction to the current alpha version of the project. A funny voice and lots of grammatical shortcomings are included for free:
</p>

<object width="640" height="385"><param name="movie" value="http://www.youtube.com/v/HI6SRqz_3D0?fs=1&amp;hl=de_DE"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/HI6SRqz_3D0?fs=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="640" height="385"></embed></object>

<br />
<p />
<p>

Transcription of the video:

<blockquote>
Hi. Platform Health Viewer &#8211; or PHV &#8211; is my current pet project.
<p/>
I need an easy and lightweight way to collect and visualize the different key performance indicators of the web platform I&#8217;m responsible for &#8211; stuff like CPU performance of important systems, user logins, http requests.
<p/>
So I started to play around with Ruby on Rails, jQuery, CouchDB and node.js, and here is an early alpha I would like to demonstrate.
<p/>
My primary goal was to make the process from feeding data into the system to visualizing that data as simple as possible.
<p/>
In order to get data into the system, all you need to make is an HTTP call, which makes it very easy to collect data from very different sources.
<p/>
Let&#8217;s try an example. I would like to visualize the cpu usage of my local machine.
<p/>
I&#8217;m going to collect this data using a standard unix command, sar.
<p/>
That&#8217;s an important aspect of my approach: It doesn&#8217;t play any role for the Platform Health Viewer where the data comes from, you&#8217;re completely free to choose how to collect it.
This way you can feed really anything into the system, from generic data like CPU load to highly individual stuff like the user logins of your specific web site.
<p/>
Ok, here is how I can get my cpu &#8220;usr&#8221; value on my OS X command line:
<p/>
sar 1 1| grep Average| cut -b 14-15
<p/>
Great, that will do.
<p/>
How do we push these values into the system? It&#8217;s a simple http post request using curl:
<p/>
curl &#8211;data &#8220;event[value]=`sar 1 1| grep Average| cut -b 14-15`&#038;event[source]=macbook&#038;event[name]=cpu_usr_percentage&#8221; http://localhost:3000/queue_event
<p/>
As you can see, the payload of the post requests is just 3 parameters: the source of the event, the name of the event, and its value.
<p/>
Again, you&#8217;re completely free here, you don&#8217;t need to configure event names and sources inside PHV &#8211; just define them when pushing data into the system, it will happily accept it. We will see in a moment how to make sense of different events that were pushed into the system.
<p/>
Ok, let&#8217;s use a small helper script I wrote in order to feed the CPU sys, idle, usr and nice values into my system:
<p/>
cat script/agents/macosx/cpu_overview_percent.sh
<p/>
As you can see, this is all done using only standard unix commands.
<p/>
Let&#8217;s start the script:
<p/>
bash ./script/agents/macosx/cpu_overview_percent.sh http://localhost:3000/ macbook
<p/>
I&#8217;m just providing two parameters here, the URL to my platform health viewer installation, which resides on the same host for this demo, and the source name, which I call &#8220;macbook&#8221;.
<p/>
As you can see, my script pushes all four CPU usage values into the system. Now let&#8217;s have a look at this data within platform health viewer.
<p/>
Well, the Dashboard is still empty, because we did not yet define any visualizations. But the &#8220;Tageditor&#8221; doesn&#8217;t show any events, too. This is because the events I pushed into the system have not yet been normalized to event-types.
<p/>
This is an additional step, because it will allow the system to push incoming events into the database as quickly as possible without the need to normalize those events regarding their name and source. This normalization is done using a rake task:
<p/>
rake queue:convert
<p/>
This task reads the events from the incoming queue, creates new event-types as needed, or connects the event values with existing event types if they already exists. It then deletes the incoming queue.
<p/>
Getting back to our tageditor, we can now see our 4 event types.
<p/>
An event type is the combination of an event source and an event name, so &#8220;macbook &#8211; cpu_idle_percentage&#8221; is one event type.
<p/>
Let&#8217;s see how we can use the tag editor to create something useful. Grouping one or more event types into a tag is what makes our data suitable for visualization. I&#8217;m not quite happy with the term &#8220;tag&#8221; by the way, maybe I will come up with something better.
<p/>
Anyway, let&#8217;s create a very simple tag which we can use to visualize exactly one value.
<p/>
I&#8217;m going to name my tag &#8220;macbook_cpu_usr&#8221;. It will hold all events whose source matches &#8220;macbook&#8221;, and whose name matches &#8220;cpu_usr_percentage&#8221;. I could type those parameters into the text box, but it&#8217;s easier to just drag&#8217;n'drop them there.
<p/>
Ok, let&#8217;s add this tag.
<p/>
Now we have this first tag, and to check if it works as expected, I can preview the values of the matching events.
<p/>
Let&#8217;s push some new values into our system and check if they are visible here.
<p/>
Ok, I&#8217;m starting my helper skript again in order to post new values to the server, and I start my rake task in order to normalize these values.
<p/>
Clicking again on &#8220;Show latest events&#8221; now shows these values.
<p/>
I will now start data push and normalization in a loop in order to get a lot of values.
<p/>
Ok, we still have no data visualization, so let&#8217;s do this now. Let&#8217;s switch to the Dashboard and add a frame, which is a container that will hold our graph.
<p/>
A frame is the visualization of all values connected to a tag, so I need to provide the name of the tag I want to visualize with this frame.
<p/>
&#8220;Add frame&#8221;, and here we go. A simple line graph representing one of my CPU values. The graph is actually an SVG, created using Raphael, an awesome JavaScript library.
<p/>
And thanks to jQuery, I can freely move and resize the graph.
<p/>
Let&#8217;s create a graph with all my CPU values in it. Back to the Tageditor, I&#8217;m going to drag all my values together.
<p/>
I can also create tags by combining event-sources and -names with already existing tags, as you can see here.
<p/>
Let&#8217;s check the values of my new tag, and there are all the different CPU values my script collects.
<p/>
Back to the Dashboard, I&#8217;m going to create another frame for my new tag. As you can see, this one contains 4 linegraphs and gives me a nice overview of my system&#8217;s CPU performance. Of course, a graph legend is needed, something that&#8217;s not yet implemented.
<p/>
Well, that&#8217;s it, that&#8217;s the current state of this project, I would love to hear your feedback, you can fork the code on github and drop me an e-mail.
<p/>
Thanks for your interest.
</blockquote>

</p>]]></content:encoded>
			<wfw:commentRss>/2011/01/11/platform-health-viewer/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Testgetriebene Administration &#8211; test driven administration</title>
		<link>/2010/09/01/testgetriebene-administration-test-driven-administration/</link>
		<comments>/2010/09/01/testgetriebene-administration-test-driven-administration/#comments</comments>
		<pubDate>Wed, 01 Sep 2010 22:57:25 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=188</guid>
		<description><![CDATA[
Ich hatte tatsächlich einmal eine ganz eigene Idee. Und sie war gut, auch nachdem ich sie mehrmals durchgekaut und von allen Seiten beleuchtet hatte.



Wieso eigentlich sollte man die Prinzipien und Methodiken von testgetriebener Softwareentwicklung nicht auch auf den Bereich der IT-Systemadministration übertragen? Also in aller Kürze: Ich definiere Tests, die das vom noch zu implementierenden [...]]]></description>
			<content:encoded><![CDATA[<p>
Ich hatte tatsächlich einmal eine ganz eigene Idee. Und sie war gut, auch nachdem ich sie mehrmals durchgekaut und von allen Seiten beleuchtet hatte.
</p>

<p>
Wieso eigentlich sollte man die Prinzipien und Methodiken von testgetriebener Softwareentwicklung nicht auch auf den Bereich der IT-Systemadministration übertragen? Also in aller Kürze: Ich definiere Tests, die das vom noch zu implementierenden System erwartete Verhalten prüfen, sehe zu wie diese Tests fehlschlagen, und erfülle dann schrittweise diese Tests, indem ich das System aufbaue. Test driven administration &#8211; TDA.
</p>

<p>
Da war ich ganz alleine drauf gekommen, und ich war sehr stolz.
</p>

<p>
Dann habe ich gegoogelt. Die Idee existiert seit mindestens 2006.
</p>

<p>
Aber hey, gut ist die Idee trotzdem, also beschreibe ich sie hier.
</p>

<p>
Warum möchte man testgetrieben administrieren? Die Gründe sind dieselben wie bei testgetriebener Entwicklung: Habe ich Tests, bin ich gegen Regression geschützt, d.h. ändert ein Stück Code / ein System sein Verhalten aufgrund von Änderungen, weisen mich die Tests darauf hin.
</p>

<p>
Gehe ich test<em>getrieben</em> vor, sind die Tests nicht irgendwas, das ich ganz unbedingt machen sollte, das aber doch am Ende runterfällt, sondern sie sind garantiert vorhanden. Mit den bekannten angenehmen Begleiterscheinungen, dass die Tests einen zwingen, sich Gedanken darüber zu machen, wie das Ziel eigentlich beschaffen sein soll, und automatisch dazu führen, die Lösung schlank und elegant umzusetzen.
</p>

<p>
Code und IT-Systeme sind aber nicht dasselbe, wie würde man also in der Praxis konkret vorgehen? Hier mein Vorschlag.
</p>

<p>
Zuerst benötigt man ein Testwerkzeug. Um in der Softwareentwicklung Unittests zu bauen, benutzt man Tools aus der xUnit Familie wie JUnit oder phpUnit. Das Äquivalent zu diesen Tools in der Systemadministration sind Monitoringsysteme wie Nagios oder Zabbix.
</p>

<p>
In der Softwareentwicklung formuliert man Unittest so, dass man eine kleine Einheit des Gesamtsystems, also in der Regel die einzelnen Methoden einer Klasse, mit einer gewissen Erwartungshaltung (&#8220;wenn ich diese Parameter reingebe, erwarte ich jenen Rückgabewert&#8221;) aufruft, und dann die erwartete Rückgabe mit der tatsächlichen vergleicht.
</p>

<p>
Was wäre dementsprechend &#8220;erwartetes Verhalten&#8221; bei einem IT-System? Nehmen wir an, die Anforderungen lauten wie folgt:
</p>

<blockquote>
Benötigt wird ein Linux-System, welches unter der IP 123.456.789.000 einen Webserver bereitstellt, und die Festplattengröße des Systems soll 100 GB betragen.
</blockquote>

<p>
In der Realität wären die Anforderungen natürlich umfangreicher, aber ich halte das Beispiel einfach.
</p>

<p>
Aus den Anforderungen lässt sich das gewünschte Verhalten ableiten:

<ul>
	<li>Bei einem Ping auf 123.456.789.000 muss eine Antwort erfolgen</li>
	<li>Die Abfrage des Betriebssystems unter dieser IP muss &#8220;Linux&#8221; ergeben</li>
	<li>Ein HTTP Request gegen diese IP unter Port 80 muss eine HTTP Antwort zur Folge haben</li>
	<li>Bei der Abfrage der Festplattengröße muss ein Wert von 100 GB zurückgeliefert werden</li>
</ul>
</p>

<p>
Daraus wiederum kann man im Monitoringsystem Tests formulieren. Diese lässt man einmalig laufen, um zu verifizieren, dass sie tatsächlich fehlschlagen. Und dann beginnt man damit, ein System aufzusetzen, das die Testbedingungen erfüllt, bis schliesslich alle Tests &#8220;grün&#8221; sind.
</p>

<p>
Das ist der Kern der Idee. Im weiteren Verlauf überwacht man die Tests regelmäßig (was man mit einem Monitoringsystem ja eh tut), und hat damit das Thema Continuous Integration gleich mit erschlagen. Ansonsten geht man genauso wie auch beim TDD vor: Möchte man Änderungen an einem System vornehmen, passt man zuerst die Tests an, verifiziert dass sie fehlschlagen, und ändert dann das System, um die Tests wieder zu erfüllen.
</p>
</p>]]></content:encoded>
			<wfw:commentRss>/2010/09/01/testgetriebene-administration-test-driven-administration/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>&lt;angular/&gt; &#8211; ein radikal neuer Weg, Ajax Applikationen zu schreiben</title>
		<link>/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/</link>
		<comments>/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/#comments</comments>
		<pubDate>Wed, 25 Aug 2010 09:21:13 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=158</guid>
		<description><![CDATA[<em>&#60;angular/&#62;</em> bringt JavaScript-Logik und das dazugehörige HTML Dokument deutlich näher zueinander als bestehende Frameworks wie beispielsweise <em>jQuery</em>. Es entfernt gleich mehrere Ebenen an Abstraktion, die ein Stück JavaScript-Code und das DOM-Element, auf welchem der Code operieren möchte, voneinander trennen.]]></description>
			<content:encoded><![CDATA[<p>
JavaScript, Ajax und DHTML sind nicht wirklich meine Welt. Zum einen, weil ich einfach grundsätzlich eher mit dem Backend einer Software als mit dem Frontend zu tun habe, zum anderen, weil ich immer schon das ungute Gefühl hatte, in diesem Bereich muss man einfach deutlich zu viel Code produzieren um damit dann gefühlt deutlich zu wenig zu erreichen.
</p>

<p>
Umso mehr hat <em>&lt;angular/&gt;</em> mein Interesse geweckt. Die Autoren versprechen:
</p>

<blockquote>Write less code. A lot less. Forget about writing all that extra JavaScript to handle event listeners, DOM updates, formatters, and input validators. <angular/> comes with autobinding and built-in validators and formatters which take care of these. And you can extend or replace these services at will. With these and other services, you’ll write about 10x less code than writing your app without <angular/>.</blockquote>

<p>
In einem Video versucht Miško Hevery zu erklären, was <em>&lt;angular/&gt;</em> eigentlich ist, und stellt fest dass diese Erklärung schwierig ist:
</p>

<p align="center">
<object width="270" height="176"><param name="movie" value="http://www.youtube.com/v/0iQCLlu1dko?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/0iQCLlu1dko?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="270" height="176"></embed></object>
</p>

<p>
Mein Verständnis ist in erster Linie: <em>&lt;angular/&gt;</em> bringt JavaScript-Logik und das dazugehörige HTML Dokument deutlich näher zueinander als bestehende Frameworks wie beispielsweise <em>jQuery</em>. Es entfernt gleich mehrere Ebenen an Abstraktion, die ein Stück JavaScript-Code und das DOM-Element, auf welchem der Code operieren möchte, voneinander trennen.
</p>

<p>
Während man bei traditioneller JavaScript-Programmierung stets gezwungen ist, explizit das HTML Dokument mit JavaScript-Code zu manipulieren, macht <em>&lt;angular/&gt;</em> die Verbindung zwischen Logik und HTML-Repräsentation implizit &#8211; etwas, das mich übrigens stark an die Mechanik erinnert, die Max Winde für <a href="http://172.16.111.147/wordpress/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/">siqqel</a> einsetzt.
</p>

<p>
Spielen wir ein einfaches Beispiel durch, welches ich dank der rein clientseitigen Arbeitsweise von <em>&lt;angular/&gt;</em> problemlos direkt hier im Post zum laufen bringen kann.
</p>

Zuerst binde ich die <em>&lt;angular/&gt;</em> JavaScript Bibliothek ein:

<code>&lt;script type="text/javascript"
 src="http://angularjs.org/ng/js/angular-debug.js" ng:autobind&gt;
&lt;/script&gt;
</code>

Nun definiere ich ein Input Feld sowie einen <em>&lt;angular/&gt;</em>-Platzhalter, welche beide in einer Beziehung zueinander stehen:

<code>Dein Name: &lt;input type="text" name="deinname" value="Manuel"/&gt;
&lt;br /&gt;
Hallo {<span class="nospace" />{deinname}<span class="nospace" />}!
</code>

<p>
Wodurch entsteht diese Beziehung? Sie ist dank Autobinding implizit, und mappt alleine aufgrund des Formularfeldnamens und des Platzhalternamens beide zusammen.
</p>

<p>
Das Ergebnis sieht man hier &#8211; einfach den Inhalt des Textfeldes ändern:
</p>

<p>
<script type="text/javascript" src="http://angularjs.org/ng/js/angular-debug.js" ng:autobind>
</script>

Eingabe: <input type="text" name="yourname" value="Welt"/> &#8211; Hallo {{yourname}}!
</p>

<p>
Dieses Beispiel geht natürlich maximal als Spielerei durch. Es zeigt aber schon, wieviel weniger Code nötig ist, als dies mit einem klassischen Framework der Fall wäre.
</p>

Ein etwas praxisnäheres Beispiel findet man unter <a href="http://angularjs.org/Cookbook:BasicForm">http://angularjs.org/Cookbook:BasicForm</a>. In diesem Beispiel geht es um den klassischen Fall, Eingaben in ein Textfeld per JavaScript clientseitig zu validieren &#8211; dies ist einfach möglich durch folgende schlanke und ausdrucksstarke Syntax:

<code>&lt;input type="text" name="user.address.state" size="2"
 ng:required ng:validate="regexp:/^\w\w$/"/&gt;
</code>

Für weitere Informationen verweise ich auf <a href="http://angularjs.org/Overview">http://angularjs.org/Overview</a>.]]></content:encoded>
			<wfw:commentRss>/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Tutorial: Testgetriebene Entwicklung mit PHP</title>
		<link>/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/</link>
		<comments>/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/#comments</comments>
		<pubDate>Mon, 23 Aug 2010 16:24:50 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=107</guid>
		<description><![CDATA[Testgetriebene Entwicklung (test driven development) ist eine Arbeitsmethodik, die Softwareentwickler dabei unterstützt, wichtige Qualitätsprinzipien bei der Erstellung von Code zu erreichen. Dieses Tutorial beschreibt Schritt für Schritt die Motivation, einem testgetriebenen Ansatz zu folgen, und stellt die notwendigen Werkzeuge und Techniken vor.]]></description>
			<content:encoded><![CDATA[<h3>Einleitung</h3>
Testgetriebene Entwicklung (test driven development) ist eine Arbeitsmethodik, die Softwareentwickler dabei unterstützt, wichtige Qualitätsprinzipien bei der Erstellung von Code zu befolgen:

<ul>
	<li><strong>Lose Kopplung (loose couping)</strong> &#8211; weil man beim Schreiben von Unittests, dem zentralen Werkzeug der Methodik, ganz automatisch dazu verführt wird, innerhalb der Tests von Codeunits (Klassen, Methoden usw.) auszugehen, die möglichst wenige Abhängigkeiten zu anderen Modulen haben &#8211; einfach deshalb, weil das Schreiben der Tests dann zu nervig wird.</li>
	<li><strong>Saubere Trennung von Verantwortlichkeiten (separation of concerns)</strong> &#8211; aus ganz ähnlichen Gründen wie der erste Punkt: Jeder Test testet genau ein gewünschtes Verhalten, und dies führt ganz automatisch dazu, dass man später den Code, der die Tests erfüllen muss, in sauber voneinander getrennte und logisch strukturierte Einheiten teilt.</li>
	<li><strong>Schlanke Lösungen</strong> &#8211; testgetrieben bedeutet eben auch, dass man von den Tests getrieben ist, im besten Sinne: Man tut alles, um einen noch fehlschlagenden Test zu erfüllen; aber eben auch nur genau das und nicht mehr. Salopp gesagt: Man programmiert nicht mehr &#8220;einfach rum&#8221;, sondern arbeitet äußerst zielgerichtet und erzeugt Code, der nur genau das tut was er tun muss, was ganz automatisch zu einer schlanken und damit eleganten Lösung führt, in der sich zum Beispiel Bugs sehr viel schlechter verstecken können.</li>
</ul>

Darüber hinaus hat der testgetriebene Ansatz weitere nützliche Nebeneffekte:

<ul>
	<li>Die im Laufe der Zeit aufgebaute Sammlung von Unittests kann man benutzen, um die mit Tests versehenen Units automatisiert immer wieder testen zu können, zum Beispiel um beim Mergen eines Entwicklungszweigs mit einem anderen Zweig (oder auch nach jedem einzelnen Commit in ein Versionskontrollsystem) sicherzustellen, dass sich alle Units auch nach der Zusammenführung zweier Entwicklungslinien noch so verhalten wie erwartet. Das Stichwort für weiterführende Lektüre ist hier die <em>Kontinuierliche Integration</em> (continuous integration).</li>
	<li>Ein Unittest ist in der Praxis nicht nur ein Stück Code, sondern immer auch Dokumentation des erwarteten Verhaltens eines Systems &#8211; zumindest in einer für Programmierer lesbaren Form. Um als Unbeteiligter ein Stück Code oder ganze Teile eines Systems kennen zu lernen, ist es häufig effizienter, die dazugehörigen Tests zu lesen, als den Code selbst.</li>
	<li>Hat man erst einmal die Tests komplett geschrieben, welche die noch zu erzeugenden Units testen sollen, ist es sehr einfach, die Arbeit am eigentlichen Code einfach mittendrin auch für längere Zeit zu unterbrechen &#8211; die Tests geben einem sofort einen Anhaltspunkt, wo man &#8220;weiterprogrammieren&#8221; muss, selbst wenn man gedanklich längst aus dem Thema war.</li>
	<li>Testgetrieben zu entwickeln, erzeugt ein gutes Gefühl. Das mag banal klingen, aber es ist ein realer und wichtiger Faktor. Irgendwo habe ich mal eine sehr gute Definition des Begriffs &#8220;legacy code&#8221; gelesen: &#8220;legacy code&#8221; ist Code, vor dem man sich fürchtet &#8211; weil man nicht genau weiss was er tut, und deshalb Angst hat, ihn zu verändern. Testgetriebene Entwicklung ist die beste Vorsorge gegen legacy code &#8211; man weiss, es gibt eine Instanz die überwacht und aussagt, was der Code tun soll. Es wächst das Vertrauen in den eigenen Code und damit auch in die eigenen Fähigkeiten.</li>
</ul>

Die Unterteilung in zentrale Effekte und Nebeneffekte ist subjektiv. Ich habe die Erhöhung der Codequalität an sich für mich als wichtiger erlebt als zum Beispiel die Tatsache, dank der sich entwickelnden Testsammlung Regressionstests durchführen zu können. Geschadet hat mir jedenfalls noch kein einziger durch testgetriebene Entwicklung entstandener Effekt.

<h3>Voraussetzungen</h3>
Was benötigt man nun, um in PHP testgetrieben zu entwickeln? Im Wesentlichen vier Dinge:

<ul>
	<li>Eine <strong>Arbeitsmethodik</strong>, um effizient zu testgetrieben entwickeltem Code zu kommen</li>
	<li>Ein <strong>Organisationsprinzip</strong>, um Tests und zu testenden Code sinnvoll strukturieren zu können</li>
	<li>Ein PHP <strong>Framework</strong>, um Testfälle schreiben zu können</li>
	<li>Ein <strong>Tool</strong>, um Testfälle ausführen und auswerten zu können</li>
</ul>

Beginnen wir mit den letzten beiden Punkten, denn dank der Maßstäbe setzenden Arbeit von Sebastian Bergmann (<a href="http://sebastian-bergmann.de/">http://sebastian-bergmann.de/</a>) existiert ein Softwareprojekt, welches beide Anforderungen hervorragend erfüllt und längst der de-facto Standard für Unittesting unter PHP ist: PHPUnit.

<p />

Unter <a href="http://www.phpunit.de/manual/current/en/installation.html">http://www.phpunit.de/manual/current/en/installation.html</a> befindet sich eine ausführliche Anleitung für die in der Regel sehr einfache Installation.

<p />

PHPUnit ist sowohl ein Framework aus PHP Klassen, die es erlauben, Unittests für den eigenen PHP Code zu schreiben, als auch Kommandzeilen-Werkzeug, um die eigenen Tests auszuführen und in verschiedenen Formaten die Testergebnisse darzustellen.

<p />

Im weiteren Verlauf des Tutorials gehe ich davon aus, dass PHPUnit installiert und funktionsfähig ist.

<p />

Im Mittelpunkt von testgetriebener Entwicklung stehen aber nicht die Werkzeuge, sondern der Arbeitsprozess. Dieser folgt stets diesem Muster:
<ul>
	<li>Schreiben des Tests für eine neu zu implementierende Funktionalität</li>
	<li>Erfüllen des Tests mit so wenig Aufwand wie möglich, so dass dieser fehlerfrei durchläuft</li>
	<li>Überarbeiten des Codes, der den Test erfüllt, so dass dieser keine Duplizierungen enthält, sauber abstrahiert ist, und dem eigenen Code-Style entspricht &#8211; und dabei immer noch den Test erfüllt</li>

</ul>
Diese Schritte werden immer wieder wiederholt, bis man keine neuen sinnvollen Tests mehr findet für die neue Funktionalität.

<p />

Möchte man bereits vorhandene Funktionalität ändern, die bereits mit Tests versehen ist, bedeutet testgetriebene Entwicklung, dass man zuerst die Tests ändert, um das neue erwartete Verhalten widerzuspiegeln, sicherstellt, dass die veränderten Tests fehlschlagen, und dann erst den Code anpasst, um die veränderten Tests wieder zu erfüllen.

<p />

Wäre noch die Frage der Testorganisation zu klären &#8211; einfacher ausgedrückt: Wohin mit den Tests? Meiner Meinung nach ist der einzig wirklich sinnvolle Ansatz, Code und Tests identisch zu strukturieren. Das bedeutet, der Test für die Klasse <em>DefaultUser</em> in

<code>lib/core/user/default_user.php</code>

sollte in der Datei

<code>tests/core/user/default_user_test.php</code>

in der Testklasse <em>DefaultUserTest</em> liegen.

<p />

Aber solange wir noch kein Beispiel für einen Unittest durchgespielt haben, bleibt vieles sehr abstrakt, also beginnen wir den praktischen Teil des Tutorials.

<h3>Ein erstes Beispiel</h3>
Angenommen, wir möchten mithilfe von PHP ein Forum programmieren. Auf die ein oder andere Art und Weise wird diese Software eine Unit enthalten müssen, die eine E-Mail Adresse auf Gültigkeit prüft. Wir haben also eine Erwartungshaltung, was der Code später einmal tun soll. Der Einfachheit halber definieren wir diese Erwartungshaltung in diesem Beispiel so:

<p />
<em>Wenn eine E-Mail Adresse ohne @-Zeichen übergeben wird, dann liefere mir FALSE zurück, sonst TRUE</em>
<p />

Diese Erwartungshaltung gießen wir nun in Form von PHP Code in einen Unittest. Da wir testgetrieben arbeiten, existiert noch keinerlei Code der diese Erwartungen erfüllen könnte.

<p />

Wir geben der Unit, die später einmal unsere formulierte Erwartung erfüllen soll, den Namen <em>Verify</em>. Daraus leitet sich als Klassenname für den Unittest die Bezeichnung <em>VerifyTest</em> ab.

<p />

Wir erzeugen daher folgende Datei:

<p />

<em>tests/verify_test.php</em>

<p />

Und füllen sie mit folgendem Grundgerüst:

<code>&lt;?php
require_once('/usr/lib/php/PHPUnit/Framework.php');

class VerifyTest extends PHPUnit_Framework_TestCase {}
</code>

Dieser Code repräsentiert einen Testcase, der noch keine Tests enthält. Wir inkludieren das PHP-Klassen Framework von PHPUnit, da wir unsere Testcase-Klassen von einer Klasse dieses Frameworks ableiten müssen. Je nach Plattform liegt die zu inkludierende Framework.php auch schon mal unter <em>/usr/share/php/PHPUnit/Framework.php</em>.

<p />

Den Testcase selbst formulieren wir, indem wir eine Klasse definieren, deren Name auf <em>Test</em> endet, und die von <em>PHPUnit_Framework_TestCase</em> erbt.

<p />

Dieser Testcase kann nun mithilfe des PHPUnit Kommandozeilentools ausgeführt werden. Dazu starten wir folgenden Befehl an der Kommandozeile:

<p />

<em>phpunit tests/verify_test.php</em>

<p />

Dadurch erhalten wir die folgende Ausgabe:

<code>PHPUnit 3.4.13 by Sebastian Bergmann.

F

Time: 0 seconds, Memory: 7.25Mb

There was 1 failure:

1) Warning
No tests found in class "VerifyTest".
</code>

PHPUnit wertet den Testlauf als nicht erfolgreich (&#8220;Failure&#8221;), da keinerlei Tests innerhalb des Testcases gefunden wurden. Als nächstes fügen wir daher einen Test hinzu:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

}
</code>

Einen Test innerhalb eines Testcase formuliert man, indem man der Testcase-Klasse eine Methode hinzufügt, deren Name mit <em>test</em> beginnt.
<br />
Innerhalb der Methode schreibt man nun den Code, der notwendig ist, um den oder die Werte von der zu testenden Unit zu bekommen, mithilfe derer man das erwartete Verhalten verifizieren kann.
<br />
Die von der Unit erhaltenen Werte testet man nun gegen eine Behauptung, einen <em>assert</em>: Wir drücken hier also aus, dass der Test erwartet, dass der zu testende Wert FALSE ist.

<p />

Letztendlich muss man sich aber immer bewusst machen: Man möchte Verhalten testen, nicht Daten. Daten drücken nur das Ergebnis eines Verhaltens aus. Entsprechen die tatsächlichen (actual) Daten den erwarteten (expected) Daten, dann entspricht das tatsächliche Verhalten dem im Test erwarteten.

<p />

Nun lassen wir den neu formulierten Testcase erneut durchlaufen, mit folgendem Ergebnis:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

PHP Fatal error:  Class 'Verify' not found in tests/verify_test.php on line 8
</code>

Wenig überraschend beschwert sich PHP (nicht PHPUnit!), dass wir eine Klasse verwenden, die nirgends definiert wurde. Tun wir dies also, indem wir eine Datei <em>lib/verify.php</em> erzeugen und mit folgendem Inhalt füllen:

<code>&lt; ?php

class Verify {}

</code>

Dann muss im Testcase noch sichergestellt werden, dass die Datei mit dieser Klasse auch inkludiert wird:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');
require_once('lib/verify.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

}
</code>

Lassen wir den Testcase nun laufen, ändert sich das Bild:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

PHP Fatal error:  Call to undefined method Verify::checkEmail() in tests/verify_test.php on line 9
</code>

Wir rufen eine Methode auf, die noch nicht existiert, also muss diese implementiert werden:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {}

}
</code>

Nun steht zumindest die Codestruktur komplett, so dass PHPUnit ohne Fatals durchlaufen kann:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

F

Time: 0 seconds, Memory: 7.00Mb

There was 1 failure:

1) VerifyTest::test_falseIfNoAtSign
Failed asserting that  is false.

tests/verify_test.php:10

FAILURES!
Tests: 1, Assertions: 1, Failures: 1.
</code>

Eine Zwischenbemerkung: Das Vorgehen ist hier natürlich sehr kleinschrittig &#8211; ob man die offensichtlichen Dinge wie das Anlegen der benötigten Klassen und Methoden nicht gleich in einem Rutsch macht, bleibt Geschmackssache. Ich persönlich habe Gefallen gefunden an dem Vorgehen, meine ganze Energie in die Tests zu stecken, und dann in einen anderen Modus zu schalten und ganz stupide Schritt für Schritt immer wieder die Implementierung anzupassen und den Testlauf neu zu starten, bis keinerlei Fehler mehr auftreten.

<p />

Wie auch immer, PHPUnit läuft nun wieder ohne PHP Fehler durch, bestätigt aber wenig überraschend, dass die nunmehr vorhandene Code-Unit nicht das Verhalten zeigt, welches wir laut Test von ihr erwarten. Wechseln wir nun also auf die inhaltliche Ebene der Implementierung und sorgen dafür, dass unser Code sich wie gewünscht verhält:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {
    if (!strstr($email, '@')) return FALSE;
  }

}
</code>

Nun besteht unser Testcase alle Tests:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

.

Time: 0 seconds, Memory: 7.00Mb

OK (1 test, 1 assertion)
</code>

Damit wäre der erste Testzyklus komplett. Stellt sich die Frage, ob uns noch weitere Verhaltensweisen für unsere Unit einfallen, die wir von ihr erwarten. Es liegt auf der Hand, dass wir den Positivfall ebenfalls testen wollen, nämlich dass eine E-Mail Adresse mit @-Zeichen als valide erkannt wird. Natürlich würde man in der Realität noch viel mehr Ansprüche an die Validierung einer E-Mail Adresse stellen, aber in diesem Beispiel bleibe ich der Einfachheit halber unrealistisch.

<p />

Eine Faustregel der testgetriebenen Entwicklung lautet, immer nur ein Verhalten pro Test zu überprüfen, anders ausgedrückt &#8220;ein assert pro Test&#8221;. Dies hilft, die einzelnen Tests übersichtlich und nachvollziehbar zu halten, und hat auch ganz praktischen Nutzen, da PHPUnit bei der Ausgabe eines Failures innerhalb eines Tests nicht darauf hinweist, welcher assert genau nicht erfüllt wurde, sondern immer den gesamten Test als fehlgeschlagen zu melden &#8211; hat man einen Test mit 20 asserts geschrieben, wird die Fehlersuche aufwendig.

<p />

Formulieren wir also einen weiteren Test:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');
require_once('lib/verify.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

  public function test_trueIfAtSign() {
    $actual = Verify::checkEmail('manuel@kiessling.net');
    $this-&gt;assertTrue($actual);
  }

}
</code>

Danach sollte man allerdings, obwohl kleinschrittig, auf jeden Fall den Testcase einmal durchlaufen lassen und ihm beim Fehlschlagen zusehen: Auch beim Schreiben von Tests können Fehler passieren, und es kommt vor, dass man einen neuen Test formuliert, der wegen eines Fehlers in der Implementation oder im Test sofort erfüllt wird &#8211; geht man nach dem Schreiben des Tests sofort an die Implementation, ohne zuvor den Test einmal fehlschlagen gesehen zu haben, übersieht man möglicherweise einen Bug in der Implementation oder im Test, wenn man erst dann den Test laufen lässt und dieser dann ohne Fehler durchläuft.
<br />
Dann sorgt gar nicht die eigene Änderung an der Implementation für das funktionieren des Tests, sondern ein Bug, den man aber eben nicht bemerkt.

<p />

Also stellen wir sicher, dass unser neuer Test fehlschlägt:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

.F

Time: 0 seconds, Memory: 7.00Mb

There was 1 failure:

1) VerifyTest::test_trueIfAtSign
Failed asserting that  is true.

tests/verify_test.php:15

FAILURES!
Tests: 2, Assertions: 2, Failures: 1.
</code>

Und nun ändern wir die Implementation, um ihn zu erfüllen:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {
    if (!strstr($email, '@')) return FALSE;
    return TRUE;
  }

}
</code>

Nun laufen beide Tests im Testcase erfolgreich durch:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

..

Time: 0 seconds, Memory: 7.00Mb

OK (2 tests, 2 assertions)
</code>

Das hier beschriebene Beispiel ist natürlich banal, aber im Grunde ist alles Wichtige zur Methodik der testgetriebenen Entwicklung gesagt.

<p />

Aber auch in der eigenen Praxis, auch bei spannenden Projekten, wird man aber immer wieder dem Gefühl begegnen, dass der einzelne Test im Grunde trivial ist. Aber das ist auch völlig in Ordnung: Selbst komplexeste Softwareprojekte sind letztendlich die Verknüpfung kleiner und für sich betrachtet trivialer Funktionseinheiten &#8211; aber aus dem Zusammenspiel dieser vielen einfachen Module ergibt sich die Lösung komplexer Probleme für den Anwender.]]></content:encoded>
			<wfw:commentRss>/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Empfehlung: Barbecue Sauce &#8220;Bone Suckin&#8217; regular thicker style&#8221; von Ford&#8217;s Food</title>
		<link>/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/</link>
		<comments>/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/#comments</comments>
		<pubDate>Tue, 13 Jul 2010 11:17:46 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Recommendations]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=98</guid>
		<description><![CDATA[Phil Ford war Immobilienmakler und hat aus einem Hobby heraus angefangen, diese Sauce zu entwickeln.]]></description>
			<content:encoded><![CDATA[Klar, über Geschmack lässt sich immer streiten, aber über den Geschmack von Barbecue Saucen wahrscheinlich am meisten.
<p/>
Die beste Sauce, die zu probieren ich bisher das Vergnügen hatte, ist jedenfalls völlig zweifelsfrei die &#8220;Bone Suckin&#8217; Sauce regular&#8221; von Ford&#8217;s Food.
<p/>
Phil Ford war Immobilienmakler und hat aus einem Hobby heraus angefangen, diese Sauce zu entwickeln. Sie hat nur wenig Raucharoma und ist nicht scharf, im Vordergrund steht vor allem ein fantastisches Tomatenaroma und eine gewisse Süße, die es aber auch nicht übertreibt. Man muss sich sehr beherrschen, sie nicht direkt auszulöffeln.
<p/>
Die offizielle Homepage zur Sauce ist <a href="http://www.bonesuckin.com/">www.bonesuckin.com</a>, zu beziehen ist sie in Deutschland zum Beispiel über BOS FOOD in Meerbusch:
<p/>
<a href="https://www.bosfood.de/Bone_Suckin%B4_Sauce_regular_Barbecue_Sauce_dickfluessig_Ford%B4s_Food_454_g_11;20504;bone+suckin%2C%2C;0.html">Bone Suckin´ Sauce regular, Barbecue-Sauce (dickflüssig), Ford´s Food, 454 g</a> bei bosfood.de.]]></content:encoded>
			<wfw:commentRss>/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Empfehlung: &#8220;Der Kuchenladen&#8221; in Berlin</title>
		<link>/2010/07/13/empfehlung-der-kuchenladen-in-berlin/</link>
		<comments>/2010/07/13/empfehlung-der-kuchenladen-in-berlin/#comments</comments>
		<pubDate>Tue, 13 Jul 2010 11:08:46 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Recommendations]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=89</guid>
		<description><![CDATA[Die Konditorei "Der Kuchenladen" ist sogar die nervige Parkplatzsuche auf der Kantstraße wert. Handgemachte Torten, Kuchen und Tarts, die klasse aussehen und einfach gut schmecken.]]></description>
			<content:encoded><![CDATA[Die Konditorei &#8220;Der Kuchenladen&#8221; ist sogar die nervige Parkplatzsuche auf der Kantstraße wert. Handgemachte Torten, Kuchen und Tarts, die klasse aussehen und einfach gut schmecken.
<p />
Das Wichtigste: Auf übertriebene effekthascherische Zuckerguß-Ungetüme wird verzichtet &#8211; die Torten zeichnet neben der spürbaren handwerklichen Qualität vor allem aus, dass sie nicht zu süß sind.
<p />
Als Schwiegersohn einer Konditoreimeisterin bin ich sehr verwöhnt, aber der Kuchenladen hat mich noch nie enttäuscht.
<p />
<address>
Der Kuchenladen<br/>
Kantstraße 138<br/>
10623 Berlin<br/>
Telefon: 030 / 310 184 24<br/>
E-Mail: uwe_gundelach@yahoo.de<br/>
<a href="http://der-kuchenladen.com/">http://der-kuchenladen.com/</a><br/>
</address>
]]></content:encoded>
			<wfw:commentRss>/2010/07/13/empfehlung-der-kuchenladen-in-berlin/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Project: PHPRestfulSubversion</title>
		<link>/2010/05/21/project-phprestfulsubversion/</link>
		<comments>/2010/05/21/project-phprestfulsubversion/#comments</comments>
		<pubDate>Fri, 21 May 2010 00:14:15 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[English articles only]]></category>
		<category><![CDATA[Projects]]></category>

		<guid isPermaLink="false">/?p=517</guid>
		<description><![CDATA[
  PHPRestfulSubversion&#8230;
  
    &#8230;provides a RESTful JSON webservice API to access information in your Subversion repository,
    &#8230;provides tools to cache your Subversion repository in order to make it searchable through the webservice in a fast and simple manner,
    &#8230;is a library of PHP classes [...]]]></description>
			<content:encoded><![CDATA[<p>
  <a href="https://github.com/ManuelKiessling/PHPRestfulSubversion">PHPRestfulSubversion</a>&#8230;
  <ol>
    <li>&#8230;provides a RESTful JSON webservice API to access information in your Subversion repository,</li>
    <li>&#8230;provides tools to cache your Subversion repository in order to make it searchable through the webservice in a fast and simple manner,</li>
    <li>&#8230;is a library of PHP classes which you can use to implement more complex use cases.</li>
  </ol>
</p>
<p>
  A secondary goal of this project is to explore how to create very clean PHP code using a 100% test-driven approach.
</p>
<p>
  The source code of this project is hosted at <a href="https://github.com/ManuelKiessling/PHPRestfulSubversion">https://github.com/ManuelKiessling/PHPRestfulSubversion</a>
</p>
]]></content:encoded>
			<wfw:commentRss>/2010/05/21/project-phprestfulsubversion/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Alte Homepage wieder verfügbar</title>
		<link>/2010/04/30/alte-homepage-wieder-verfugbar/</link>
		<comments>/2010/04/30/alte-homepage-wieder-verfugbar/#comments</comments>
		<pubDate>Fri, 30 Apr 2010 12:36:52 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Other]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=26</guid>
		<description><![CDATA[Meine alte Homepage (2000-2005) ist wiederauferstanden und unter <a href="http://old.manuel.kiessling.net/">http://old.manuel.kiessling.net/</a> erreichbar.]]></description>
			<content:encoded><![CDATA[Meine alte Homepage (2000-2005) ist wiederauferstanden und unter <a href="http://old.manuel.kiessling.net/">http://old.manuel.kiessling.net/</a> erreichbar.]]></content:encoded>
			<wfw:commentRss>/2010/04/30/alte-homepage-wieder-verfugbar/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>siqqel: SQL-Abfragen direkt aus HTML heraus ausführen und darstellen</title>
		<link>/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/</link>
		<comments>/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/#comments</comments>
		<pubDate>Wed, 07 Apr 2010 23:15:47 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://localhost/wordpress/?p=6</guid>
		<description><![CDATA[Ein Kollege von mir, Max Winde, hat in den vergangenen Wochen ein Tool geschrieben welches sich innerhalb kürzester Zeit zu einem Renner in den verschiedensten Abteilungen entwickelt hat, und schon jetzt aus dem Arbeitsalltag kaum noch wegzudenken ist.]]></description>
			<content:encoded><![CDATA[Ein Kollege von mir, Max Winde, hat in den vergangenen Wochen ein Tool geschrieben welches sich innerhalb kürzester Zeit zu einem Renner in den verschiedensten Abteilungen entwickelt hat, und schon jetzt aus dem Arbeitsalltag kaum noch wegzudenken ist: <strong>siqqel</strong>.
<p />
<h3>Welchen Zweck erfüllt siqqel?</h3>
<p />
Die verschiedensten Leute in einem Unternehmen müssen aus den verschiedensten Gründen auf relationale Datenbanken zugreifen. Klassischerweise gibt es zwei Szenarien:
<p />
<ul>
<li><strong>Ich brauche eine einfache und kurze Information</strong></li>
<li>Beispiel: &#8220;Wie war noch gleich der &#8216;name&#8217; des &#8216;product&#8217; mit der Id 12345?&#8221; oder &#8220;Wieviele Einträge waren doch gleich in der &#8216;city&#8217; Tabelle?&#8221;</li>
</ul>
<p />
Üblicherweise schmeisst man dafür direkt die SQL Kommandozeile an, oder man benutzt ein Tool wie Toad, phpMyAdmin, oder irgend einen anderen Query-Browser.
<p />
<ul>
<li><strong>Ich benötige eine komplexe Auswertung wichtiger Kennzahlen, inklusive historischer Betrachtung und Querverweisen, und diese brauche ich langfristig und regelmäßig</strong></li>
<li>Beispiel: &#8220;Wir müssen die Conversions unserer User auswerten&#8221; oder &#8220;Ich brauche eine täglich aktualisierte Auswertung unserer Produktverkäufe&#8221;</li>
</ul>
<p />
Üblicherweise werden hierfür komplexe und spezialisierte Enterprise-Tools wie Data Warehouses benutzt und manchmal auch selbst implementiert.
<p />
Das ist auch alles fein, und die Tools für beide Szenarien sind vielfältig und ausgereift. In der Praxis gibt es aber ein weiteres Szenario, welches sozusagen &#8220;dazwischen&#8221; liegt: Hier ein paar Beispiele:
<p />
<ul>
<li>Die QA Abteilung soll einem Bug nachspüren und muss dafür über einen Zeitraum von einigen Tagen einige mittelkomplexe Datenanalysen fahren und diese regelmäßig aktualisieren (&#8220;zu welcher Tageszeit kommt es vor dass User aus Gruppe X auf Seite Y Aktion Z durchführen, und dann die Kombination der Daten aus Tabelle A, B, und C gleich D ergibt?&#8221;)</li>
<li>Ein Produktmanager soll ein neues Feature konzeptionieren und benötigt dafür über einen sehr begrenzten Zeitraum eine Auswertung über verschiedene Business-Kennzahlen. Da die Analyse auf ganz neuen Annahmen beruht, helfen die im Data Warehouse vorhandenen Reports nicht weiter.</li>
<li>Ein Softwareentwickler arbeitet an der Anbindung eines externen Webservice, und möchte während der Implementations- und Testphase alle Tabellen und die zusammengehörenden Daten, die aus Webservice-Calls resultieren, im Blick haben, ohne jedes Mal 30 einzelne Queries abfeuern und miteinander in Verbindung bringen zu müssen.</li>
<li>Ein Business Analyst soll einen größeren Report vorbereiten, möchte aber erst mal ein Gefühl dafür bekommen welche Daten er benötigt und wie er diese sinnvoll miteinander verknüpfen kann.</li>
</ul>
<p />
Alle diese Beispiele haben eines gemeinsam: Die &#8220;kleine&#8221; Lösung, direkt einzelne Queries nacheinander an die DB zu schicken und die Ergebnisse dann händisch zusammenzutragen und miteinander in Verbindung zu bringen, ist <em>zu</em> klein, damit zu anstrengend und ineffektiv. Man kennt das, man fängt dann an sich die Queries in irgendein Textfile zu pasten damit sie nicht verloren gehen, oder man hat in Tools wie phpmyadmin plötzlich 15 Browsertabs auf und wird langsam wahnsinnig.
<p />
Die &#8220;große&#8221; Lösung ist aber wiederum <em>zu</em> groß:  Es lohnt in der Regel nicht, einen Business-Analysten mehrere Stunden oder Wochen mit dem Bau eines Reports aus dem Data Warehouse zu beauftragen, nur weil man wenige Tage lang etwas beobachten oder nur vorübergehend Daten debuggen muss.
<p />
Der Kompromiss sieht dann häufig so aus, dass man anfängt eine Zwischen-Notlösung auf irgend einer Insel zu bauen: Man fängt an, mit irgendwelchen ODBC Kontrukten und Excel. Schick mir so eine Excel Datei, und ich sehe nichts, denn ich habe ODBC gerade nicht richtig eingerichtet. Oder der Produktmanager, der seine temporäre, aber komplexe Auswertung braucht, bekommt eine virtuelle Maschine mit einer Basisinstallation von PHP, ein Developer gibt ihm einen Crash-Kurs in PHP-Entwicklung, und los geht das Gefrickel. Irgendwo fliegen dann diese Skripte rum, nach ein paar Monaten, wo sie vielleicht für eine neue, ähnliche Analyse noch mal nützlich gewesen wären, findet sie dann auch keiner mehr. Der PM schlägt sich mit Programmierung rum, Sysops meckert zu Recht, dass sie jetzt auch noch diese Spielkiste managen müssen, alle sind unglücklich, und irgendwo in der Ferne fängt ein kleines Kind an zu weinen.
<p />
Das muss nicht sein!
<p />
Denn genau diese Nische zwischen &#8220;einfach mal ein Query&#8221; und &#8220;das große böse komplette Data Warehouse&#8221; besetzt <em>siqqel</em> exzellent, ohne die Probleme der frickeligen Insellösungen einzuführen.

<h3>Wie funktioniert siqqel?</h3>

Die Mächtigkeit von siqqel liegt darin, dass es den Applikationsstack, der benötigt wird, um Anfragen an die Datenbank zu übermitteln, die Antwort entgegenzunehmen und die empfangenen Daten darzustellen, auf etwas recht bekanntes und verbreitetes beschränkt: den Browser.
<p />
SQL Queries werden direkt in einer statischen HTML Datei notiert. Per Ajax werden diese an ein zentral abgelegtes Backend-Skript übermittelt. Das Result Set wird an den Browser zurückgeliefert und direkt dort per DHTML dargestellt. Mit (D)HTML Bordmitteln, JavaScript und CSS kann man direkt innerhalb des HTML Dokuments dann beliebig flexibel mit den Result Sets arbeiten.

Richtig, ganz ohne PHP geht es nicht. Es braucht einen Punkt im Backend, welcher den SQL Query vom Browser entgegennimmt, an die DB übermittelt, und das Result Set als JSON an den Browser zurückliefert. Aber man beachte die Vorteile zur vorhin beschriebenen Insellösung:
<ul>
	<li>Das Skript wird einmalig an zentraler Stelle in der Serverlandschaft hinterlegt &#8211; zum Beispiel an dieselbe Location, an der bereits der phpMyAdmin läuft; dann hat man vielleicht sogar gleich die Frage der Zugriffsrechte erschlagen, denn (üblicherweise) haben nur die richtigen Personen im Unternehmen Zugriff auf diese Ressource, und Sicherheitsmechanismen, die für die Zugriffsicherung des phpMyAdmin bereits implementiert wurden (wie .htaccess, SSL Public Keys etc.), greifen ohne zusätzlich notwendige Handgriffe auch für das PHP Backend von siqqel.</li>
	<li>Nun kann jeder sofort anfangen, Reports auf Basis von siqqel zu bauen &#8211; alles was er braucht: Zugriffsrecht auf die HTTP Location des PHP Backend Skripts &#8211; und einen Browser!</li>
</ul>
<p />
Wie funktioniert das nun im Detail?
<p />
Angenommen, es gibt im Intranet eine MySQL Datenbank mit einer Tabelle, in der stehen alle Produkte des Unternehmens. Nennen wir sie &#8216;product&#8217;, und nehmen an sie befindet sich im Schema &#8216;data&#8217;. Nehmen wir weiterhin an, es gibt einen Server, auf dem wurde phpmyadmin installiert, damit man über diese Datenbank browsen kann. Dieses ist erreichbar unter unter <em>http://intranet/secure/phpmyadmin</em>. <em>/secure</em> ist der mit Zugriffsrechten versehene Teil des Servers.

Nun muss ein Systemadministrator die PHP Backendskripte unter <em>http://intranet/secure/siqqel/</em> hinterlegen, und die Konfiguration anpassen um dem siqqel PHP Code Zugriff auf die genannte Datenbank zu ermöglichen.

Ein siqqel User muss dann nur eine HTML Datei erzeugen (auf seinem Desktop oder wo auch immer, ein LAMP Kontext wird ja nicht benötigt), die folgendes enthält:
<code>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://intranet/secure/siqqel/siqqel.js.php&quot;&gt; &lt;/script&gt;

&lt;/head&gt;
&lt;body&gt;

&lt;table sql=&quot;SELECT * FROM data.product&quot;&gt;&lt;/table&gt;

&lt;/body&gt;
</code>

Öffnet er diese Datei lokal in seinem Browser, wird das SQL Statement im Attribut der Table an das Backend Skript übermittelt, das Result Set als JSON zurückgegeben, und der Inhalt der Datenbanktabelle automatisch in das table Element gerendert.

<p />

Von hier aus hat man alle Möglichkeiten: Man möchte mehrere Tabellen auf einmal anzeigen? Man erzeugt einfach mehrere table Elemente mit den entsprechenden Queries. Man möchte alle Zeilen im Result Set, bei denen die Spalte <em>name</em> mit &#8220;a&#8221; beginnt in der HTML Tabelle hervorheben? Kein Problem, jede Tabelle, Zeile und Spalte liefert ein &#8220;loaded&#8221; Event, also hat man mit einem JavaScript-Konstrukt wie

<code>
$('td.name').live('loaded', function(name) {
  // do something useful.
});
</code>

alle Möglichkeiten. Der Client Teil von siqqel basiert auf jQuery, also kann man schnell und einfach Reports bauen mit allen sinnvollen und sinnlosen Möglichkeiten, die jQuery bietet.

<p />

Was sind die weiteren Vorteile? Nun, die HTML Datei ist nicht nur der View des Reports, die HTML Datei <em>IST</em> der Report. Man kann ihn in die vielleicht vorhandenen Coderepositories im Unternehmen packen, man kann ihn per Mail verschicken, man, wenn die Wikisoftware es zulässt, seine Reports sogar direkt nativ in eine Wikiseite packen und so besonders effizient mit den Kollegen im Unternehmen teilen.

Die Projektseite von siqqel ist <a href="http://github.com/MyHammerOpenSource/siqqel">http://github.com/MyHammerOpenSource/siqqel</a>. Nicht wundern, bis vor kurzem hieß das Projekt noch &#8220;sqlHammer&#8221;, der Begriff mag noch an verschiedenen Stellen auftauchen.

Bei Fragen zu siqqel empfehle ich, ein <a href="http://github.com/MyHammerOpenSource/siqqel/issues">Issue Ticket bei github zu öffnen</a>, oder wendet euch an <a href="mailto:opensource@myhammer.com">opensource@myhammer.com</a>.]]></content:encoded>
			<wfw:commentRss>/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Database Change Management mithilfe von VCS: Teil 1</title>
		<link>/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/</link>
		<comments>/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/#comments</comments>
		<pubDate>Fri, 26 Feb 2010 12:39:01 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=28</guid>
		<description><![CDATA[Dieses Dokument beschreibt Werkzeuge und Prozesse, um Datenbankänderungen innerhalb von großen Softwareprojekten einfach, fehlerfrei und nachvollziehbar durchzuführen und zu managen.]]></description>
			<content:encoded><![CDATA[<strong>Dieser Artikel ist Work in Progress!</strong>

<h3>Vorüberlegungen</h3>
  
Dieses Dokument beschreibt Werkzeuge und Prozesse, um Datenbankänderungen innerhalb von großen Softwareprojekten einfach, fehlerfrei und nachvollziehbar durchzuführen und zu managen.

Zentraler Ansatz dieser Lösung ist: Datenbankänderungen und Codeänderungen sind <strong>prinzipiell genau dasselbe</strong>. Denn Datenbankänderungen haben genau wie Codeänderung die folgenden Eigenschaften:

<ul>
 <li>Sie ändern das Verhalten des Softwaresystems</li>
 <li>Sie entwickeln sich verteilt in verschiedenen Projekten bzw. Branches, und müssen für Abnahme und Rollout/Release zusammengeführt werden</li>
 <li>Beim Zusammenführen kann es Überschneidungen und Konflikte geben, die man mitbekommen und lösen können möchte</li>
 <li>Man möchte sie auch später noch nachvollziehen können, also sehen wer wann was gemacht hat</li>
 <li>Man möchte diese Änderungen ggf. einem Reviewprozess unterziehen</li>
</ul>

Wenn wir Datenbankänderungen in diesem Sinne genau wie Codeänderungen <em>verstehen</em>, macht es auch Sinn, Datenbankänderungen genau wie Codeänderungen zu <em>behandeln</em>. Und das bedeutet, diese innerhalb des bereits vorhandenen Entwicklungsprozesses zu managen und im selben VCS Repository zu verwalten.

<h3>Abbildung der Datenbankänderungen im VCS</h3>

Unter <em>Datenbankänderungen</em> müssen wir verstehen: Alle SQL Statements, welche die Strukturen oder Inhalte einer Datenbank verändern.

Eine Datenbankänderung im Zuge eines Projekts, Bugfixes oder sonstigen Tickets ist daher folgerichtig eine Sammlung von SQL Statements, welche zusammen mit den Codeänderungen des zugehörigen Tickets im selben Branch vom Entwickler hinterlegt wird. Hinzu kommt, dass es eine klar definierte <em>Lokalität</em> für diese Änderung geben muss, damit ein Raum geschaffen ist, in dem Konflikte entstehen (und gelöst werden) können. So wie die gleichzeitige Änderung an der Datei <em>myFile.txt</em> in zwei verschiedenen, zu mergenden Branches zu einem Konflikt führt &#8211; da in beiden Branches die Datei den selben Speicherort, also dieselbe Lokalität besitzt &#8211; müssen auch Änderungen an derselben Tabelle in zwei Branches innerhalb derselben Lokalität des jeweiligen Branches stattfinden. Der vorgeschlagene Ansatz ist daher, die Struktur der Datenbank, also die Databases mit den darunterliegenden Tables, in einer analog aufgebauten Ordner-Datei-Struktur abzubilden.

Die Lokalität  für die Tabelle <em>users.hobbies</em> wäre beispielsweise die Datei <em>/dbchanges/users/hobbies.sql</em> innerhalb des VCS. Abgebildet wird die gesamte DB Struktur, also alle Databases mit allen ihren Tables:

<code>
/dbchanges/users/hobbies.sql
/dbchanges/users/contact.sql
...
/dbchanges/products/colors.sql
/dbchanges/products/forms.sql
...
</code>

und so weiter. Gerade bei komplexen Datenbanken macht es natürlich Sinn, diese Struktur mit einem Skript zu erzeugen, für MySQL kann man dazu in einem beliebigen Verzeichnis auf dem DB Server folgenden Code ausführen (geht davon aus, dass die MySQL Daten unterhalb /var/lib/mysql liegen):

<code>
find /var/lib/mysql -type f -name *.frm -exec dirname {} \;| cut -d "/" -f 5| xargs mkdir -pfind /var/lib/mysql -type f -name *.frm | cut -d "/" -f 5,6 | sed "s/.frm/.sql/g" | xargs touch
</code>

Diese Dateien nenne ich im folgenden <em>DB Change Container</em>.

<h3>Prozessbeschreibung</h3>

<h4>Während der Produktion eines neuen Release</h4>

Wichtig ist, dass sämtliche DB Change Container nach einem Release, nachdem diese Änderungen also auf dem Produktivsystem angewendet wurden, wieder leer sind &#8211; denn zum Start der Produktion eines neuen Releases liegen noch keine neuen Änderungen für die DB vor.

Nun beginnen die Entwickler, Tickets (Feature Requests, Bugs etc.) umzusetzen, einige gemeinsam in einem Branch, einige in eigenen Branches. Sind im Zuge einer Implementation Datenbankänderungen notwendig, hinterlegt der Entwickler innerhalb des zugehörigen Branches diese Änderungen nach folgendem Muster:

<ul>
 <li><u><strong>Case 1:</strong> Die Tabelle user.hobbies soll verändert werden (neues Feld, Feld löschen, Index anlegen oder löschen, einfügen, löschen oder ändern von Einträgen etc.)</u>
   
  Der Entwickler legt alle benötigten Statements in der Datei <em>/dbchanges/users/hobbies.sql</em> ab:

  <code>
USE users;
ALTER TABLE hobbies ADD newfield1 INT NOT NULL AFTER userId;
ALTER TABLE hobbies DROP oldfield;
ALTER TABLE hobbies ADD newfield2 TINYINT NOT NULL;
ALTER TABLE hobbies ADD INDEX (newfield2);
INSERT INTO hobbies ( id, name, value ) VALUES (1234, 'hobbyname', 'hobbyvalue');
  </code>
 </li>

 <li><u><strong>Case 2:</strong> Der Entwickler legt eine komplett neue Tabelle pets im vorhandenen Schema users an</u>
    
  Er erzeugt dazu eine neue Datei <em>/dbchanges/users/pets.sql</em> und füllt sie mit dem CREATE Statement (sowie ggf. INSERT Statements):

  <code>
USE users;
CREATE TABLE pets(
 id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
 petname VARCHAR( 64 ) NOT NULL,
 FULLTEXT ( petname )
);
  </code>
 </li>

 <li><u><strong>Case 3:</strong> Der Entwickler legt eine neue Database products und darin eine neue Tabelle colors an</u>
 
   Er erzeugt einen neuen Ordner <em>/dbchanges/products</em> und darin eine Datei <em>colors.sql</em> mit folgendem Inhalt:
   <code>
CREATE DATABASE products;
USE products;
CREATE TABLE colors (
 id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
 colorname VARCHAR( 24 ) NOT NULL);
   </code>
 </li>

 <li><u><strong>Case 4:</strong> Der Entwickler löscht die Tabelle colors in der Database products</u>
    
   Er füllt die Datei <em>/dbchanges/products/colors.sql</em> mit folgendem Inhalt:
   
  <code>
USE products;
DROP TABLE colors;
  </code>
 </li>
</ul>
  
Ansonsten läuft der Entwicklungsprozess wie gewohnt.

<h4>Merge aller Tickets für den Release</h4>

Werden nun verschiedene Tickets für den Release gebündelt, werden die einzelnen Branches wie gehabt gemerged. In Hinblick auf die DB Änderungen passiert nun folgendes:

Sämtliche Änderungen in den einzelnen Branches unterhalb von <em>/dbchanges</em> werden naturgemäß unterhalb <em>/dbchanges</em> im Merge zusammengeführt. Hierbei greifen die bekannten VCS Mechanismen: Wurden Änderungen in einer Datei nur in einem einzigen Branch oder Commit vorgenommen, werden diese Änderungen einfach angewendet. Wurden Änderungen an einer Datei (also innerhalb derselben Lokalität) in mehreren Branches vorgenommen, kommt es zu einem Konflikt.

Dies ist der erste wichtige Mechanismus der hilft, die drei Anforderungen &#8211; einfach, fehlerfrei und nachvollziehbar &#8211; zu gewährleisten: Da der Konflikt garantiert eintritt, ist auch garantiert, dass der Vorgang völlig automatisch die notwendige Aufmerksamkeit erzeugt und nicht übersehen werden kann.

Nun muss, wie auch bei Codekonflikten, gelöst werden: Machen beide Änderungen Sinn, oder widersprechen sie sich? Wie genau kann man sie am sinnvollsten zusammenführen? Relevant ist hier nur, dass am Ende ein Set an Änderungsanweisungen in den Approval committet wird, welches in sich rund ist. Falls es eine eigene Test oder QA Datenbank gibt auf die diese Änderungen angewendet werden müssen, wird dies gemacht nachdem alle Tickets fertig gemerged wurden.

<h4>Durchführung des Release</h4>

Wurde im Vorfeld alles richtig gemacht, muss im Zuge des Rollout oder Release nur noch das zusammengefasste Set an Änderungen ermittelt werden, und diese müssen dann, entsprechend ihrer jeweiligen Eigenschaft, ausgeführt werden. Die Summe der Änderungen ergibt sich aus der Summe aller Anweisungen in den DB Change Containern unterhalb <em>/dbchanges</em> &#8211; hier macht es natürlich Sinn, dass man diese mithilfe eines Skripts &#8220;zusammensammelt&#8221;, aber ich gehe hier nicht näher darauf ein.

Nach dem Rollout/Release, und vor dem Erzeugen neuer Branches, müssen dann im Trunk sämtliche Datenbank-Änderungsanweisungen aus den DB Change Containern entfernt werden (auch hier macht ein Skript wie z.B.

<code>
for f in `find . -type f -name *.sql`; do echo -n "" &gt; $f; done
</code>

Sinn, um diesen Schritt zu vereinfachen), und dies muss in den Trunk (oder von wo aus auch immer neue Branches gebildet werden) committet werden &#8211; denn sonst würden dieselben Änderungen beim nächsten Rollout erneut angewendet werden.
]]></content:encoded>
			<wfw:commentRss>/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Wie man Replikationsunterbrechung durch Deadlocks bei INSERT INTO … SELECT verhindert</title>
		<link>/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/</link>
		<comments>/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/#comments</comments>
		<pubDate>Tue, 07 Aug 2007 13:04:17 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://172.16.111.147/wordpress/?p=36</guid>
		<description><![CDATA[Der My-Hammer Auftragsradar, der unsere Auftragnehmer auf Wunsch regelmässig per E-Mail über neu eingestellte Auktionen anhand einstellbarer Filterkriterien informiert, baut bei jedem Durchlauf eine eigene Suchtabelle auf. Diese wird gefüllt mit einer Untermenge der Daten unserer Haupt-Auktionstabelle, nämlich nur den derzeit laufenden Auktionen.

Die Verwendung von INSERT INTO … SELECT ist hier naheliegend, zum Beispiel so:

INSERT [...]]]></description>
			<content:encoded><![CDATA[Der <a href="http://web.archive.org/web/20080407200839/http://www.my-hammer.de/showPage.php?id=auftragservice">My-Hammer Auftragsradar</a>, der unsere Auftragnehmer auf Wunsch regelmässig per E-Mail über neu eingestellte Auktionen anhand einstellbarer Filterkriterien informiert, baut bei jedem Durchlauf eine eigene Suchtabelle auf. Diese wird gefüllt mit einer Untermenge der Daten unserer Haupt-Auktionstabelle, nämlich nur den derzeit laufenden Auktionen.

Die Verwendung von <em>INSERT INTO … SELECT</em> ist hier naheliegend, zum Beispiel so:

<p class="wp_syntax"><p class="code"><pre class="sql"><span style="color: #993333; font-weight: bold">INSERT</span> <span style="color: #993333; font-weight: bold">INTO</span> Suchtabelle
&nbsp;<span style="color: #993333; font-weight: bold">SELECT</span> a, b, c <span style="color: #993333; font-weight: bold">FROM</span> Auktionstabelle <span style="color: #993333; font-weight: bold">WHERE</span> x = y</pre></p></p>


Es ergab sich folgendes Problem: Der Query wird wie jeder andere auch auf die Datenbankslaves repliziert. Dort wurde er auch korrekt ausgeführt. Jedoch nicht immer auf dem Master: hier kam es regelmäßig zu Deadlocks auf der Auktionstabelle, da dies eine InnoDB Tabelle ist (bei MyISAM Tabellen können Deadlocks nicht auftreten).

Wenn ein MySQL Slave jedoch feststellt, dass beim gleichen Query auf dem Master und auf dem Slave unterschiedliche Fehler auftreten (Slave: no error; Master: deadlock), unterbricht dieser die Replikation. Es hilft dann nur ein <em>SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1; START SLAVE;</em>.

Ich habe mich daraufhin nach Lösungen umgeschaut. Erste Anlaufstelle ist das Kapitel <a href="http://web.archive.org/web/20080407200839/http://dev.mysql.com/doc/refman/5.1/de/innodb-deadlocks.html"><em>Vom Umgang mit Deadlocks</em></a> im MySQL Handbuch.

Mein erster Versuch war, den 4. Tipp dieses Kapitels zu befolgen: Das Einstellen eines niedrigeren Isolationslevels. Da perfekte Datenkonsistenz für die benötigte Suchtabelle nicht nötig ist (<em>dirty reads</em> also akzeptabel sind), verwendete ich gleich den niedrigsten Level <em>READ UNCOMMITED</em>. Das Ergebnis war gelinde gesagt verheerend, es traten noch mehr Deadlocks auf als zuvor.

Deshalb bin ich dazu übergegangen, die beteiligten Tabellen explizit mit einem <em>READ LOCK</em> zu sperren &#8211; viele Artikel zu diesem Thema haken diese Vorgehensweise sofort als nicht gangbar ab, da die Performance darunter leide. Da es sich beim Auftragsradar jedoch um einen Cronjob handelt, der nur alle paar Minuten einmal läuft, und der <em>INSERT INTO … SELECT</em> Query sehr schnell durchläuft, erschien mir das Risiko, eine unserer wichtigsten Tabellen für diesen Query zu sperren, als gering.

Wie sich zeigte, brachte dies den gewünschten Erfolg: Seitdem sind an dieser Stelle keinerlei Deadlocks mehr aufgetreten, und der Rest der Plattform zeigt sich von den seltenen und kurzen Locks völlig unbeeindruckt.]]></content:encoded>
			<wfw:commentRss>/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Recycelter Artikel: &#8220;My-Hammer, das Fernsehen und die Serverlast&#8221;</title>
		<link>/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/</link>
		<comments>/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/#comments</comments>
		<pubDate>Mon, 16 Jul 2007 23:22:28 +0000</pubDate>
		<dc:creator>Manuel Kiessling</dc:creator>
				<category><![CDATA[Software]]></category>

		<guid isPermaLink="false">http://localhost/wordpress/?p=13</guid>
		<description><![CDATA[
Vor mittlerweile auch schon wieder einer halben Ewigkeit hatte ich mal eine kurze Artikelserie zum Thema Serverlast-Problemlösungen bei MyHammer online, die ich nun wieder ausgegraben habe. Vieles entspricht gar nicht mehr den aktuell bei MyHammer eingesetzten Lösungen, aber verwahrenswert finde ich den Schrieb allemal. Leider fehlen die Grafiken, vielleicht finde ich die noch mal irgendwo.



Hier [...]]]></description>
			<content:encoded><![CDATA[<p>
Vor mittlerweile auch schon wieder einer halben Ewigkeit hatte ich mal eine kurze Artikelserie zum Thema Serverlast-Problemlösungen bei MyHammer online, die ich nun wieder ausgegraben habe. Vieles entspricht gar nicht mehr den aktuell bei MyHammer eingesetzten Lösungen, aber verwahrenswert finde ich den Schrieb allemal. Leider fehlen die Grafiken, vielleicht finde ich die noch mal irgendwo.
</p>

<p>
Hier der Artikel:
</p>

<p>
Vergangenen Donnerstag zeigte das ProSieben Magazin <em>Galileo</em> einen ca. 10-minütigen Beitrag über My-Hammer (<a href="http://www.prosieben.de/wissen/galileo/themen/artikel/40712/">kurze Infos zur Sendung hier</a>). Vom Ansatz her ging es um “Branchenbuch vs. My-Hammer”, aber für die Betrachtung hier ist das gar nicht so sehr von Interesse &#8211; es ist noch nichtmal von Interesse, ob so ein Beitrag positiv oder negativ für uns ist (in dem Fall war’s wie fast immer positiv) &#8211; sobald das Magazin, in dem über uns berichtet wird, genug Reichweite hat, schießen die Zugriffe in die Höhe. Die wichtigste Erkenntnis, die wir immer wieder machen: zumindest bei den Privaten scheinen die Zuschauer sprichwörtlich mit dem Laptop auf den Knien vorm Fernseher zu sitzen. Die Zugriffe kommen extrem schnell und gebündelt (beim Galileo-Beitrag war aber interessant, dass die Zugriffe wieder auf einen Schlag kamen, aber erst in dem Moment, in dem der Beitrag vorbei war).
</p>

<p>
Genau dieses plötzliche Auftreten so vieler Zugriffe ist natürlich die Herausforderung &#8211; dieselbe Anzahl User auf nur 15 Minuten verteilt wären kein Problem, aber TV sorgt dafür, dass das meiste innerhalb der ersten 5 Minuten passiert. Und es ist wirtschaftlich natürlich ziemlich unvernünftig, die für diese 5 Minuten benötigte Rechenpower anzuschaffen, nur damit sie die anderen 525.595 Minuten im Jahr vor sich hindümpelt.
</p>

<p>
Trotzdem kann man eine Webseite auch auf solche Extremsituationen vorbereiten &#8211; My-Hammer hat am Donnerstag perfekt standgehalten, lediglich eine leichte Verzögerung in den Ladezeiten war während der kritischen Phase spürbar.
</p>

<p>
Um kurz die Dimensionen klarzumachen, erstmal eine Grafik, welche den ein- und ausgehenden IP Traffic für unser Netzwerk anzeigt. Man sieht sehr deutlich den Sprung auf das gut 2,5-fache des normalen Werts. Der Faktor selbst klingt vielleicht erstmal nicht so dramatisch, aber wie erwähnt geht es nicht um die Masse an sich, sondern das extrem gebündelte Auftreten dieser Masse an Zugriffen:
</p>

<p>
(Die Grafik ist leider nicht mehr auffindbar)
</p>

<p>
Ich behaupte mal, man erkennt recht gut, wann die Sendung lief…
</p>

<p>
Also, wie kann man die Serversysteme auf so etwas vorbereiten? Klar: mehr Server kaufen. Das ist durchaus ein Aspekt, aber nicht das Allheilmittel. Vor allen Dingen kann das sehr ineffektiv und unwirtschaftlich sein. Angenommen, man hat Server A mit einer gewissen Leistungsfähigkeit. Nun kann man sich Server B mit doppelt so schnellem Prozessor, doppeltem Arbeitsspeicher und doppelt so schnellen Festplatten kaufen. Dann hat man schon Unmengen von Geld ausgegeben, und hat gerade mal eine Steigerung der Leistungsfähigkeit von 100% (mal davon abgesehen, dass die Rechnung “doppelt so schnelle Hardware, doppelt so viel Leistung” in der Praxis auch nicht wirklich hinhaut). Dagegen kann ein einziger geschickt gesetzter Index in der Datenbank manchmal 1000% bessere Performance bringen, ohne dass man etwas an der Hardware tut.
</p>

<p>
Wenn man den Anschaffungspreis neuer Hardware mal auf den Stundenlohn eines Entwicklers umrechnet, wird man schnell zu dem Schluss kommen, dass es sich auch finanziell durchaus rechnen kann, diesen einige Tage lang auf die Datenbank anzusetzen um zu schauen, ob nicht doch irgendwo ein wichtiger Index vergessen wurde oder einige Tabellenstrukturen besser ganz anders aufgebaut sein sollten.
</p>

<p>
Das sind nur ein paar grundsätzliche Überlegungen. Spürbaren Erfolg wird man nur haben, wenn man ein ganzes Bündel an Massnahmen ergreift und vor allem immer das Gesamtsystem vom Code über die Datenbank bis hin zu den Servern und dem Netzwerk im Überblick hat. Die vielleicht wichtigste Faustregel, wenn man über Performanceoptimierung von Webseiten spricht, scheint mir daher zu sein: Coder und Admins an einen Tisch! Es hilft nichts, wenn die Entwickler meinen, die Geschwindigkeit des Systems sei doch schliesslich Sache des Admins. Umgekehrt ist es extrem hilfreich, wenn die Programmierer auch einen gewissen Sysadmin-Background haben, und die Admins umgekehrt auch Programmiererfahrung haben; was bei uns glücklicherweise sogar sehr ausgeprägt der Fall ist.
</p>

<p>
Die weiteren Teile befassen sich mit den konkreten Massnahmen, die man ergreifen kann um sich auf einen TV Beitrag vorzubereiten.
<strong>Hinweis:</strong> Thematisch durchaus verwandt berichtet Tom Bachem <a href="http://blog.thomasbachem.com/2007/05/28/die-sevenload-systemarchitektur/">über die Systemarchitektur von sevenload</a>.
</p>

<p>
Welche Massnahmen kann man nun konkret ergreifen, um sich auf einen TV Beitrag über die eigene Webseite vorzubereiten? Ich versuche so allgemein wie möglich zu bleiben, aber da es um konkrete Ratschläge gehen soll und ich holprige Umschreibungen vermeiden möchte, wird das Vokabular ab jetzt etwas LAMP-lastig; bitte entsprechend auf die eigene Technik ummünzen.
</p>

<h3>Massnahme 1: Datenbankoptimierung</h3>
<p>
Wurde ja schon erwähnt: die Indizes. Ich verrate wahrscheinlich nicht einmal DB Anfängern etwas neues, wenn ich betone, dass dies essentiell ist. Wenn man die Indizes nicht im Griff hat, braucht man sich die anderen Punkte noch gar nicht anschauen. Deshalb: Ins Slow-Log gucken. Vor allem: Immer wieder. Einen Status Quo gibt es nicht! Immer wieder <a href="http://dev.mysql.com/doc/refman/5.0/en/explain.html">EXPLAIN</a> bemühen, vom stumpf auf die Strukturen in phpMyAdmin gucken findet man die Performancefresser nicht.
</p>

<p>
Es gibt diese missverständliche Formel “Braucht man Geschwindigkeit, nimmt man MyISAM, braucht man Sicherheit, InnoDB”. InnoDB ist nicht nur einen Blick wert, wenn man Transaktionssicherheit braucht. Im Gegensatz zu MyISAM lockt InnoDB bei schreibenden Queries immer nur die betreffenden Zeilen, MyISAM dagegen grundsätzlich die gesamte Tabelle. InnoDB hat zwar aufgrund der größeren Komplexität etwas mehr “Grundoverhead”, aber das intelligentere Locking kann immens wertvoll sein in bestimmten Szenarien und das mehr als wettmachen. Wenn man eine Tabelle hat die man hinsichtlich Struktur und Indices schon perfekt durchoptimiert hat (genau das aber wiederum erstmal sicherstellen!), und trotzdem tauchen Queries auf diese Tabelle immer noch im Slow Log auf, dann sollte man prüfen, ob diese Queries vielleicht immer auf einen Lock warten. In diesem Fall InnoDB auf jeden Fall eine Chance geben. Das hat bei uns konkret bei den Session und Cachetabellen (dazu später mehr) enorm viel gebracht, weil dort die Lese- und Schreibzugriffe ein ausgewogenes Verhältnis haben.
</p>

<p>
Ein Aspekt, der wenig berücksichtigt wird, ist die Größe der Felder, auf die man Indices setzt. Es kann sich lohnen, hier sparsam zu sein, denn ein kleinerer Spaltentyp bedeutet auch weniger Speicherplatzverbrauch für den Index auf diese Spalte, und das kann im Zweifel nur gut (= schneller) sein. Man ist halt geneigt, seine Primary IDs immer als INT anzulegen. Aber nehmen wir mal den Klassiker Benutzertabelle: Wird man wirklich in nächster Zeit 4 Milliarden User haben? Das dürfte selbst bei eBay noch ein bisschen dauern. Erstmal tut es also auch ein MEDIUMINT, setzt man diesen UNSIGNED, ist das Limit bei 16 Millionen. Hat man soviele User, bewegt man sich wohl eh in völlig anderen Dimensionen.
</p>

<p>
Zumal das Umwandeln einer Spalte in einen Typ mit größerem Wertbereich (also z.B. von MEDIUMINT nach INT) unproblematisch ist. Wichtig ist allerdings auch, dass man sämtliche Felder, die einen Fremdschlüssel auf ein MEDIUMINT Feld darstellen, ebenfalls als MEDIUMINT anlegt, sonst hat man bei Joins nichts gewonnen.
</p>

<p>
Was bei der Skalierung von MySQL immer enorm hilft ist Replikation. Dazu wurde schon so viel geschrieben, dass ich mir die Wiederholung spare, nur dies: Wir fahren bisher sehr gut damit, das Balancing der Nur-Lese Zugriffe direkt in unserer Applikation zu regeln, und nicht über einen eigenen Software- oder Hardware-Loadbalancer. Da bei fast jedem Seitenaufruf der Master sowieso früher oder später konnektiert werden muss, kann man diese Verbindung auch nutzen, um MASTER STATUS und SLAVE STATUS zu vergleichen, um so ein Fallback auf den Master zu realisieren, falls alle Slaves einmal mehr als 0 Sekunden hinter dem Master zurückhängen. Was sich übrigens ziemlich gut vermeiden lässt, wenn man Master und Slaves per Gigabit statt Fast Ethernet anbindet.
</p>

<p>
Ein oft nicht wahrgenommener Vorteil von Replikation: Man kann einen Slave für’s Backup bereitstellen, auf dem man die Datenbank stoppen und auf Dateisystemebene wegkopieren kann (oder man hält nur den Slave Thread an und macht einen Dump), so dass man einen sauberen Snapshot der Datenbank hat, ohne das Gesamtsystem anhalten zu müssen.
</p>

<p>
Ein weiterer wichtiger Hebel für die Skalierung ist es, für spezielle Aufgaben jeweils eigene DB Server bereitzustellen, z.B. ein oder mehrere Maschinen nur für die Sessiontabellen, nur für Tabellen mit Cache-Inhalten, nur für Logtabellen; prinzipiell kann jede Tabelle, die nicht in Form von Joins oder Subselects zusammen mit anderen Tabellen gleichzeitig abgefragt werden muss, auch getrennt von den anderen Tabellen auf einem eigenen Server liegen. Darüber hinaus macht die Trennung von sehr verschiedenen Tabellen wie Session- und Logtabellen alleine deshalb schon Sinn, weil man dann die Datenbanksoftware für diese speziellen Aufgaben optimieren kann.
</p>

<p>
Eine praxisnahe Zusammenstellung der Massnahmen, die sich bei <a href="http://www.my-hammer.de/">My-Hammer.de</a> bewährt haben:
<p>

<h4>InnoDB vs MyISAM</h4>
<p>
Ich schrieb bereits, dass man InnoDB nicht nur dann in Erwägung ziehen sollte, wenn man Transaktionssicherheit benötigt. Eine Tabelle von MyISAM auf InnoDB umzustellen kann unter Umständen Geschwindigkeitsvorteile bringen, nämlich dann, wenn das zweite wichtige Feature von InnoDB neben der Transaktionssicherheit, das Row Level Locking, effektiv zum Zug kommen kann. Um herauszufinden, ob dies der Fall ist, kann man wie folgt vorgehen:
</p>

<h4>Mitloggen aller Queries</h4>
<p>
Wenn man für einen bestimmten Zeitraum (bei einer gut besuchten Seite reichen wenige Minuten) einmal alle Abfragen, die an die Datenbank gestellt werden, mitschreibt, kann man aus diesem Log eine Menge interessanter Informationen ziehen. Um festzustellen, ob eine Tabelle vom Row Level Locking profitieren könnte, muss man die lesenden (SELECT) und schreibenden (INSERT, UPDATE, DELETE etc.) Abfragen gegenüberstellen.
</p>

<p>
Wird aus einer Tabelle sehr häufig gelesen, die Daten in der Tabelle aber nur sehr selten verändert, dann macht das Table Level Locking von MyISAM in der Regel keine Probleme: Zwar wird bei einem UPDATE, INSERT oder DELETE die gesamte Tabelle für nachfolgende Lesezugriffe gesperrt (d.h. diese müssen warten), bis der Schreibprozess abgeschlossen ist. Aber da dies nur selten geschieht, kommt es auch selten vor, dass ein Leseprozess warten muss, so dass daraus keine spürbare Verzögerung im Gesamtsystem resultiert.
</p>

<p>
Gleiches gilt im umgekehrten Fall: Wird in eine Tabelle praktisch nur geschrieben, aber selten daraus gelesen (wie es z.B. bei Logtabellen häufig der Fall ist), dann kollidieren auch hier die “Interessen” nur so selten, dass nicht mit Performanceeinbußen zu rechnen ist.
</p>

<h4>Slow Log</h4>
<p>
Interessant sind also jene Tabellen, bei denen Schreib- und Lesezugriffe in einem ausgeglicheneren Verhältnis stehen. In welcher Relation die beiden Zugriffsarten dabei mindestens stehen müssen, damit es sich “lohnt” InnoDB einzusetzen, ist schwer zu sagen. Ein Blick ins Slow-Log von MySQL hilft hier weiter: Wenn man immer wieder bei denselben Tabellen auf langsame Queries stösst, die nicht wegen des Queries selbst langsam waren, sondern weil sie auf ein Lock warten mussten, hat man auf jeden Fall aussichtsreiche Kandidaten.
</p>

<h4>SHOW PROCESSLIST</h4>
<p>
Eine weitere Methode ist, sich einmal für einige Minuten immer wieder die Liste der laufenden Prozesse in MySQL auflisten zu lassen (SHOW PROCESSLIST). Wenn man dort immer wieder dieselben Queries sieht, deren Status <em>Locked</em> ist, dann weiss man wo das Problem liegt. Diese Methode mag zwar auf den ersten Blick wie ein Glücksspiel wirken, aber gerade weil man immer nur die Prozesse sieht, die zufällig gerade laufen wenn man den Befehl absetzt, fallen die problematischen Prozesse erst recht auf, die immer wiederkehren und oft vielleicht sogar während zwei oder mehr SHOW Aufrufen immer noch laufen. Meiner Meinung nach die schnellste Methode, Flaschenhälse zu finden.
</p>

<p>
Mehr zum Thema Locking gibt es im <a href="http://dev.mysql.com/doc/refman/5.0/en/internal-locking.html">Kapitel ‘Internal Locking Methods’ des MySQL Handbuchs</a>.
</p>

<p>
Nehmen wir also an, man hat einige Tabellen identifiziert, bei denen Queries öfter als gesund ist auf einen Lock warten müssen. Dies könnte beispielsweise eine Sessiontabelle sein (falls man z.B. PHP nutzt und die Sessionfunktionen so angepasst hat, dass diese eine MySQL Datenbank als Storage nutzen, ein ziemlich klassisches Szenario). Diese Tabelle wird bei jedem Seitenaufruf zu Beginn einmal gelesen, um die Session des aufrufenden Benutzers zu laden, und am Ende des Skripts wird der Sessioninhalt dieses Benutzers wieder geschrieben. Also ein sehr ausgewogenes Verhältnis zwischen lesenden und schreibenden Zugriffen &#8211; jeder Seitenaufruf, der gerade an dem Punkt angelangt ist, an dem die Session geschrieben wird, würde also die Tabelle sperren für sämtliche anderen Seitenaufrufe, die in diesem Moment aus der Sessiontabelle lesen möchten &#8211; das Performanceproblem ist ab einer bestimmten Anzahl von gleichzeitigen Benutzern vorprogrammiert.
</p>

<p>
Klassischerweise geht man nun so vor, dass man die Tabelle in InnoDB umwandelt und wieder einige Zeit das Slow Log oder die Prozessliste beobachtet &#8211; sinkt die Lock_Time der Abfragen deutlich, hat man einen Flaschenhals erfolgreich eliminiert.
</p>

<p>
Nun, es wäre freilich zu schön, wenn es nicht doch den ein oder anderen Haken bei der Sache gibt; zum Glück lassen sich die meisten aber zumindest einigermassen elegant umschiffen.
</p>

<p>
Eine Einschränkung von InnoDB ist beispielsweise, dass der FULLTEXT Index nicht unterstützt wird. Dies war bei My-Hammer ein Problem, weil wir eine Tabelle, die ziemlich eindeutiger Kandidat für eine Umstellung von MyISAM auf InnoDB war, in einem Teil unserer Applikation auch durchsuchen mussten, und zwar eben gerade einige TEXT-Felder, was ohne FULLTEXT Index nicht wirklich Spass macht.
</p>

<p>
Die Lösung war, die Tabelle umzuwandeln und damit in der Tabelle selbst auf die FULLTEXT Indizes zu verzichten, per cronjob aber eine weitere Tabelle regelmässig mit den Daten der Ursprungstabelle zu füllen. Geschrieben wurde in diese Tabelle nur durch besagten Crobjobs, ansonsten fanden ausschliesslich Lesezugriffe statt, womit MyISAM wieder die perfekte Wahl war &#8211; und wir hatten unsere FULLTEXTs wieder. Schöner Nebeneffekt: durchsucht werden müssen eh nur eine Untermenge aller Zeilen der Ursprungstabelle, und es müssen auch nicht alle der (recht zahlreichen) Spalten in die Suchtabelle übertragen werden.
</p>

<p>
Dadurch konnten wir nicht nur das Lockingproblem der ursprünglichen Tabelle lösen, sondern aufgrund der schlankeren Datenbasis in der Suchtabelle die Suche deutlich beschleunigen.
</p>

<p>
Wichtig ist jedoch: diese Lösung ist nur möglich, weil wir in diesem Fall darauf verzichten können, auf absoluten Livedaten zu suchen.
</p>

<p>
Meiner Erfahrung nach kann man zusammenfassend sagen: Es gibt nur eine einzige Massnahme, die mehr Performance bringt als Caching, und das ist noch mehr Caching. Das gilt, um mal zum Haupthema zurückzukehren, vor allem in Bezug auf Performance bei plötzlichen Besucheranstürmen.
</p>

<h3>Statische Inhalte</h3>
<p>
Wenn man einen TV Beitrag über die eigene Plattform überleben will, dann gibt es nichts, aber auch wirklich gar nichts Wichtigeres als dies hier: <em>Die Startseite der Plattform ist eine statische HTML Seite</em>. Und zwar in aller Konsequenz, was heissen soll, dass der Aufruf der Seite nicht nur keine Datenbankverbindung zur Folge hat, sondern dass noch nicht einmal der PHP Interpreter auch nur gestartet wird. Die Startseite von My-Hammer ist eine .html Seite, die im Gegensatz zu den .php Seiten per Apache-Konfiguration mod_php noch nicht mal von Weitem zu sehen bekommt. Selbiges sollte konsequenterweise auch für alle JavaScript und CSS Dateien, die von der Startseite eingebunden werden, gelten. Ob man hierfür nun mit Proxylösungen arbeitet oder Seiten regelmäßig vorgeneriert, ist Geschmackssache.
</p>

<p>
Man darf nie den Performancevorteil reinen HTMLs unterschätzen &#8211; selbst wenn sich die Datenbanken schon alle verabschiedet haben und die Webserver bereits richtig unter Dampf sind: Eine HTML Seite auszuliefern schafft sogar ein Webserver, der schon ziemlich am Ende ist. Und man wahrt vor allem noch am ehesten sein Gesicht, wenn die ganzen neuen Benutzer, die aufgrund des TV Beitrages neugierig geworden sind, zumindest die Startseite zu sehen bekommen. Was immer man neben der Startseite noch an Seiten statisch vorgenerieren kann, ohne dass der angebotene Dienst selber “statisch” wird, sollte man natürlich machen (denn wie oft ändern sich schon Seiten wie <em>Über uns</em>?). Hierbei macht es Sinn, sich mithilfe der Zugriffsstatistiken einmal anzuschauen, welchen Weg neue Benutzer in der Regel auf der Plattform nehmen, um so auch wirklich jene Seiten zu cachen, die bei einem Ansturm am ehesten angesurft werden.
</p>

<p>
Eine dynamische Seite als statische Seite vorzugenerieren ist dabei natürlich die konsequenteste Version von Caching, aber nicht immer praktikabel. My-Hammer nutzt eine zweite Stufe des Cachings, bei dem zwar weiterhin dynamische Seiten ausgeliefert werden, diese aber ganz oder teilweise in dedizierten Cache-Backends (wir nutzen dazu <a href="http://www.danga.com/memcached/">memcached</a>) abgelegt sind, um Ergebnisse teurer Datenbankabfragen, die nicht immer absolut live zur Verfügung stehen müssen, zwischenzuspeichern. Diese zwischengespeicherten Einträge können zum einen nach einer gewissen Zeit ablaufen und werden dann neu aus der Datenbank erzeugt, oder können gezielt als “dirty” markiert werden, wenn die Datenbestände die sie widerspiegeln sich ändern.
</p>

<p>
Wie oben erwähnt kann es sich außerordentlich lohnen, diese Cacheinhalte auf dedizierten Maschinen bereitzustellen &#8211; was wiederum deutlich zeigt, dass manche Massnahmen zur Performancesteigerung bestenfalls halbgar sind, wenn Admins und Programmierer nicht zusammenarbeiten.
</p>

<h3>Everybody needs a 304</h3>
<p>
(oder: Wie ich dem Browser des Users helfe, optimal zu cachen)
</p>

<p>
Bisher bin ich lediglich auf ein Ziel von Performanceoptimierung eingegangen &#8211; zu verhindern, dass die eigenen Server zusammenbrechen, wenn’s mal brenzlig wird. Man muss sich aber unbedingt bewusst machen, dass Optimierungen auf dem Server erstmal keinen Wert an sich darstellen, sondern nur dem eigentlichen Ziel dienen: dem User die Benutzung der eigenen Seite so schnell und angenehm wie möglich zu machen &#8211; indem die Seite grundsätzlich erreichbar bleibt, und indem die Seite sich so schnell wie möglich aufbaut.
</p>

<p>
Wenn man sich das erstmal bewusst gemacht hat, ist auch klar dass es sich sogar lohnen kann, etwas Rechenzeit auf dem Server zu investieren, um sie dem Client (also Browser) abzunehmen.
</p>

<p>
Aber der Reihe nach. Es gibt ein wichtiges Hilfsmittel, um den Aufbau einer Webseite im Browser deutlich zu beschleunigen (abgesehen von den üblichen Massnahmen wie geringer Dateigröße, möglichst wenig eingebetteten Objekten etc.), und das ist die Verwendung des HTTP Status <em>304 Not Modified</em>. Diesen kann der Server senden, wenn er anhand der Anfrage des Clients erkennt, dass exakt der Inhalt, den der Browser bereits in seinem Cache hat, nochmal über die Leitung wandern würde &#8211; in diesem Fall sendet der Server diesen Inhalt dann eben nicht nochmal, sondern teil dem Browser nur mit, er möge auf den Inhalt seines Caches zurückgreifen.
</p>

<p>
Dies kann zu erheblichen Performancesteigerungen auf Seiten des Clients führen, denn die Zeit die zum Download des Inhalts einer Seite benötigt wird, entfällt.
</p>

<p>
Es gibt nun zwei Faktoren, die das Status 304 Handling beeinflussen und spezielle Anpassungen erfordern, um optimales Clientcaching zu ermöglichen: Die Auslieferung von Seiten über PHP Skripte (gilt prinzipiell auch für andere Skriptsprachen) und der Betrieb einer Plattform in einem Webserver-Cluster.
</p>

<p>
Zuerst zu letzterem: Um in der gegenseitigen Kommunikation festzustellen, ob ein Inhalt vom Server neu ausgeliefert werden muss oder der Browser den Inhalt aus dem eigenen Cache lädt, gibt es den sogenannten <em>Etag</em>. Ein ganz kurzer Abriss, wie die Verwendung abläuft. Der Client fragt eine Ressource beim Server an. Es ist der erste Zugriff innerhalb dieser Browsersitzung, deshalb schickt der Client kein Etag mit. Der Server sendet daraufhin die Inhalte aus, und schickt in den Headern den Etag des aktuellen Inhalts dieser Ressource mit, sagen wir “12345″ (der Server schickt dazu den Header <em>Etag: “12345″</em>).
</p>

<p>
Fragt der Client nun erneut dieselbe Ressource beim Server an, schickt er in seinen Headern wiederum die Information mit, dass er in seinem Cache bereits die Inhalte mit dem ETag “12345″ gespeichert hat, und der Server ihn informieren möge falls sich die Inhalte nicht geändert haben (der Client schickt dazu den Header <em>If-None-Match: “12345″</em>). Der Server kann dann schauen, ob die Inhalte die er ausliefern würde immer noch das ETag “12345″ haben, und in diesem Fall den erwähnten HTTP Status 304 senden, oder, falls Inhalt und ETag nicht mehr zueinander passen, den neuen Inhalt schicken.
</p>

<p>
Die Frage ist nun: Wie genau ist denn definiert, was im Etag steht? Nun, im Prinzip gar nicht. Es gibt kein vorgeschriebenes Format, wichtig ist nur die Definition des Etag an sich: dass nämlich ein eindeutiges Etag zu einem eindeutigen Inhalt einer bestimmten Ressource gehört, und deshalb festgestellt werden kann ob sich der Inhalt einer Ressource zwischen zwei Requests geändert hat oder nicht. Man kann sich den Etag deshalb der Einfachheit halber als Checksumme des Inhalts vorstellen (und in der Tat besteht eine Möglichkeit den Etag zu generieren darin, z.B. die MD5 Summe des Inhalts zu berechnen).
</p>

<p>
Woher kommt der Etag? Beim Apache ist es Teil der Kernfunktionalität, für eine angeforderte Ressource den Etag zu berechnen und mitzusenden, sowie entsprechend zu reagieren wenn ein Client den <em>If-None-Match</em> Header sendet. Alles out-of-the-box also, aber genau hier liegen für uns die Probleme:
</p>

<p>
<strong>Problem 1:</strong> Defaultmässig berechnet Apache den Etag für eine Ressource, indem eine Art Quersumme aus diesen Informationen generiert wird: I-Node-Nummer der angefragten Datei, letzter Änderungszeitpunkt (mtime) der angefragten Datei, und Größe der angefragten Datei. Betreibt man eine Webseite auf nur einem Server, hat man kein Problem, denn wenn z.B. die Datei <em>/index.html</em> zwischen zwei Aufrufen nicht verändert wird, hat sie bei beiden Zugriffen denselben Etag, da keiner der drei Faktoren inode, mtime, size zwischenzeitlich verändert wurde.
</p>

<p>
Betreibt man aber einen Cluster aus mehreren Webservern, und besteht die Möglichkeit, dass ein Client bei zwei aufeinanderfolgenden Aufrufen derselben Ressource zuerst auf einem Webserver, beim zweiten Aufruf aber auf einem anderen landet, dann ist, auch wenn auf beiden Servern die exakt gleiche Datei liegt, der Etag beide Male ein anderer, denn selbst wenn letzter Änderungszeitpunkt und Größe der Datei auf beiden Servern identisch sind: dass die I-Node-Nummer die gleiche ist, ist praktisch ausgeschlossen. Der Server wird also keine 304 Status senden, obwohl er es könnte.
</p>

<p>
Abhilfe ist zum Glück sehr einfach möglich, und lohnt sich schon beim Wechseln von einem auf zwei Server: Man muss dem Apache mitteilen, dass er die I-Node-Nummer nicht mehr zur Berechnung des Etag heranziehen soll. Dies erledigt an zentraler Stelle die Anweisung <em>FileETag MTime Size</em>. Mehr dazu im <a href="http://httpd.apache.org/docs/2.2/mod/core.html#fileetag">Apache Handbuch</a>.
</p>

<p>
<strong>Problem 2:</strong> <em>mod_php</em> hebelt die Verwendung von Etag für PHP Skripte aus. Das macht ja prinzipiell auch Sinn: selbst wenn die Skriptdatei <em>/index.php</em> sich zwischen zwei Aufrufen inhaltlich überhaupt nicht geändert hat, kann sie dennoch völlig unterschiedliche Inhalte an den Client ausliefern &#8211; genau das ist ja Sinn und Zweck des Einsatzes von dynamischen Seiten.
</p>

<p>
Trotzdem kann es Sinn machen, dass der Server den Status 304 an einen Client sendet, wenn dieser dieselbe Ressource erneut anfragt. Zum Beispiel bei CSS Skripten, die von jeder Seite der Plattform eingebunden werden, und aus programmiertechnischen Erwägungen als PHP Skripte realisiert sind, aber deren Inhalt sich trotzdem sehr selten ändert. Jeder Seitenaufruf würde den Browser veranlassen, dies referenzierte CSS Datei anzufragen, und der Server würde jedes Mal den Inhalt senden, obwohl sich dieser seit dem letzten Aufruf nicht geändert hat. Das macht den Seitenaufbau im Client langsam, und ist zudem eine Ressourcenverschwendung.
</p>

<p>
Wie kann man nun sicherstellen, dass ein Client den 304 Status auch beim Abruf von PHP Skripten erhält, falls sich der Inhalt nicht verändert hat, aber auch auf keinen Fall einen 304 Status bekommt, falls der Inhalt sich geändert hat? Die Lösung ist leider nicht ganz so trivial wie beim ersten Problem, aber doch vergleichsweise einfach zu realisieren.
</p>

<p>
Da wie erwähnt der Zustand der Skriptdatei selbst praktisch keine Rolle spielt, darf man nur mit dem von diesem Skript auszuliefernden Inhalt arbeiten. Eine Lösung wäre, bei den Skripten, für die man den Etag Mechanismus einsetzen möchte, folgenden Code ans Ende anzuhängen (lässt sich natürlich einfach in eine zentrale Funktion kapseln):

<code>&lt;?php
$output = ob_get_clean(); // Gesamte Ausgabe, die an den Client gesendet werden soll, abfangen und zwischenspeichern

$etag = ‘”‘.sha1($output).’”‘; // Prüfsumme der Ausgabe berechnen// Ist der Inhalt identisch mit dem, den der Client gecached hat?

if ($_SERVER['HTTP_IF_NONE_MATCH'] == $etag) // Wenn ja, dann sende nur den Status 304
{
    header(‘HTTP/1.x 304 Not Modified’);
    header(‘Etag: ‘.$etag);
    die();
}
else // Wenn nicht, dann sende den Inhalt inkl. des neuen Etags
{
    header(‘Etag: ‘.$etag);
    echo $output;
    die();
}
?&gt;
</code>


</p>

<p>
Voraussetzung ist dafür die Verwendung von <em>output buffering</em>.
</p>

<p>
Eines muss man ganz klar festhalten &#8211; für den Server fällt exakt dieselbe Arbeit an, egal ob der User den Inhalt schlussendlich zugesendet bekommt oder nur die lapidare Meldung, er möge doch auf seinen Cache zurückgreifen. Es mag nach deutlich zuviel Overhead aussehen, PHP soviel Arbeit erledigen zu lassen, nur um das Ergebnis dieser Arbeit dann wegzuschmeissen; aber der Effekt auf die Lade- und damit Seitenaufbauzeiten beim Client ist wirklich beeindruckend, wenn man diesen Mechanismus geschickt einsetzt.
</p>]]></content:encoded>
			<wfw:commentRss>/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
