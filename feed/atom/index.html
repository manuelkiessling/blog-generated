<?xml version="1.0" encoding="UTF-8"?><feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
  xml:base="/wp-atom.php"
   >
	<title type="text">The Log Book of Manuel Kiessling</title>
	<subtitle type="text">Writings about agile software development and system operation topics</subtitle>

	<updated>2013-09-25T19:12:09Z</updated>
	<generator uri="http://wordpress.org/" version="2.9.2">WordPress</generator>

	<link rel="alternate" type="text/html" href="/" />
	<id>/feed/atom/</id>
	<link rel="self" type="application/atom+xml" href="/feed/atom/" />

			<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Converting a running physical machine to a KVM virtual machine]]></title>
		<link rel="alternate" type="text/html" href="/2013/03/19/converting-a-running-physical-machine-to-a-kvm-virtual-machine/" />
		<id>/?p=759</id>
		<updated>2013-03-21T20:02:44Z</updated>
		<published>2013-03-19T17:40:48Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="Tutorial" />		<summary type="html"><![CDATA[Let's assume you have a physical machine running a Linux system, and you would like to convert this system into a virtual KVM/QEMU machine, keeping everything as close to the original as possible. What follows is my approach.]]></summary>
		<content type="html" xml:base="/2013/03/19/converting-a-running-physical-machine-to-a-kvm-virtual-machine/"><![CDATA[<p>
Let&#8217;s assume you have a physical machine running a Linux system, and you would like to convert this system into a virtual KVM/QEMU machine, keeping everything as close to the original as possible. What follows is my approach.
</p>

<p>
The first thing we need is a raw image file which mirrors the exact layout of the physical hard drive in our physical server.
</p>

<p>
In our example scenario, the physical box has one hard drive at <em>/dev/sda</em> with a <em>/boot</em> Partition on <em>/dev/sda2</em> and a physical LVM volume on <em>/dev/sda3</em>. This LVM volume houses a volume group with two logical volumes, one of them housing the root partition <em>/</em>, and the other one being unused. Also, <em>/dev/sda1</em> is unused. Grub is installed into the Master Boot Record.
</p>

<p>
(While this setup may sound like it doesn&#8217;t make too much sense, trust me that I encountered this very setup the other day. The good news is that it&#8217;s an excellent example case because it is quite complicated, which gives me the chance to explain a lot of different concepts and solutions in detail.)
</p>

<p>
We need to recreate this hard drive in the virtual world. If we could stop the server, this would actually be quite simple: shutdown the machine, boot from a linux rescue cd, and <em>dd</em> every single byte from <dev>/dev/sda</em> into a raw image file. This might even work when done from a running system where the disk is mounted, at least to a certain degree. But if you want to learn about all the little details that make up hard drive partitions and their file system, then continue reading.
</p>

<p>
Let&#8217;s look at the layout of the physical disk with <em>fdisk -ul /dev/sda</em>:
</p>

<code>Disk /dev/sda: 299.4 GB, 299439751168 bytes
255 heads, 63 sectors/track, 36404 cylinders, total 584843264 sectors
Units = sectors of 1 * 512 = 512 bytes

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *          63     1012094      506016    b  W95 FAT32
/dev/sda2         1012095     1220939      104422+  83  Linux
/dev/sda3         1220940   584830259   291804660   8e  Linux LVM</code>

<p>
Next, we need to switch to our KVM host server and create a raw image file that is exactly the same size as the physical hard drive (which, according to the first line of fdisk&#8217;s output, is 299.4 GB, or 299439751168 bytes):
</p>

<code># qemu-img create image.raw 299439751168</code>

<p>
We could now re-create the partitioning scheme of the physical disk on the image by hand, but there is a simple shortcut: we only need to write the first 512 bytes of the physical disk into the first 512 bytes of the image. That&#8217;s the Master Boot Record (MBR) where all partitioning information resides.
</p>

<p>
<em>dd</em> is the tool of choice for reading and writing raw bytes. We use the following to read the MBR from our physical disk: 
</p>

<code># dd if=/dev/sda of=./mbr.bin bs=512 count=1</code>

<p>
It will write exactly one 512 byte block into a file called mbr.bin. Transfer this file to your KVM host, then write its content into the image file:
</p>

<code># dd if=./mbr.bin of=./image.raw bs=512 count=1 conv=notrunc</code>

<p>
This writes exactly one block of 512 bytes into the image, and does not truncate the rest of the image.
</p>

<p>
Now run
</p>

<code># fdisk -l image.raw</code>

<p>
to verify that the image file now has a partition layout which mirrors that of the physical disk:
</p>

<code>    Device Boot      Start         End      Blocks   Id  System
image.raw1   *          63     1012094      506016    b  W95 FAT32
image.raw2         1012095     1220939      104422+  83  Linux
image.raw3         1220940   584830259   291804660   8e  Linux LVM</code>

<p>
Now we can start to create filesystems on our imagefile&#8217;s partitions. But to create file systems, we need to address these partitions as devices. <em>losetup</em> comes to the rescue, because it allows us to present parts of a raw image file to the host system as loopback devices.
</p>

<p>
We need to create two loopback devices, one for the <em>sda2</em> partition (<em>/boot</em> on our physical system), and one for the <em>sda3</em> partition, which is a physical LVM volume. Afterwards, we will be able to use the <em>sda2</em> loopback device directly &#8211; because of LVM, <em>sda3</em> needs some extra care, as we will see.
</p>

<p>
Here is how we create a loopback device <em>/dev/loop0</em> which points at the section of <em>image.raw</em> that makes up the <em>sda2</em> partition:
</p>

<code># losetup /dev/loop0 image.raw -o 518192640 --sizelimit 106928128</code>

<p>
You probably wonder where those insane numbers come from. It&#8217;s actually quite simple: The image file is, of course, just one continuous stream of bytes. A certain range of bytes within this stream represents the <em>sda2</em> partition we just created on the image file. We don&#8217;t want the loopback device to point at the whole image file, but rather on the <em>sda2</em> section only. And this section starts at byte 518192640 (the offset) and ends 106928128 bytes later (the sizelimit). How do we know? This is the calculation:
</p>

<code>offset = partition start block * 512
sizelimit = (partition end block - partition start block) * 512</code>

<p>
See the output of <em>fdisk -l image.raw</em> above for the partition start and end block numbers.
</p>

<p>
We now have a loopback device <em>/dev/loop0</em> that looks and feels just like a real physical device &#8211; in this case, a hard disk partition. As this, it can be formatted:
</p>

<code># mkfs.ext3 /dev/loop0</code>

<p>
Great, so now we have the <em>/boot</em> partition of our virtual server available, with the same layout as on our physical server. Let&#8217;s tackle the LVM volume on <em>/dev/sda3</em> next.
</p>

<p>
What we need is the LVM header from our physical server&#8217;s disk. Again, <em>dd</em> is the tool of choice:
</p>

<code># dd if=/dev/sda3 of=lvmheader.bin bs=512 count=24</code>

<p>
This writes the first 24 blocks of 512 bytes into the file <em>lvmheader.bin</em>. It&#8217;s the part if partition <em>sda3</em> where the layout of the LVM setup is described. Just like the MBR, this needs to be transferred to our KVM host and must be written to the right place within our raw image file.
</p>

<p>
To do so, we will create another loopback device, <em>/dev/loop1</em>, which points at the byte section for the <em>sda3</em> partition within our image file:
</p>

<code># losetup /dev/loop0 image.raw -o 625121280 --sizelimit 298807971328</code>

<p>
The numbers were calculated accordingly, of course.
</p>

<p>
Now we can write the LVM header:
</p>

<code># dd if=lvmheader.bin of=/dev/loop1 bs=512 count=24 conv=notrunc</code>

<p>
Afterwards, you can run
</p>

<code># pvs</code>

which should display the newly found LVM volume group:

<code>  PV         VG         Fmt  Attr PSize   PFree
  /dev/loop0 VolGroup00 lvm2 a-   278.28g    0</code>

<p>
Run
</p>

<code># lvm vgchange -ay</code>

<p>
to activate it. Now, when running
</p>

<code># lvm lvs</code>

<p>
its volumes should be listed like this:
</p>

<code>  LV       VG         Attr   LSize   Origin Snap%  Move Log Copy%  Convert
  LogVol00 VolGroup00 -wi-a- 268.53g                                      
  LogVol01 VolGroup00 -wi-a-   9.75g</code>

<p>
At this point, the logical volume that will house the root partition can be accessed, and therefore we can format it:
</p>

<code># mkfs.ext3 /dev/mapper/VolGroup00-LogVol00</code>

<p>
With this, we reached the point where we can mount the root and the <em>/boot</em> partition from the image on our KVM host:
</p>

<code># mount /dev/mapper/VolGroup00-LogVol00 /mnt
# mkdir /mnt/boot
# mount /dev/loop0 /mnt/boot</code>

<p>
Next, we can copy over all the files from our physical server to the mounted image partitions on the KVM host. This could be done using rsync, for example:
</p>

<code># rsync -aAXvP / your.kvm.host:/mnt/ \
--delete \
--exclude={/dev/*,/proc/*,/sys/*,/tmp/*,/run/*,/mnt/*,/media/*,/lost+found,/home/*/.gvfs}</code>

<p>
The nice here is that you can transfer the files on-the-fly from the running system. Of course, at one point you need to make a &#8220;last sync&#8221; just before the virtual machine replaces the physical machine. However, you can transfer most of the data beforehand, and when the moment comes, you can shut down all services like databases, crond etc. and do the last sync in a relatively short time window.
</p>

<p>
Once this is done, we have a working, bootable KVM raw image. We could now unmount the partitions and import the image as a new virtual machine, like so:
</p>

<code># virt-install \
 -n mymachine \
 -r 512 \
 --os-variant rhel5.4 \
 --disk /var/lib/libvirt/images/image.raw,device=disk,bus=virtio,cache=none \
 --nonetworks \
 --graphics vnc,listen=0.0.0.0,port=5910 \
 --import \
 --prompt
</code>

<p>
In case the VM does <em>not</em> boot, one reason could be that its <em>initrd</em> does not have the <em>virtio</em> drivers and thus cannot access the virtual drive. In this case, you must build a new <em>initrd</em> as follows:
</p>

<p>If you still have the partitions mounted, bind the raw image file itself into the mounted filesystem, like so:</p>

<code># mkdir /mnt/images
# mount --bind /var/lib/libvirt/images/image.raw /mnt/images</code>

<p>
Now, chroot into your VM&#8217;s root partition:
</p>

<code># chroot /mnt</code>

<p>In there you must mount the <em>/proc</em> pseudo-filesystem, remove your current <em>initrd</em> image, and build a new one with the <em>virtio</em> drivers included:

<code># mount -t proc none /proc
# rm /boot/initrd.img
# mkinitrd --with virtio_pci --with virtio_blk /boot/initrd.img the.kernel.version
# umount /proc
# exit</code>

<p>
Don&#8217;t forget to unmount everything afterwards:
</p>

<code># umount /mnt/boot
# umount /mnt/images
# umount /mnt</code>]]></content>
		<link rel="replies" type="text/html" href="/2013/03/19/converting-a-running-physical-machine-to-a-kvm-virtual-machine/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2013/03/19/converting-a-running-physical-machine-to-a-kvm-virtual-machine/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Applying The Clean Architecture to Go applications]]></title>
		<link rel="alternate" type="text/html" href="/2012/09/28/applying-the-clean-architecture-to-go-applications/" />
		<id>/?p=723</id>
		<updated>2013-05-14T06:25:46Z</updated>
		<published>2012-09-28T07:34:09Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="Tutorial" /><category scheme="/" term="golang" />		<summary type="html"><![CDATA[I would like to contribute to Uncle Bob's concept of The Clean Architecture by demonstrating how its principles could be applied to an actual Go application.]]></summary>
		<content type="html" xml:base="/2012/09/28/applying-the-clean-architecture-to-go-applications/"><![CDATA[<h2>What this text is about</h2>

<p>
I would like to contribute to Uncle Bob&#8217;s concept of <a href="http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html">The Clean Architecture</a> by demonstrating how its principles could be applied to an actual Go application. It doesn&#8217;t make much sense to completely rephrase Uncle Bob&#8217;s blog post here, thus reading <a href="http://blog.8thlight.com/uncle-bob/2012/08/13/the-clean-architecture.html">his text</a> first is definitely a prerequisite for understanding mine.
</p>

<p>
In it, he mainly describes the Dependency Rule, which, when applied to an architecture where different areas of the software are organized as circles within other circles, says <em>&#8220;&#8230;that source code dependencies can only point inwards. Nothing in an inner circle can know anything at all about something in an outer circle. In particular, the name of something declared in an outer circle must not be mentioned by the code in an inner circle. That includes functions, classes, variables, or any other named software entity.&#8221;</em>
</p>

<p>
In my opinion, the Dependency Rule is the single most important rule that must be applied when building software systems whose parts are to be testable and independent of frameworks, UIs, or databases. When following this rule, one ends up with a loosely coupled system with clear separation of concerns.
</p>

<h2>Decoupled systems</h2>

<p>
Systems whose parts are testable and loosely coupled are systems that can grow without pain, that is, systems which can be easily understood, modified, extended, and scaled. I will try to demonstrate that these qualities do in fact emerge when the Dependency Rule is applied.
</p>

<p>
To do so, I will guide you through the creation of a simple yet complete Go application, while reasoning on when, how and why the concepts of The Clean Architecture should be applied.
</p>

<p>
The application is a very (very!) rudimentary but working online shopping system which allows to retrieve a list of items, belonging to an order, through an HTTP web service. You can find the source code of the final application, including some unit tests, at <a href="https://github.com/ManuelKiessling/go-cleanarchitecture">https://github.com/ManuelKiessling/go-cleanarchitecture</a>.
</p>

<p>
In order to keep the code graspable, other use cases, like browsing the shop, checkout, or payment, are not implemented. Also, I concentrated on implementing those parts of the code that help explain the discussion of architecture &#8211; thus, the code lacks a lot in other regards, for example, there is a lot less error handling than one would expect in a decent application. It also contains a lot of redundancy &#8211; that clearly is a code smell, but it allows to read the code from top to bottom without the need to follow redundancy-reducing indirections.
</p>

<h2>Architecture of the example application</h2>

<p>
Let&#8217;s start by looking at the different areas of our software and their respective place within the architecture. The architecture of our software will be separated into four layers: domain, use cases, interfaces and infrastructure. We will discuss each layer from a high-level perspective, starting at the innermost layer. We will then look at the actual low-level code implementation of each layer, again moving from inner to outer layers.
</p>

<p>
The domain, or business, of our shopping application is that of human beings shopping for stuff, or, in more formal terms, of customers adding items to orders. We need to represent these business entities and their rules as code in the innermost layer, the domain layer.
</p>

<h2>What to put where, and why</h2>

<p>
I chose to talk about customers, and not users. While our application is of course going to have users, these are not of interest when talking about the domain of the application. I believe that if we want to take separation of concerns seriously, we must be very precise when thinking about what to put into which layer &#8211; &#8220;user&#8221; is a concept that has to do with the use cases, but not with the business itself.
</p>

<p>
Not surprisingly, as software developers, we are used to view things from a very software-centric point of view. Thus, when reasoning about architecture, whenever I feel that I might trap into an obvious decision that might turn out to be very problematic in the long run due to some nasty subtleties, I try to make up metaphors that don&#8217;t include computers. What if, for example, the business domain would not be part of a software program, but part of a board game?
</p>

<p>
Imagine we would have to implement eBay or Amazon as a board game &#8211; then the things that would be at the core of the implementation, no matter if it was a computer application or a board game, make up our business domain.
</p>

<p>
eBay the website and eBay the board game both need buyers, sellers, items, and bids &#8211; but only eBay the website needs users, sessions, cookies, login/logout etc.
</p>

<p>
I call those differences subtle because, when your program is still small, deciding that users and customers could as well be the same thing doesn&#8217;t seem like a big deal. It&#8217;s one of those things that only much later turn out to be painful to correct. The reason is that for 99% of the stuff the application needs to do, users and customers actually can be treated as being the same &#8211; but them being treated the same does not mean that they are the same, and not realizing this distinction will pay out negatively as soon as you reach the point where the remaining 1% comes into play. Our application will have an example of this.
</p>

<p>
So, while orders and items belong into the domain layer, users, representing a concept of the application at hand, belong into the next layer, use cases.
</p>

<p>
What else belongs into the use cases layer? The use cases layer is the location within our software where the use cases are implemented that arise from the fact that users of the software want to actually &#8220;do&#8221; something with the entities of the underlying domain. An example for a use case might be &#8220;a customer adds items to an order&#8221;. To realize this and other use cases, methods are needed that put the business entities in motion.
</p>

<p>
While this could be implemented within the domain layer, I recommend against it. The main reason is that use cases are application-specific, while domain entities are not. Imagine two applications for our shop, one that allows customers to browse and buy stuff, and another one that is used by administrators to manage and fulfill placed orders. While the domain entities remain the same in both applications, the use cases are very different: &#8220;add item to order&#8221; versus &#8220;mark order as shipped&#8221;, for example.
The domain and use cases layers together form the core of our application, representing the realities of the business we are operating in. Everything else is implementation details that are not related to the nature of our business. Our shop might be implemented as a web site or as a standalone GUI application &#8211; as long as we do not change the domain entities and the application&#8217;s use cases, it&#8217;s still the very same shop, business-wise.
</p>

<p>
We might switch the web service implementation from HTTP to SPDY, or the database from MySQL to Oracle &#8211; this doesn&#8217;t change the fact that we have a shop with customers that have orders which contain items (domain), and customers that are allowed to place orders, change quantities, and pay (use cases).
</p>

<p>
At the same time, this is the litmus test for our inner layers &#8211; do we have to change even just a single line of code within the use cases or domain layers when switching from MySQL to Oracle (or flat file)?
</p>

<p>
If the answer is yes, then we violated the Dependency Rule, because we made at least parts of our inner layers depend on parts in the outer layers.
</p>

<p>
But of course, there is a place for the code that works with the database or handles HTTP requests. The parts of our application that interact with external agencies like a web or database server live inside the interfaces layer.
</p>

<p>
For example, if our shop is made available as a web site, the controllers that handle an incoming HTTP request have their place within the interfaces layer, because they form an interface between the HTTP server and the application layer. It is events from the outer world, triggered by HTTP requests, mouse clicks in a GUI or remote procedure calls that make our shop run. Without these, the methods in the use cases layer and entities in the domain layer would just &#8220;sit there&#8221;, doing nothing. But because elements within these inner layers may not interact with or even know about anything in the outer world, interfaces are needed that transform events in the outer world into actions within the inner layers.
</p>

<p>
If we would like to store our shop&#8217;s data, like its items, orders and users, into a database, we also need an interface to the database. This is where application of the Dependency Rule becomes particularly interesting: If the code that builds the underlying SQL statements lives in the interfaces layer, and nothing in the application layer is allowed to call anything in an outer layer, but triggering the persisting of a domain entity does take place in the use cases layer &#8211; then how can we avoid violating the Dependency Rule? We will look into this in detail when going through the code.
</p>

<p>
The last layer is called infrastructure. Distinguishing what belongs to interfaces and what belongs to infrastructure isn&#8217;t always straightforward. The definition that makes sense for me is that both contain code that interacts with the outer world, like code that talks to a database, but while the code in interfaces is specific to your program at hand, infrastructure code is not and can be used in completely different applications. For example, while the functions that will handle the HTTP requests to our web service only make sense within our application, net/http from the Go Standard Library is general-purpose code that can be used to create web services for any application. In this sense, large parts of the Go Standard Library lie, conceptually, within the infrastructure layer.
</p>

<p>
Let&#8217;s sum this all up by creating a list of all layers and the parts of our software therein:
</p>

<p>
<ul>

<li>
Domain:
<ul>
<li>Customer entity</li>
<li>Item entity</li>
<li>Order entity</li>
</ul>
</li>

<li>
Use Cases:
<ul>
<li>User entity</li>
<li>Use case: Add Item to Order</li>
<li>Use case: Get Items in Order</li>
<li>Use case: Admin adds Item to Order</li>
</ul>
</li>

<li>
Interfaces:
<ul>
<li>Web Services for Item/Order handling</li>
<li>Repositories for Use Cases and Domain entities persistence</li>
</ul>
</li>

<li>
Infrastructure:
<ul>
<li>The Database</li>
<li>Code that handles DB connections</li>
<li>The HTTP server</li>
<li>Go Standard Library</li>
</ul>
</li>

</ul>
</p>

<p>
As you can see, this list includes some elements I have not yet talked about &#8211; the admin use case and the repositories will be explained in detail when we discuss the implementation.
</p>

<p>
One last thought before we dive into the code. If we look at how we separated our application, there&#8217;s a pattern here. If you look at the several layers and align them along the dimensions of how application-specific the contained code is and how business-specific it is, the pattern becomes apparent:
</p>

<p>
<table>
<tr>
<td>
<strong>
Infrastructure
</strong>
</td>
<td>
<strong>
Interfaces
</strong>
</td>
<td>
<strong>
Use Cases
</strong>
</td>
<td>
<strong>
Domain
</strong>
</td>
</tr>
</thead>
<tr>
<td style="color: #00f;">
application-agnostic
</td>
<td style="color: #090;">
application-specific
</td>
<td style="color: #090;">
application-specific
</td>
<td style="color: #00f;">
application-agnostic
</td>
</tr>
<tr>
<td style="color: #00f;">
business-agnostic
</td>
<td style="color: #00f;">
business-agnostic
</td>
<td style="color: #090;">
business-specific
</td>
<td style="color: #090;">
business-specific
</td>
</tr>
</table>
</p>

<p>
The more you move to the left, the more low-level the code becomes (&#8220;put that byte on the wire on port 80&#8230;&#8221;), the more you move to the right, the more high-level it becomes (&#8220;add item to order&#8230;&#8221;).
</p>


<h2>Implementing the architecture</h2>

<h3>The domain</h3>

<p>
We will first create the domain layer. As said, our application and its use cases will be fully working, but it won&#8217;t be a complete shop. Therefore, the code that defines our domain will be short enough to justify putting it into a single file:
</p>

<p>
<span class="filename beforecode">$GOPATH/src/domain/domain.go</span>
<code>package domain

import (
	"errors"
)

type CustomerRepository interface {
	Store(customer Customer)
	FindById(id int) Customer
}

type ItemRepository interface {
	Store(item Item)
	FindById(id int) Item
}

type OrderRepository interface {
	Store(order Order)
	FindById(id int) Order
}

type Customer struct {
	Id   int
	Name string
}

type Item struct {
	Id        int
	Name      string
	Value     float64
	Available bool
}

type Order struct {
	Id       int
	Customer Customer
	Items    []Item
}

func (order *Order) Add(item Item) error {
	if !item.Available {
		return errors.New("Cannot add unavailable items to order")
	}
	if order.value()+item.Value > 250.00 {
		return errors.New(`An order may not exceed
			a total value of $250.00`)
	}
	order.Items = append(order.Items, item)
	return nil
}

func (order *Order) value() float64 {
	sum := 0.0
	for i := range order.Items {
		sum = sum + order.Items[i].Value
	}
	return sum
}
</code>
</p>

<p>
It&#8217;s immediately apparent that this code does not depend on anything significant &#8211; we only import the &#8220;errors&#8221; package because some methods return an error. Although the domain entities described here will end up as rows in a database, there is no database-related code in sight.
</p>

<p>
Instead, we define Go interfaces for three so-called repositories. A repository is a concept from <a href="http://en.wikipedia.org/wiki/Domain-driven_design#Building_blocks_of_DDD">Domain-driven Design</a>: it abstracts away the fact that domain entities need to be saved to or loaded from some kind of persistence mechanism. From the domain&#8217;s point of view, a repository is just a container where domain entities come from (FindById) or go to (Store).
</p>

<p>
CustomerRepository, ItemRepository and OrderRepository are only interfaces. They will be implemented in the interfaces layer, because their implementation is an interface between the database and the application. This is how the Dependency Rule can be applied in Go applications &#8211; an abstract interface that does not refer to anything in outer layers is defined within an inner layer; its implementation is defined in an outer layer. The implementation is then injected into the layer that wants to use it; in this case, as we will see later, that&#8217;s the use cases layer.
</p>

<p>
This way, the use cases layer can refer to a concept of the domain layer &#8211; repositories &#8211; while using only the language of the domain layer. Still, the actual code executed is in the interfaces layer.
</p>

<p>
For every part in every layer, there are three questions of interest: where is it used, where is its interface, where is its implementation?
</p>

<p>
If we look at the OrderRepository, the answers are as follows: it&#8217;s used by the use cases layer, its interface belongs to the domain layer, and its implementation belongs to the interfaces layer.
</p>

<p>
The Add method of the Order entity, on the other hand, is used by the uses cases layer, too, and also, its interface belongs to the domain layer. But, its implementation belongs there as well, because it doesn&#8217;t need anything outside the domain layer itself.
</p>

<p>
The repository interface declarations are followed by three structs: Customer, Order, and Item. These represent our three domain entities. The Order entity comes with some additional behaviour in form of two methods, <em>Add</em> and <em>value</em>, the latter being only a helper function for internal use. <em>Add</em> implements a domain-specific function that is needed by the use cases.
</p>

<p>
There are some additional details in this code that are relevant when talking about the overall architecture. As you can see, we added some rules to the Add method. As we will see, our application has several rules in several places, and it&#8217;s interesting to discuss which rules belong where.
</p>

<p>
The first rule here refuses to add those items to a order that are not available &#8211; this is clearly a business rule. Not allowing customers to order unavailable items is a rule that applies to the web shop as well as to orders placed via a telephone hotline; it&#8217;s nothing that&#8217;s specific to (our) software &#8211; it&#8217;s a rule we decided to enforce business-wise.
</p>

<p>
The same goes for the rule that orders may not exceed a total value of $250 &#8211; no matter if our shop is a web site or a board game, it&#8217;s a business rule that always applies.
</p>

<p>
Other rules live in other places &#8211; somewhere, the value of an item has to be saved to a database, and we must take care to only store floats to the value field within our database; however, this is a technical rule, not a business rule, and does not belong into our domain package.
</p>

<p>
On the other hand, the database interface code and the database itself would happily obey when asked to persist orders whose items exceed a total value of $250 &#8211; as this is a business rule, the database and the according interface code simply must not care about it. This example makes a very strong case for what I like so much about what Uncle Bob preaches, because, just imagine doing the exact opposite &#8211; for example, adding the $250 order limit rule as a stored procedure in the database. Good luck getting a complete picture of all your business rules once your application grows large. I prefer having it all in one place any day.
</p>

<h3>The use cases</h3>

<p>
Let&#8217;s now look at the code of the use cases layer &#8211; again, this perfectly fits into one file:
</p>

<p>
<span class="filename beforecode">$GOPATH/src/usecases/usecases.go</span>
<code>package usecases

import (
	"domain"
	"fmt"
)

type UserRepository interface {
	Store(user User)
	FindById(id int) User
}

type User struct {
	Id       int
	IsAdmin  bool
	Customer domain.Customer
}

type Item struct {
	Id    int
	Name  string
	Value float64
}

type Logger interface {
	Log(message string) error
}

type OrderInteractor struct {
	UserRepository  UserRepository
	OrderRepository domain.OrderRepository
	ItemRepository  domain.ItemRepository
	Logger          Logger
}

func (interactor *OrderInteractor) Items(userId, orderId int) ([]Item, error) {
	var items []Item
	user := interactor.UserRepository.FindById(userId)
	order := interactor.OrderRepository.FindById(orderId)
	if user.Customer.Id != order.Customer.Id {
		message := "User #%i (customer #%i) "
		message += "is not allowed to see items "
		message += "in order #%i (of customer #%i)"
		err := fmt.Errorf(message,
			user.Id,
			user.Customer.Id,
			order.Id,
			order.Customer.Id)
		interactor.Logger.Log(err.Error())
		items = make([]Item, 0)
		return items, err
	}
	items = make([]Item, len(order.Items))
	for i, item := range order.Items {
		items[i] = Item{item.Id, item.Name, item.Value}
	}
	return items, nil
}

func (interactor *OrderInteractor) Add(userId, orderId, itemId int) error {
	var message string
	user := interactor.UserRepository.FindById(userId)
	order := interactor.OrderRepository.FindById(orderId)
	if user.Customer.Id != order.Customer.Id {
		message = "User #%i (customer #%i) "
		message += "is not allowed to add items "
		message += "to order #%i (of customer #%i)"
		err := fmt.Errorf(message,
			user.Id,
			user.Customer.Id,
			order.Id,
			order.Customer.Id)
		interactor.Logger.Log(err.Error())
		return err
	}
	item := interactor.ItemRepository.FindById(itemId)
	if domainErr := order.Add(item); domainErr != nil {
		message = "Could not add item #%i "
		message += "to order #%i (of customer #%i) "
		message += "as user #%i because a business "
		message += "rule was violated: '%s'"
		err := fmt.Errorf(message,
			item.Id,
			order.Id,
			order.Customer.Id,
			user.Id,
			domainErr.Error())
		interactor.Logger.Log(err.Error())
		return err
	}
	interactor.OrderRepository.Store(order)
	interactor.Logger.Log(fmt.Sprintf(
		"User added item '%s' (#%i) to order #%i",
		item.Name, item.Id, order.Id))
	return nil
}

type AdminOrderInteractor struct {
	OrderInteractor
}

func (interactor *AdminOrderInteractor) Add(userId, orderId, itemId int) error {
	var message string
	user := interactor.UserRepository.FindById(userId)
	order := interactor.OrderRepository.FindById(orderId)
	if !user.IsAdmin {
		message = "User #%i (customer #%i) "
		message += "is not allowed to add items "
		message += "to order #%i (of customer #%i), "
		message += "because he is not an administrator"
		err := fmt.Errorf(message,
			user.Id,
			user.Customer.Id,
			order.Id,
			order.Customer.Id)
		interactor.Logger.Log(err.Error())
		return err
	}
	item := interactor.ItemRepository.FindById(itemId)
	if domainErr := order.Add(item); domainErr != nil {
		message = "Could not add item #%i "
		message += "to order #%i (of customer #%i) "
		message += "as user #%i because a business "
		message += "rule was violated: '%s'"
		err := fmt.Errorf(message,
			item.Id,
			order.Id,
			order.Customer.Id,
			user.Id,
			domainErr.Error())
		interactor.Logger.Log(err.Error())
		return err
	}
	interactor.OrderRepository.Store(order)
	interactor.Logger.Log(fmt.Sprintf(
		"Admin added item '%s' (#%i) to order #%i",
		item.Name, item.Id, order.Id))
	return nil
}
</code>
</p>

<p>
The use cases layer for our shop mainly consists of a User entity and two use cases. The entity has a repository, just like the entities from the domain layer, because users need to be stored to and loaded from a persistence mechanism.
</p>

<p>
The use cases are, not surprisingly, functions, i.e., methods on the OrderInteractor struct. That&#8217;s not a must &#8211; they could be realized as unbound functions as well. However, attaching them to a struct eases injection of certain dependencies, as we will see.
</p>

<p>
The code above is a prime example for a &#8220;what to put where&#8221; discussion that lies at the heart of software architecture musings. First of all, the externalities all need to be injected into the OrderInteractor and AdminOrderInteractor by outer layers, and the structs only name things from the use cases layer and inwards. Again, this is all about the Dependency Rule. The way this package is set up, it doesn&#8217;t depend on anything outside the domain or the use cases itself &#8211; it can, for example, be tested using mocked repositories, or the actual implementation of the Logger could be exchanged without hassle, that is, without the need to change anything in the above code.
</p>

<p>
Bob Martin writes that use cases <em>&#8220;&#8230;orchestrate the flow of data to and from the entities, and direct those entities to use their enterprise wide business rules to achieve the goals of the use case.&#8221;</em>
</p>

<p>
If you look at, say, the Add method of OrderInteractor, you see this in action. The method does the orchestration of getting the required objects and putting them to work in a sensible way for the use case to be fulfilled. It manages the error cases that may arise for this specific use case, and it enforces certain rules &#8211; however, it&#8217;s important to note which rules. The $250 limit rule is handled in the domain layer, because that&#8217;s a business rule that transcends all use cases. Checking which users may add items to an order is, on the other hand, use case specific &#8211; plus, it contains an entity, User, that the domain layer must not be bothered with. It&#8217;s therefore handled in the use cases layer, and it&#8217;s handled differently depending on whether a normal user or an admin user tries to add items.
</p>

<p>
Let&#8217;s also discuss how logging is handled in this layer. In software applications, all kinds of logging takes place within several layers. While all log entries might end up in a text file on a hard drive, again it&#8217;s important to separate the technical from the conceptual details. Our use cases layer doesn&#8217;t know about text files and hard drives. Conceptually, this layer just says: &#8220;Regarding the application use cases, something interesting just happened, and I would like to have this event logged&#8221;, where &#8220;logged&#8221; does not mean &#8220;written somewhere&#8221;, it just means &#8220;logged&#8221; &#8211; without any further thought about what this actually means implementation-wise.
</p>

<p>
Thus, we just provide an interface that satisfies the needs of the use cases, and inject the actual implementation &#8211; this way we can, at any point in the future, and no matter how complex the application has become, decide to start writing our log messages into a database instead of a flat file &#8211; as long as we still satisfy the interface that its callers expect from the implementation, we don&#8217;t need to change even one line within any inner layers.
</p>

<p>
The way we&#8217;ve set up the two different order interactors here, even more niceties arise. If we would like to log admin operations into one file, and normal user operations into another one, then this is very simple. We would just have to create two different Logger implementations, both satisfying the usecases.Logger interface, and inject them into the interactors accordingly.
</p>

<p>
Another important detail in the use cases code is the Item struct. Don&#8217;t we already have one in the domain layer? Why not just return these in the Items() method? Because it&#8217;s wise to not leak domain entities into higher level layers. Entities might carry with them not only data, but also behaviour. This behaviour should only be triggered by use cases. If we don&#8217;t export our entities into upper layers in the first place, we make sure that this will always be the case.
The upper layers only need dumb data structure to do their job, therefore, this is all we should serve them.
</p>

<p>
As with the domain layer, this code shows how a clean architecture helps to understand how a given software actually works: while we only need to look into the domain layer code to see what parts our business is made of and which rules it has, we only need to look into the use cases code to see all the interactions that are possible between a user and the business. We can see that this application allows customers themselves to add items to an order and list items within an order, and that administrators may add items to an order for customers. Print it out and you have an up-to-date documentation of all your use cases in the most reliable and accurate format possible.
</p>

<h3>The interfaces</h3>

<p>
At this point, everything that has to be said, code wise, about our actual business and our application use cases, is said. Let&#8217;s see what that means for the interfaces layer&#8217;s code. While all code in the respective inner layers logically belongs together, the interfaces layer consists of several parts that exist separately &#8211; therefore, we will split the code in this layer into several files.
</p>

<p>
As our shop has to be accessible through the web, let&#8217;s start with the web service:
</p>

<p>
<span class="filename beforecode">$GOPATH/src/interfaces/webservice.go</span>
<code>package interfaces

import (
	"fmt"
	"io"
	"net/http"
	"strconv"
	"usecases"
)

type OrderInteractor interface {
	Items(userId, orderId int) ([]usecases.Item, error)
	Add(userId, orderId, itemId int) error
}

type WebserviceHandler struct {
	OrderInteractor OrderInteractor
}

func (handler WebserviceHandler) ShowOrder(res http.ResponseWriter, req *http.Request) {
	userId, _ := strconv.Atoi(req.FormValue("userId"))
	orderId, _ := strconv.Atoi(req.FormValue("orderId"))
	items, _ := handler.OrderInteractor.Items(userId, orderId)
	for _, item := range items {
		io.WriteString(res, fmt.Sprintf("item id: %d\n", item.Id))
		io.WriteString(res, fmt.Sprintf("item name: %v\n", item.Name))
		io.WriteString(res, fmt.Sprintf("item value: %f\n", item.Value))
	}
}
</code>

<p>
We are not going to implement all web services here, because they all look more or less the same. In a real application, adding an item to an order, and the show order use case for administration, need to be made available as web services, too, of course.
</p>

<p>
The most notable thing about what this code does is that it really doesn&#8217;t do much! Interfaces, if done right, tend to be simple, because their main task is to simply transport and translate data between layers. This is the case here. What happens here is that the code does what it takes to make the fact that an HTTP call arrived unrecognizable for the use cases layer.
</p>

<p>
Note that once again, injection is used to handle dependencies. The order interactor would be the real usecases.OrderInteractor in the production environment, but it could be mocked in the unit tests, making the web service handler testable in isolation, which means that its unit tests would only test the behaviour of the web service handler itself (&#8220;does it really use the â€˜userId&#8217; request parameter as the first parameter for the call to OrderInteractor.Items?&#8221;).
</p>

<p>
It&#8217;s worth discussing what a full fledged web service handler could look like. There is no authentication here, we just trust the userId parameter from the request to be valid &#8211; in a real world application, the web service handler would probably extract the requesting user from the session, which is transported using, e.g., cookies.
</p>

<p>
Whoa, wait, we already have customers and users, now we also have sessions and cookies? All the while these are more or less the same?
</p>

<p>
Well, only more or less, that&#8217;s the point. Each of them lives on a different conceptual level. Cookies are a very low-level mechanism, dealing with a bag of bytes in some browser&#8217;s memory and HTTP headers. Sessions are already a bit more abstract, a concept of different stateless requests belonging to one client &#8211; with cookies used to sort out the details.
</p>

<p>
Users are already quite high-level &#8211; a very abstract idea of &#8220;an identifiable person interacting with the application&#8221; &#8211; with sessions used to sort out the details. And lastly, there is the customer, an entity that is recognized in pure business terms &#8211; with users used to&#8230; well, you get the idea.
</p>

<p>
I recommend making these differences explicit rather than dealing with the pain that arises when using the same representation on different conceptual levels. Should you choose to replace the session&#8217;s transport mechanism from cookies to SSL client certificates, you only need to introduce a new library for the low-level details of these certificates to your infrastructure layer, and have to change the code in the interfaces layer that identifies sessions based on those low-level HTTP details &#8211; users and customers are not tangent to this change.
</p>

<p>
Also in your interfaces layer lives the code that creates HTML responses from data structures it receives from the use cases layer. In a real application, that&#8217;s probably done by using a templating library that lives in the infrastructure layer.
</p>

<p>
Let&#8217;s now look at the last building block of our application: persistence. We have a working business domain, we have use cases that put the domain in motion, and we have implemented an interface that allows users to access our application over the web. Now all we need to do is implement the mechanisms that store our business and application data on a hard drive, and we are ready for an IPO.
</p>

<p>
This is done by creating the concrete implementations of the abstract repository interfaces of our domain and use cases layers. This implementation belongs to the interfaces layer, because repositories are an interface between the low level world of databases on the one side and the high level world of our business on the other side &#8211; what is a stream of bytes on a hard drive on the one side must become an entity object on the other. The job of transforming the one into the other is that of a repository.
</p>

<p>
Some repository implementations might be limited, in their dependencies, to the interfaces layer and below, for example when writing pure in-memory runtime object caches, or when mocking a repository for a unit test. Most real world repositories however need to talk to an external persistence mechanism like a database, probably by using a library that handles the low level connection and query details &#8211; and which lives in the infrastructure layer of the system. Thus, as in other layers, we once again need to make sure that we do not violate the Dependency Rule.
</p>

<p>
It&#8217;s not that the repository is database-agnostic! It&#8217;s well aware of the fact that it talks to an SQL database. But it is directly concerned only with the high level, or, one could say, &#8220;logical&#8221; aspects of this conversation. Get data from this table, put data into that table. The low level, or &#8220;physical&#8221;, aspects, are out of its scope &#8211; stuff like connecting to the database daemon through the network, deciding to use a slave for reads and the master for writes, handling timeouts, and so forth, are infrastructural issues.
</p>

<p>
In other words, our repository would like to use a reasonably high level interface that hides all those nasty little infrastructural details and just talk some SQL to what appears to be a server that is just there and just works.
</p>

<p>
Let&#8217;s create such an interface in src/interfaces/repositories.go:
</p>

<p>
<code>type DbHandler interface {
  Execute(statement string)
  Query(statement string) Row 
}

type Row interface {
  Scan(dest ...interface{})
  Next() bool
}
</code>
</p>

<p>
That&#8217;s really a very limited interface, but it allows for all the operations the repositories need to perform: reading, inserting, updating and deleting rows.
</p>

<p>
In the infrastructure layer, we will implement some glue code that uses a sqlite3 library to actually talk to the database, while satisfying this interface &#8211; but first, let&#8217;s fully implement the repositories:
</p>


<p>
<span class="filename beforecode">$GOPATH/src/interfaces/repositories.go</span>
<code>package interfaces

import (
	"domain"
	"fmt"
	"usecases"
)

type DbHandler interface {
	Execute(statement string)
	Query(statement string) Row
}

type Row interface {
	Scan(dest ...interface{})
	Next() bool
}

type DbRepo struct {
	dbHandlers map[string]DbHandler
	dbHandler  DbHandler
}

type DbUserRepo DbRepo
type DbCustomerRepo DbRepo
type DbOrderRepo DbRepo
type DbItemRepo DbRepo

func NewDbUserRepo(dbHandlers map[string]DbHandler) *DbUserRepo {
	dbUserRepo := new(DbUserRepo)
	dbUserRepo.dbHandlers = dbHandlers
	dbUserRepo.dbHandler = dbHandlers["DbUserRepo"]
	return dbUserRepo
}

func (repo *DbUserRepo) Store(user usecases.User) {
	isAdmin := "no"
	if user.IsAdmin {
		isAdmin = "yes"
	}
	repo.dbHandler.Execute(fmt.Sprintf(`INSERT INTO users (id, customer_id, is_admin)
	                                    VALUES ('%d', '%d', '%v')`,
	                                    user.Id, user.Customer.Id, isAdmin))
	customerRepo := NewDbCustomerRepo(repo.dbHandlers)
	customerRepo.Store(user.Customer)
}

func (repo *DbUserRepo) FindById(id int) usecases.User {
	row := repo.dbHandler.Query(fmt.Sprintf(`SELECT is_admin, customer_id
	                                         FROM users WHERE id = '%d' LIMIT 1`,
	                                         id))
	var isAdmin string
	var customerId int
	row.Next()
	row.Scan(&#038;isAdmin, &#038;customerId)
	customerRepo := NewDbCustomerRepo(repo.dbHandlers)
	u := usecases.User{Id: id, Customer: customerRepo.FindById(customerId)}
	u.IsAdmin = false
	if isAdmin == "yes" {
		u.IsAdmin = true
	}
	return u
}

func NewDbCustomerRepo(dbHandlers map[string]DbHandler) *DbCustomerRepo {
	dbCustomerRepo := new(DbCustomerRepo)
	dbCustomerRepo.dbHandlers = dbHandlers
	dbCustomerRepo.dbHandler = dbHandlers["DbCustomerRepo"]
	return dbCustomerRepo
}

func (repo *DbCustomerRepo) Store(customer domain.Customer) {
	repo.dbHandler.Execute(fmt.Sprintf(`INSERT INTO customers (id, name)
	                                    VALUES ('%d', '%v')`,
	                                    customer.Id, customer.Name))
}

func (repo *DbCustomerRepo) FindById(id int) domain.Customer {
	row := repo.dbHandler.Query(fmt.Sprintf(`SELECT name FROM customers
	                                         WHERE id = '%d' LIMIT 1`,
	                                         id))
	var name string
	row.Next()
	row.Scan(&#038;name)
	return domain.Customer{Id: id, Name: name}
}

func NewDbOrderRepo(dbHandlers map[string]DbHandler) *DbOrderRepo {
	dbOrderRepo := new(DbOrderRepo)
	dbOrderRepo.dbHandlers = dbHandlers
	dbOrderRepo.dbHandler = dbHandlers["DbOrderRepo"]
	return dbOrderRepo
}

func (repo *DbOrderRepo) Store(order domain.Order) {
	repo.dbHandler.Execute(fmt.Sprintf(`INSERT INTO orders (id, customer_id)
	                                    VALUES ('%d', '%v')`,
	                                    order.Id, order.Customer.Id))
	for _, item := range order.Items {
		repo.dbHandler.Execute(fmt.Sprintf(`INSERT INTO items2orders (item_id, order_id)
		                                    VALUES ('%d', '%d')`,
		                                    item.Id, order.Id))
	}
}

func (repo *DbOrderRepo) FindById(id int) domain.Order {
	row := repo.dbHandler.Query(fmt.Sprintf(`SELECT customer_id FROM orders
	                                         WHERE id = '%d' LIMIT 1`,
	                                         id))
	var customerId int
	row.Next()
	row.Scan(&#038;customerId)
	customerRepo := NewDbCustomerRepo(repo.dbHandlers)
	order := domain.Order{Id: id, Customer: customerRepo.FindById(customerId)}
	var itemId int
	itemRepo := NewDbItemRepo(repo.dbHandlers)
	row = repo.dbHandler.Query(fmt.Sprintf(`SELECT item_id FROM items2orders
	                                        WHERE order_id = '%d'`,
	                                        order.Id))
	for row.Next() {
		row.Scan(&#038;itemId)
		order.Add(itemRepo.FindById(itemId))
	}
	return order
}

func NewDbItemRepo(dbHandlers map[string]DbHandler) *DbItemRepo {
	dbItemRepo := new(DbItemRepo)
	dbItemRepo.dbHandlers = dbHandlers
	dbItemRepo.dbHandler = dbHandlers["DbItemRepo"]
	return dbItemRepo
}

func (repo *DbItemRepo) Store(item domain.Item) {
	available := "no"
	if item.Available {
		available = "yes"
	}
	repo.dbHandler.Execute(fmt.Sprintf(`INSERT INTO items (id, name, value, available)
	                                    VALUES ('%d', '%v', '%f', '%v')`,
	                                    item.Id, item.Name, item.Value, available))
}

func (repo *DbItemRepo) FindById(id int) domain.Item {
	row := repo.dbHandler.Query(fmt.Sprintf(`SELECT name, value, available
	                                         FROM items WHERE id = '%d' LIMIT 1`,
	                                         id))
	var name string
	var value float64
	var available string
	row.Next()
	row.Scan(&#038;name, &#038;value, &#038;available)
	item := domain.Item{Id: id, Name: name, Value: value}
	item.Available = false
	if available == "yes" {
		item.Available = true
	}
	return item
}
</code>
</p>

<p>
I hear you: from more than one point of view, this is terrible code! A lot of duplication, no error handling, and several other smells. But the point of this tutorial is neither code style nor design patterns &#8211; it&#8217;s all about the architecture of the application, and therefore I took the freedom to create very simplistic code that only has to be straightforward and comprehensible, not elegant and clever &#8211; oh and yes, I&#8217;m still a Go beginner, which shows.
</p>

<p>
Note the dbHandlers map in every repository &#8211; that&#8217;s here so every repository can use every other repository without giving up on Dependency Injection &#8211; if some of the repositories use a different DbHandler implementation than others, then repositories using other repositories don&#8217;t need to know who uses what; it&#8217;s kind of a poor man&#8217;s Dependency Injection Container.
</p>

<p>
Let&#8217;s dissect one of the more interesting methods, DbUserRepo.FindById(). It&#8217;s a good example to illustrate that in our architecture, interfaces really are all about transforming data from one layer to the next. FindById reads database rows and produces domain and usescases entities. I have deliberately made the database representation of the User.IsAdmin attribute more complicated than neccessary, by storing it as &#8220;yes&#8221; and &#8220;no&#8221; varchars in the database. In the usecases entity User, it&#8217;s represented as a boolean value of course. Bridging the gap of these very different representations is the job of the repository.
</p>

<p>
User entities have a Customer attribute, which in turn is a domain entity; the User repository simply uses the Customer repository to retrieve the entity it needs.
<p>

<p>
It&#8217;s easy to imagine how our architecture can help us when the application grows. By following the Dependency Rule, we will be able to rework the details of entity persistence without the need to touch the entities themselves. We might decide to split the data of the User entities into multiple tables &#8211; the repository will have to sort out the details of putting together a single entity from multiple tables, but the clients of the repositories won&#8217;t be concerned.
</p>

<h3>The infrastructure</h3>

<p>
As stated above, our repositories view &#8220;The Database&#8221; as an abstract being where SQL queries can be send to and rows can be retrieved from. They don&#8217;t care about infrastructural issues like connecting to the database or even figuring out which database to use. This is done in src/infrastructure/sqlitehandler.go, where the high level DbHandler interface is implemented using low level means:
</p>

<p>
<span class="filename beforecode">$GOPATH/src/infrastructure/sqlitehandler.go</span>
<code>package infrastructure

import (
	"database/sql"
	"fmt"
	_ "github.com/mattn/go-sqlite3"
	"interfaces"
)

type SqliteHandler struct {
	Conn *sql.DB
}

func (handler *SqliteHandler) Execute(statement string) {
	handler.Conn.Exec(statement)
}

func (handler *SqliteHandler) Query(statement string) interfaces.Row {
	rows, err := handler.Conn.Query(statement)
	if err != nil {
		fmt.Println(err)
		return new(SqliteRow)
	}
	row := new(SqliteRow)
	row.Rows = rows
	return row
}

type SqliteRow struct {
	Rows *sql.Rows
}

func (r SqliteRow) Scan(dest ...interface{}) {
	r.Rows.Scan(dest...)
}

func (r SqliteRow) Next() bool {
	return r.Rows.Next()
}

func NewSqliteHandler(dbfileName string) *SqliteHandler {
	conn, _ := sql.Open("sqlite3", dbfileName)
	sqliteHandler := new(SqliteHandler)
	sqliteHandler.Conn = conn
	return sqliteHandler
}
</code>
</p>


<p>
(Again, zero error handling, among other things, in order to keep out code that doesn&#8217;t contribute to the architectural ideas).
</p>

<p>
Using Yasuhiro Matsumoto&#8217;s sqlite3 library, this infrastructure code implements the DbHandler interface that allows the repositories to talk to the database without the need to fiddle with low level details.
</p>

<h3>Putting it all together</h3>

<p>
That&#8217;s it, all our architectural building blocks are now in place &#8211; let&#8217;s put them together in main.go:
</p>

<p>
<span class="filename beforecode">$GOPATH/main.go</span>
<code>package main

import (
	"usecases"
	"interfaces"
	"infrastructure"
	"net/http"
)

func main() {
	dbHandler := infrastructure.NewSqliteHandler("/var/tmp/production.sqlite")

	handlers := make(map[string] interfaces.DbHandler)
	handlers["DbUserRepo"] = dbHandler
	handlers["DbCustomerRepo"] = dbHandler
	handlers["DbItemRepo"] = dbHandler
	handlers["DbOrderRepo"] = dbHandler

	orderInteractor := new(usecases.OrderInteractor)
	orderInteractor.UserRepository = interfaces.NewDbUserRepo(handlers)
	orderInteractor.ItemRepository = interfaces.NewDbItemRepo(handlers)
	orderInteractor.OrderRepository = interfaces.NewDbOrderRepo(handlers)

	webserviceHandler := interfaces.WebserviceHandler{}
	webserviceHandler.OrderInteractor = orderInteractor

	http.HandleFunc("/orders", func(res http.ResponseWriter, req *http.Request) {
		webserviceHandler.ShowOrder(res, req)
	})
	http.ListenAndServe(":8080", nil)
}
</code>
</p>

<p>
Due to our quite excessive use of dependency injection, some construction work is necessary before the building blocks of our application can start moving. Our repositories must be injected with a DbHandler implementation, and in turn, they are injected into the use case interactor. The interactor gets injected into the webservice handler, which is then set up to server a specific route. At last, the http server starts.
</p>

<p>
Boxes in boxes in boxes, and every single one can be exchanged with something that works completely different under the hood &#8211; as long as it serves the same API, it will work.
</p>

<p>
We can use the following SQL to create a minimal data set in /var/tmp/production.sqlite:
</p>

<p>
<code>CREATE TABLE users (id INTEGER, customer_id INTEGER, is_admin VARCHAR(3));
CREATE TABLE customers (id INTEGER, name VARCHAR(42));
CREATE TABLE orders (id INTEGER, customer_id INTEGER);
CREATE TABLE items (id INTEGER, name VARCHAR(42), value FLOAT, available VARCHAR(3));
CREATE TABLE items2orders (item_id INTEGER, order_id INTEGER);

INSERT INTO users (id, customer_id, is_admin) VALUES (40, 50, "yes");
INSERT INTO customers (id, name) VALUES (50, "John Doe");
INSERT INTO orders (id, customer_id) VALUES (60, 50);
INSERT INTO items (id, name, value, available) VALUES (101, "Soap", 4.99, "yes");
INSERT INTO items (id, name, value, available) VALUES (102, "Fork", 2.99, "yes");
INSERT INTO items (id, name, value, available) VALUES (103, "Bottle", 6.99, "no");
INSERT INTO items (id, name, value, available) VALUES (104, "Chair", 43.00, "yes");

INSERT INTO items2orders (item_id, order_id) VALUES (101, 60);
INSERT INTO items2orders (item_id, order_id) VALUES (104, 60);
</code>
</p>

<p>
Now, we can start the application, and point our browser at http://localhost:8080/orders?userId=40&#038;orderId=60. The result should be:
</p>

<p>
<pre>item id: 101
item name: Soap
item value: 4.990000
item id: 104
item name: Chair
item value: 43.000000
</pre>
</p>

<p>
And with this, it&#8217;s time to pat ourselves on the shoulder.
</p>

<h3>Afterthoughts</h3>

<p>
Which doesn&#8217;t mean that the application can&#8217;t be further improved. For example, repositories using other repositories is currently limited because all repositories must be DbHandler repositories; should we decide to store items in a MongoDB while keeping orders in a relational SQL database, then our DbOrderRepo can&#8217;t create the DbItemRepo the way it does; the solution would be to create a registry or dependency injection container that provides the full repositories, not only the db handlers.
</p>

<p>
However, we have created an architecture that allows such changes easily. Only very specific parts of the applications would need to be changed, without risking to break use cases or domain logic. Which is the beauty that is The Clean Architecture.
</p>

<h2>Acknowledgements</h2>

<p>
This tutorial would not exist if &#8220;Uncle&#8221; Bob Martin wouldn&#8217;t restlessly teach us how to do software development and software architecture.
</p>

<p>
Many people from the golang-nuts mailing list gave valuable feedback, among them, in no particular order: Gheorghe Postelnicu, Hannes Baldursson, Francesc Campoy Flores, Christoph Hack, Gaurav Garg, Paddy Foran, Sanjay Menakuru, Larry Clapp, Steven Degutis, Sanjay, Jesse McNelis, Mateusz CzapliÅ„ski, and Rob Pike. Jon Jagger has again been a critical and helpful mentor.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/09/28/applying-the-clean-architecture-to-go-applications/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/09/28/applying-the-clean-architecture-to-go-applications/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[A year of Go]]></title>
		<link rel="alternate" type="text/html" href="/2012/08/04/a-year-of-go/" />
		<id>/?p=717</id>
		<updated>2012-08-04T12:36:09Z</updated>
		<published>2012-08-04T12:01:59Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="golang" />		<summary type="html"><![CDATA[I went breadth-first now for a while,<br />and it's time to switch to depth-first.]]></summary>
		<content type="html" xml:base="/2012/08/04/a-year-of-go/"><![CDATA[<p>
When it comes to software technologies, I have a problem sticking with things. Shiny new stuff comes along way too often, and quite often I can&#8217;t resist. The last months were a self-inflicted tour de force from PHP to JavaScript, Node.js, C, Objective-C, Assembler, Clojure, Python, frontend, backend, web, mobile&#8230;
</p>
<p>
It also was a lot of fun, and I achieved things I was sure I wasn&#8217;t capable of, like <a href="http://www.nodebeginner.org">writing a book</a> and successfully selling it, or <a href="https://github.com/ManuelKiessling/jsDCPU16">implementing a software CPU</a>.
</p>
<p>
But it also stressed me, to a certain degree. I went breadth-first now for a while, and it&#8217;s time to switch to depth-first.
</p>
<p>
Which means starting with something new, once again, but then sticking with it for at least one year.
</p>
<p>
I truly believe that the Go programming language is here to stay and will be at the forefront of software development in the future. And I want to learn it, not only superficially, but truly and deeply. I&#8217;m currently working through <a href="http://www.amazon.com/gp/product/0321774639/ref=as_li_ss_tl?ie=UTF8&#038;camp=1789&#038;creative=390957&#038;creativeASIN=0321774639&#038;linkCode=as2&#038;tag=thenodbegboo-20">Programming in Go</a><img src="http://www.assoc-amazon.com/e/ir?t=thenodbegboo-20&#038;l=as2&#038;o=1&#038;a=0321774639" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />, forcing myself to contemplate about everything I learn until I fully understand it.
</p>
<p>
If my time (and intelligence) allow, I will blog about what I learn. While learning Go, I want to create a multivariate web-testing suite with it, which makes it likely that the blog series will be a step-by-step guide on how I create this suite &#8211; that is, if all goes well.
</p>
<p>
Wish me luck &#8211; and don&#8217;t show me shiny new things, please :-)
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/08/04/a-year-of-go/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/08/04/a-year-of-go/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Tutorial: Developing HTML5 Canvas Games for Facebook with JavaScript &#8211; Part 1]]></title>
		<link rel="alternate" type="text/html" href="/2012/04/02/tutorial-developing-html5-canvas-games-for-facebook-with-javascript-part-1/" />
		<id>http://172.16.233.129/wordpress/?p=533</id>
		<updated>2012-07-28T19:40:59Z</updated>
		<published>2012-04-02T13:34:10Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="Tutorial" />		<summary type="html"><![CDATA[Some days ago, my son asked me how computer games are made. The result is a simple yet fully-functional 2D space shooter, available on Facebook. Here's how it's done.]]></summary>
		<content type="html" xml:base="/2012/04/02/tutorial-developing-html5-canvas-games-for-facebook-with-javascript-part-1/"><![CDATA[<p class="introduction">
  This is a multi-part tutorial series. You are currently reading part 1.
</p>

<p>
  Some days ago, my son asked me how computer games are made. He&#8217;s five years old and couldn&#8217;t quite imagine
  how games are created, because <em>&#8220;you can&#8217;t just paint them like animation movies, can you?&#8221;</em>.
</p>

<p>
  I quickly realized that explaining it to a child is really difficult. And so I decided to create a game, with him
  as the <em>Creative Director</em> who decides what the game should be like.
</p>

<p>
  The result is a simple yet fully-functional 2D space shooter, available on Facebook. This tutorial describes how to
  create the game using the HTML5 canvas element and JavaScript, and how to integrate it into Facebook.
</p>

<p>
  I never did game development before, and for me, the two major takeaways from this experience are <strong>a) JavaScript/Canvas
  game development is much more simpler than you think</strong>, and <strong>b) Facebook integration actually is a lot of fun</strong>.
</p>

<p>
  I mention the second point because I always greeted Facebook development with smiles, but to be honest, it&#8217;s a very pleasing
  experience, and the whole social graph thing makes a lot of sense for games.
</p>

<p>
  But let&#8217;s talk about game development first. As said, it was way simpler than I thought &#8211; in fact, being the old web
  developer that I am, I always deemed it to be black art. Turned out, it&#8217;s not difficult, it&#8217;s just completely different
  &#8211; and therefore only <em>seems</em> difficult.
</p>

<p>
  When developing web services, you are used to think in terms of request and response, stateless connections, model-view-controller&#8230;
  &#8211; game development takes place in a completely different context.
</p>

<p>
  When looking at games, I always thought <em>&#8220;there is so much happening at the same time, so many objects interacting with each other
  &#8211; how do you develop code for this without ending up with a chaotic mess?&#8221;</em>.
</p>

<p>
  The most important thing to understand was the concept of the <strong>game loop</strong>, which lies at the heart of any
  action game: this loop continuously cycles through two steps: <em>update</em> and <em>draw</em>.
</p>

<p>
  And from a high-level perspective, both steps are very simple: <em>update</em> just takes every game object and asks it
  to update its state, that is, it asks <em>&#8220;coming from your current state, what will your state be in the next iteration of
  the game loop?&#8221;</em>. Every object takes care of its own state and how to change its state when going from one game loop cycle
  to the next.
</p>

<p>
  A very simple example is that of game object movement &#8211; let&#8217;s say you have a spaceship object with a
  certain state. Whenever this object is asked to update its state, it will set its position within the game world to new values,
  using a certain algorithm &#8211; e.g., it could have a <em>horizontal speed</em> value and a <em>vertical speed</em> value, which
  is the number of pixels the object moves within the game world in horizontal and vertical direction during one loop cycle.
</p>

<p>
  Thus, when the spaceship object is asked to update its state, it adds the value of its horizontal speed to its
  horizontal position, and the value of its vertical speed to its vertical position &#8211; this is all. Because this is done
  every game loop cycle, the result is a moving spaceship.
</p>

<p>
  Of course, the internal state of game objects might be influenced by external events &#8211; e.g., the player&#8217;s spaceship should
  only move if the user presses the arrow keys on his or her keyboard, and should only shoot whenever the space bar is
  pressed. These external events are delegated to the game object and handled there.
</p>

<p>
  A third type of update logic is that of object interactions &#8211; for our game, it&#8217;s all about object collisions: did a bullet hit an enemy?
  Did the player&#8217;s spaceship collide with an enemy ship? The collision logic triggers certain events within the affected game objects &#8211; e.g.,
  if the collision logic detects that an enemy is hit by a bullet from the player&#8217;s ship, it triggers the <em>beenHit()</em> method on that
  enemy. This, in turn, is just another external event which results in the enemy object changing its internal state &#8211; e.g. triggering the explosion
  of that enemy, or just lowering its life-points by 1; whatever the game rules are that you decide to implement.
</p>

<p>
  The second step of every loop cycle is the <em>draw</em> step. This one is even simpler: in every loop cycle, the screen is cleared, and the
  application cycles through every single game object and asks it to draw itself. Because objects have changed their internal state in the <em>update</em>
  step, their position (or color, or size, or orientation) has probably changed, and they are drawn at a different position than in the previous loop cycle. This
  results in the illusion of object movement.
</p>

<p>
  How often does the application cycle through the <em>update-&gt;draw</em> cycle? That&#8217;s simple &#8211; if your game runs at 60 frames per second,
  <em>update</em> and <em>draw</em> are called 60 times per second. And that is all.
</p>

<p>
  Let&#8217;s wrap this up with some example (and pseudo) code. Let&#8217;s say we have a game world that exists of only two game objects &#8211; a spaceship and a bullet.
</p>

<p>
  When the game starts, the spaceship is in the upper left corner of the world, and the bullet is at the very bottom of the world, at the horizontal center:
</p>

<p>
  <code>
 ---------------------------------------------------------
 |                                                       |
 |   P                                                   |
 |                                                       |
 |                                                       |
 |                                                       |
 |                                                       |
 |                                                       |
 |                                                       |
 |                                                       |
 |                           B                           |
 ---------------------------------------------------------
  </code>
</p>

<p>
  Where does the &#8220;world&#8221; actually come from? We will talk about that in a moment, let&#8217;s keep the discussion abstract for now.
</p>

<p>
  When thinking about placing and moving objects in a 2D world, you need some kind of coordinate system. Let&#8217;s say the
  horizontal axis of our world is called <strong>x</strong> and measured from 0 (left end of the world) to 54 (right end of the world),
  and the vertical axis is called <strong>y</strong> and measured from 0 (upper end of the world) to 9 (bottom end of the world).
</p>

<p>
  In this system, the Player <strong>P</strong> is currently located at <strong>x: 3</strong> and <strong>y: 1</strong>, and the bullet
  is at position <strong>x: 27, y: 9</strong>:
</p>

<p>
  <code>
  0123456789012345678901234567890123456789012345678901234  x â†’
 ---------------------------------------------------------
0|                                                       |
1|   P                                                   |
2|                                                       |
3|                                                       |
4|                                                       |
5|                                                       |
6|                                                       |
7|                                                       |
8|                                                       |
9|                           B                           |
 ---------------------------------------------------------

y
â†“
  </code>
</p>

<p>
  These x and y coordinates are the first and most fundamental attributes that make up the current state of each game object. We will later implement
  these as simple numerical attributes of the objects in our application that represent game objects.
</p>

<p>
  Ok, so our game objects have a basic state represented by their position coordinates. We want our objects to move when the game is running, or in
  our game programming lingo: we want their positional values to be altered when cycling through the update loop.
</p>

<p>
  How could this be implemented? As said, the game update loop iterates over the list of all game objects and calls their <em>update</em> method.
  Thus, the <em>update</em> method of the <em>Player</em> object is the place where the logic that alters the x and y value of this object should be
  implemented. The same applies to the <em>Bullet</em> object, respectively.
</p>

<p>
  For now, let&#8217;s ignore the fact that the player&#8217;s object should be steered by keystrokes. It just moves on it&#8217;s own using a very simple algorithm, implemented
  in its <em>update</em> method.
</p>

<p>
  What could this algorithm look like? Again, we keep it extremely simple for now. Let&#8217;s add two other attributes to each game object:
  <strong>xVelocity</strong> and <strong>yVelocity</strong>. They are the parameters for our incredibly sophisticated algorithm: xVelocity is the amount
  of pixels that the game object moves on the x-axis, and yVelocity &#8211; well, you get the idea. Here is the pseudo-code for the player object which starts at
  position x: 3, y: 1 and moves from the upper left corner of the game world to its lower right corner, one pixel per loop cycle:
</p>

<p>
  <code>
Player = {
  x: 3
  y: 1
  xVelocity: 1
  yVelocity: 1

  update = function() {
    x = x + xVelocity
    y = y + yVelocity
  }
}
  </code>
</p>

<p>
  And that&#8217;s it. The game, running at, say, 60 frames per seconds, asks the <em>Player</em> object 60 times per second to <em>update()</em> itself,
  resulting in a player moving diagonally across the screen. That is, if we implement some draw logic:
</p>

<p>
  <code>
Player = {
  x: 3
  y: 1
  xVelocity: 1
  yVelocity: 1
  symbol: 'P'

  update = function() {
    x = x + xVelocity
    y = y + yVelocity
  }

  draw = function() {
    drawAtPosition(x, y, symbol);
  }
}
  </code>
</p>

<p>
  After asking all game objects to update themselves, the game loop asks them to draw themselves &#8211; update-&gt;draw-&gt;update-&gt;draw-&gt;update-&gt;draw&#8230; and so on.
</p>

<p>
  Let me give you an idea of how the final game will look like:
</p>

<p>
<a href="/wp-content/uploads/2012/03/html5_canvas_javascript_tutorial_game_screenshot.jpg"><img src="/wp-content/uploads/2012/03/html5_canvas_javascript_tutorial_game_screenshot-300x259.jpg" alt="" title="html5 canvas javascript tutorial game screenshot" width="300" height="259" class="aligncenter size-medium wp-image-548" /></a>
</p>

<p>
  The game world is a 740 x 640 pixel canvas. The screenshot shows 5 game objects:
  <ul>
    <li>
      A terrain tile
    </li>
    <li>
      The player&#8217;s spaceship
    </li>
    <li>
      2 bullets shot by the player, flying upwards
    </li>
    <li>
      An enemy (which is likely to be hit by the bullets in a moment)
    </li>
  </ul>
</p>

<p>
  Remember, from the game loop point of view, these objects are all treated same: in every loop cycle, each one is asked to update itself, and then each one is
  asked to draw itself. In every loop cycle, the update method of the terrain tile moves the tile one pixel downwards (resulting in a scroll effect), the player&#8217;s ship is
  moved on the x and y axis depending on the arrow keys the player presses, the bullets simply fly upwards with the speed defined in their yVelocity attribute,
  and the enemy ship is changing its x and y position values depending on some randomized algorithm (simply to make its movements more interesting than
  just moving in a straight line). If the player presses the <em>space</em> key, a new bullet object is created and inserted into the game world, and if a bullet
  objects collides with an enemy object, the enemy object and bullet object disappear (sorry, explosions do not fall within the scope of this document).
</p>

<p>
  We will need to write quite some code for these mechanics to work, but at least the GFX part is dead simple: every game object on this screenshot is simply
  represented with a PNG image. Thanks to PNG&#8217;s alpha transparency, it&#8217;s very simple to draw the game world by drawing the image for each game object on
  top of each other: first the screen is cleared, then the terrain tile is drawn, then the enemy, then the bullets, then the player. The shadows of the player and
  enemy ship are nothing special &#8211; they are simply part of the PNG with the player&#8217;s ship and enemy ship, respectively, and are transparent by using the alpha
  channel of the images:
</p>

<p>
  <img src="/wp-content/uploads/2012/03/player2.png" alt="" title="html5 canvas javascript tutorial spaceship" width="256" height="256" class="aligncenter size-full wp-image-555" />
</p>

<p>
  Well then, let&#8217;s get our hands dirty, and let&#8217;s start where every journey begins: at index.html.
</p>

<p>
  Open your favorite editor and create a simple HTML page that provides the canvas element for our game world:
</p>

<p>
  <span class="filename beforecode">/index.html</span>
  <code>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;&lt;/head&gt;

  &lt;body&gt;

    &lt;canvas id="world" width="740" height="640"&gt;
      This browser can not run this game (canvas support missing).
    &lt;/canvas&gt;

  &lt;/body&gt;

&lt;/html&gt;
  </code>
</p>

<p>
  Now that we talked about the game loop again and again, let&#8217;s finally create it.
  Not surprisingly, it&#8217;s a piece of JavaScript code. As I don&#8217;t like these &#8220;let&#8217;s put
  all our code into one single file because that&#8217;s so much simpler&#8221; tutorials, we should
  think about where to put it. How about representing the game with an object that has the
  update and draw cycles as methods, and starts the game loop that iterates through these
  cycles? We could put these into a <em>Game</em> constructor, in a file named
  <em>Game.js</em>.
</p>

<p>
  I suggest we put this file in a <em>lib</em> subfolder, which itself
  is a subfolder of <em>scripts</em>. This way, we keep the different concerns of our
  application neatly separated. HTML files live at the root, all JavaScript goes into
  <em>scripts</em>, which in turn will be separated into <em>lib</em> for the building
  blocks of the game itself, <em>app</em> for the JavaScript files that relate to the
  HTML files (thus, the script code for <em>/index.html</em> will be in
  <em>/scripts/app/index.js</em>), and <em>vendor</em> for external libraries.
</p>

<p>
  But let&#8217;s not get ahead of outselves &#8211; what we need now is the constructor for the main
  game object:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Game.js</span>
  <code>
var Game = function() {
  this.fps = 60;

  var game = this;
  var gameloop = setInterval(function() {
    game.updateAll();
    game.drawAll();
  }, 1000 / this.fps);
}

Game.prototype.updateAll = function() {
  //
}

Game.prototype.drawAll = function() {
  //
}
  </code>
</p>

<p>
  Actually, this already creates a running game &#8211; with the minor restraint that the game world is completely empty,
  and no action takes place.
</p>

<p>
  Let&#8217;s quickly fill the world so we can experience the pleasure of seeing something on the screen.
</p>

<p>
  We need at least one game object which is able to at least draw itself. We will solve this by providing a very
  basic player.
</p>

<p>
  The player is represented by its own object. Let&#8217;s create a constructor for it:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Player.js</span>
  <code>
var Player = function(game) {
  this.game = game;
  this.x = 100;
  this.y = 100;
}

Player.prototype.update = function() {
  //
}

Player.prototype.draw = function() {
  this.game.drawRectangle('#f00', this.x, this.y, 10, 10);
}
  </code>
</p>

<p>
  Like any other game object, the player object needs an update and a draw function. These will be called by the master
  game object on every cycle.
</p>

<p>
  Our player doesn&#8217;t do anything yet, therefore its update method is empty, but it knows how to draw itself &#8211; by drawing
  a red 10 x 10 pixel rectangle into the gameworld, at its current position.
</p>

<p>
  However, we don&#8217;t want to hardwire the low level canvas drawing logic into our game objects &#8211; therefore, we pass the
  <em>game</em> object into the player&#8217;s constructor, and call the appropriate drawing method on the game object.
  This way, the actual drawing can be handled outside the game objects, and inside the main game object.
</p>

<p>
  This means we need to actually pass the <em>game</em> object into our player object when we create it. This is
  done in the <em>Game</em> constructor. Let&#8217;s also add the update and drawing calls to our player to the game loop:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Game.js</span>
  <code>
var Game = function() {
  this.fps = 60;
<span class="addedline">  this.player = new Player(this);</span>

  var game = this;
  var gameloop = setInterval(function() {
    game.updateAll();
    game.drawAll();
  }, 1000 / this.fps);
}

Game.prototype.updateAll = function() {
<span class="addedline">  this.player.update();</span>
}

Game.prototype.drawAll = function() {
<span class="addedline">  this.player.draw();</span>
}

<span class="addedline">Game.prototype.drawRectangle = function() {
  //
}</span>
  </code>
</p>

<p>
  Well, this still won&#8217;t put any pixels on our screen, of course. We have the <em>&lt;canvas&gt;</em> element in our
  <em>index.html</em> file, and we want to draw into it from our JavaScript code in <em>Game.js</em> &#8211; it&#8217;s time
  to put some code into place which brings both together.
</p>

<p>
  Technically, you don&#8217;t draw into a canvas directly. In JavaScript, you ask the canvas element for a <em>context</em>, which
  gives you an object that provides several draw methods, like this:
</p>

<p>
  <code>
var context = document.getElementById('myCanvas').getContext('2d');

context.fillStyle = '#f00';
context.fillRect(5, 10, 90, 20);
  </code>
</p>

<p>
  This would draw a red rectangle onto the canvas element with ID <em>myCanvas</em>, with the upper left corner 5 pixels
  from the left and 10 pixels from the top of the canvas, a width of 90 pixels, and 20 pixels high.
</p>

<p>
  We need to get the 2D context of our <em>world</em> canvas in <em>index.html</em>, and implement the actual
  <em>drawRectangle</em> functionality:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Game.js</span>
  <code>
var Game = function() {
  this.fps = 60;
<span class="addedline">  var canvas = document.getElementById('world');</span>
<span class="addedline">  this.context = canvas.getContext('2d');</span>
  this.player = new Player(this);

  var game = this;
  var gameloop = setInterval(function() {
    game.updateAll();
    game.drawAll();
  }, 1000 / this.fps);
}

Game.prototype.updateAll = function() {
  this.player.update();
}

Game.prototype.drawAll = function() {
  this.player.draw();
}

Game.prototype.drawRectangle = function(color, x, y, width, height) {
<span class="addedline">  this.context.fillStyle = color;</span>
<span class="addedline">  this.context.fillRect(x, y, width, height);</span>
}
  </code>
</p>

<p>
  Now we can wire everything together in our <em>index.html</em> and see our player for the first time:
</p>

<p>
  <span class="filename beforecode">/index.html</span>
  <code>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
<span class="addedline">    &lt;script type="text/javascript" src="scripts/lib/Game.js"&gt;&lt;/script&gt;</span>
<span class="addedline">    &lt;script type="text/javascript" src="scripts/lib/Player.js"&gt;&lt;/script&gt;</span>
  &lt;/head&gt;

  &lt;body&gt;

    &lt;canvas id="world" width="740" height="640"&gt;
      This browser can not run this game (canvas support missing).
    &lt;/canvas&gt;

<span class="addedline">    &lt;script type="text/javascript"&gt;
      var game = new Game();
    &lt;/script&gt;</span>

  &lt;/body&gt;

&lt;/html&gt;
  </code>
</p>

<p>
  Now if you open the <em>index.html</em> file in your browser, you see a red square. Isn&#8217;t it beautiful?
</p>

<p>
  Ok, let&#8217;s add a tad of action, and make our player move across the screen:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Player.js</span>
  <code>
var Player = function(game) {
  this.game = game;
  this.x = 100;
  this.y = 100;
}

Player.prototype.update = function() {
<span class="addedline">  this.x += 1;</span>
<span class="addedline">  this.y += 1;</span>
}

Player.prototype.draw = function() {
  this.game.drawRectangle('#f00', this.x, this.y, 10, 10);
}
  </code>
</p>

<p>
  Reload and enjoy &#8211; but wait, what&#8217;s that? Instead of moving square, we see a bold red line being drawn over the screen.
  That&#8217;s because the canvas doesn&#8217;t know about our game loop &#8211; it does not know that on every loop cycle, the screen
  needs to be completely redrawn; it just draws everything we ask it to draw on top of each other. 
</p>

<p>
  This can be easily fixed. At the start of each loop cycle, we will simply fill the full canvas with one large white rectangle:
</p>

<p>
  <span class="filename beforecode">/scripts/lib/Game.js</span>
  <code>
var Game = function() {
  this.fps = 60;
  var canvas = document.getElementById('world');
  this.context = canvas.getContext('2d');
<span class="addedline">  this.context_width = canvas.width;</span>
<span class="addedline">  this.context_height = canvas.height;</span>
  this.player = new Player(this);

  var game = this;
  var gameloop = setInterval(function() {
    game.updateAll();
    game.drawAll();
  }, 1000 / this.fps);
}

Game.prototype.updateAll = function() {
  this.player.update();
}

Game.prototype.drawAll = function() {
<span class="addedline">  this.drawRectangle('#fff', 0, 0, this.context_width, this.context_height);</span>
  this.player.draw();
}

Game.prototype.drawRectangle = function(color, x, y, width, height) {
  this.context.fillStyle = color;
  this.context.fillRect(x, y, width, height);
}
  </code>
</p>

<p>
  Refresh your browser, and you will see a moving red square. First achievement unlocked!
</p>

<p>
  That&#8217;s it for today. Part 2 of this tutorial series will introduce the concept of steering our player with the arrow
  keys, and other goodies.
</p>

<p>
  You can browse the code of part 1 at
  <a href="https://github.com/ManuelKiessling/HTML5CanvasJavaScriptFacebookTutorialCode/tree/part1">https://github.com/ManuelKiessling/ HTML5CanvasJavaScriptFacebookTutorialCode/tree/part1</a>.
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2012/04/02/tutorial-developing-html5-canvas-games-for-facebook-with-javascript-part-1/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/04/02/tutorial-developing-html5-canvas-games-for-facebook-with-javascript-part-1/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[&#8220;We need to work longer&#8221; &#8211; Why this is not what you want to say]]></title>
		<link rel="alternate" type="text/html" href="/2012/04/02/we-need-to-work-longer-why-this-is-not-what-you-want-to-say/" />
		<id>http://172.16.233.129/wordpress/?p=333</id>
		<updated>2012-04-03T18:13:38Z</updated>
		<published>2012-04-02T12:32:21Z</published>
		<category scheme="/" term="Other" />		<summary type="html"><![CDATA[Every now and then, a manager at your company might say something like "We need to work longer" or "Our people need to work longer".
Here's why I don't think it is a good idea to say this.]]></summary>
		<content type="html" xml:base="/2012/04/02/we-need-to-work-longer-why-this-is-not-what-you-want-to-say/"><![CDATA[<p>
Every now and then, a manager at your company might say something like &#8220;We need to work longer&#8221; or &#8220;Our people need to work longer&#8221;.
</p>
<p>
Here&#8217;s why I don&#8217;t think it is a good idea to say this.
</p>
<p>
Well, first of all, there may really be the need for people to change something about their work, and so the motivation and the goal of the manager might be valid and important.
</p>
<p>
However, if the manager puts emphasis on people working <strong>longer</strong>, he probably won&#8217;t achieve his goal. Why? Because his <em>real</em> goal is not that people actually work longer &#8211; but he doesn&#8217;t actually know this. Weâ€™ll see why in a moment.
</p>
<p>
Ok, now if you&#8217;re telling people you want them to work longer, what are you <em>really</em> telling them? You are telling them this:
</p>
<p>
<blockquote>&#8220;Look, you&#8217;re working about 8 hours per day now, but I want you to work 9, 10 or 11 hours, because we need to become better. So, I&#8217;m asking you to work 3 hours more, which means I want you to do around 30% more work, which means I want us as a company to become around 30% better.&#8221;</blockquote>
</p>
<p>
30%? Really? That&#8217;s the amount of how much you want us to do better? And all we have to do is stay longer, and that&#8217;s it?
</p>
<p>
Let&#8217;s face it, doing about 30% better isn&#8217;t going to get you anywhere significant. And I&#8217;m not even going to start talking about why working 30% longer of course won&#8217;t bring anything significant in your company up by 30%, especially not the &#8220;doing better&#8221; &#8211; much has been written about this already.
</p>
<p>
Here is what I propose: Ask your people that you want them to become <em>300%</em> better at what they do! That you want the whole company to become 300% better than it is right now. And don&#8217;t even mention working longer.
</p>
<p>
Because if you tell people <em>this</em>, then everybody is going to understand that this goal won&#8217;t be achieved by working 300% <em>longer</em>, because 8 x 3 is 24, and for obvious reasons this is not going to work out.
</p>
<p>
You should probably even go this far as to tell your people you want them to achieve the 300% goal by actually working normal 8 hour workdays &#8211; as a new, strict rule! Now this is an interesting challenge for intelligent people, isn&#8217;t it?
</p>
<p>
Because then they are forced to contemplate about how <em>well</em> they will need to work, and not how <em>much</em> they will need to work, because that is going to be the only way to achieve the goal.
</p>
<p>
At the end of the day, what you want is people to <em>change</em>, and that is the hardest thing of all. Maybe the only thing people hate even more than change is being critizised. Which is why it isn&#8217;t such a good idea to start with a negative connotation if you want people to change.
</p>
<p>
Why does asking people to work longer have a negative connotation? Because it sounds like a punishment. It sounds like &#8220;we haven&#8217;t been doing well, and as a result, we need to stay longer&#8221;. Well, at least in Germany, &#8220;staying longer&#8221; is <em>exactly</em> the kind of punishment we all have actually been raised with in school (it is called &#8220;Nachsitzen&#8221;).
</p>
<p>
Here is another point of view: What you actually want is people to become worthier for your company, that they create more value, right? Well, it seems obvious to me that some people in companies seem to already have partly achieved this goal, because they get paid a lot more than others. Which basically means that they create more value for the company (which is or at least should be the only valid reason they get paid more, right)?
</p>
<p>
So, let&#8217;s say you have some Juniors and some Seniors in your company, and the Seniors get paid twice the money compared to the Juniors.
</p>
<p>
How did the Seniors get there? Was it something like this?
<ul>
<li>As a Junior, they worked 5 hours</li>
<li>As a Regular, they worked 7 hours</li>
<li>As a Senior, they now work 10 hours</li>
</ul>
Well, I guess we can agree that this is not how this kind of stuff works.
</p>
<p>
But then what&#8217;s happening here? Why is it ok to pay them more? Well, those people don&#8217;t work <em>longer</em>, they work <em>better</em>. So, if <em>this</em> is what enables people to create more value for the company, then why would you want to put the focus on working <em>longer</em>?
</p>
<p>
The conclusion: Don&#8217;t ask people to work longer. Instead, think really hard about how to create an environment which allows people to work <em>a lot better</em> than they do today, and put a lot of effort into implementing this environment.
</p>
<p>
This of course is a lot harder than just asking them to work longer &#8211; but this is a very good sign that it is going to achieve you a lot more.
</p>

<p>
  <em>Manuel Kiessling is currently working at <a href="http://news.myhammer.de/unternehmen/jobs">MyHammer AG</a>, a company that puts a lot of effort into continuously improving by 300%.</em>
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/04/02/we-need-to-work-longer-why-this-is-not-what-you-want-to-say/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/04/02/we-need-to-work-longer-why-this-is-not-what-you-want-to-say/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[True universal JavaScript modules with write-once-run-anywhere Jasmine specs]]></title>
		<link rel="alternate" type="text/html" href="/2012/03/30/true-universal-javascript-modules-with-write-once-run-anywhere-jasmine-specs/" />
		<id>http://172.16.233.129/wordpress/?p=611</id>
		<updated>2012-08-04T11:10:53Z</updated>
		<published>2012-03-30T08:06:43Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="Tutorial" />		<summary type="html"><![CDATA[Writing JavaScript modules that can be seamlessly included in client-side as well as server-side applications, and providing Jasmine test suites which allow to test these modules in a browser environment as well as a Node.js environment is possible without any dirty workarounds.]]></summary>
		<content type="html" xml:base="/2012/03/30/true-universal-javascript-modules-with-write-once-run-anywhere-jasmine-specs/"><![CDATA[<p>
  As a JavaScript developer writing general-purpose libraries, you probably wish you could go fully universal, that is, you might want to create a library:
  <ul>
    <li>that can be used in the browser <em>and</em> in Node.js &#8211; without any hacks for either platform, and without code-duplication</li>
    <li>that transparently utilizes AMD via RequireJS in the browser and CommonJS via <em>require</em> in Node.js &#8211; without any hacks for either platform, and without code-duplication</li>
    <li>with a Jasmine spec suite which runs in the browser and in Node.js &#8211; without any hacks for either platform, and without code-duplication</li>
  </ul>
</p>

<p>
  Turns out this is perfectly possible. Consider the following library structure:
</p>

<p>
  <code>
      lib/
        multiply.js

      spec/
        multiply.spec.js
  </code>
</p>

<p>
  As stated above: Without making use of any nasty hacks or duplicating any or all of our lib or spec code, we would like to be able to use our <em>multiply.js</em> module in a web-based browser application, or in a server-side application for Node.js &#8211; of course assuming that <em>multiply.js</em> provides a general-purpose functionality that is usable on both platforms, and not e.g. something working with a browser-DOM. In our example library, <em>multiply</em> provides a function that does the very useful job of multiplying two number, like this:
</p>

<p>
  <code>
    console.log(multiply(2, 5)); // outputs "10"
  </code>
</p>

<p>
  Furthermore, when using our lib in the browser, we would like to be able to load it using RequireJS, and when using it in a Node.js application, we would like to be able to <em>require()</em> and use it just like any other npm module:
</p>

<p>
  <code>
    // Browser script
    
    define("../lib/multiply", function(multiply) {
      console.log(multiply(2, 5));
    });
  </code>
</p>

<p>
  <code>
    // Node.js script
    
    var multiply = require("../lib/multiply.js");

    console.log(multiply(2, 5));
  </code>
</p>

<p>
  And we want our Jasmine specification <em>multiply.spec.js</em> to be able to run in a web-based Jasmine runner as well as in a Node.js based runner, enabling us to test our library on both platforms. Again, without duplicating the spec or lib code.
</p>

<p>
All this can be achieved with the following steps:
<ol>
  <li>Export the lib module and its spec via RequireJS&#8217; <em>describe</em> logic</li>
  <li>For Node.js, add the <em>amdefine</em> package to make the lib and its spec loadable via Node&#8217;s <em>require()</em></li>
  <li>For Node.js, add the <em>jasmine-node</em> package (but no node-specific RequireJS implementation!)</li>
  <li>For the client-side, add <em>Jasmine</em> and <em>RequireJS</em></li>
  <li>For the client-side, write a Jasmine SpecRunner that uses RequireJS&#8217; <em>describe()</em> to load the spec files for the test run</li>
</ol>
</p>

<p>
  Once this is done, the library structure will look as follows:
</p>

<p>
  <code>
      package.json

      lib/
        multiply.js

      spec/
        multiply.spec.js
        run.sh
        SpecRunner.html

      vendor/
        require.js

        jasmine/
          jasmine-html.js
          jasmine.css
          jasmine.js
          jasmine_favicon.png
          MIT.LICENSE
  </code>
</p>

<p>
  The <em>/package.json</em> file is the place where the <em>amdefine</em> and <em>jasmine-node</em> dependecy for the Node.js environment will be defined, <em>/spec/run.sh</em> is the Jasmine runner for the Node.js environment, <em>/spec/SpecRunner.html</em> is the Jasmine runner for the client-side environment, and <em>/vendor</em> hosts the external libraries RequireJS and Jasmine, again for the client-side environment.
</p>

<p>
  Let&#8217;s tackle each file in turn:
</p>

<p>
  /package.json:
  <code>
{
  "name": "CafeGraph",
  "version": "0.0.1",
  "dependencies": {
    "amdefine": ">=0.0.2"
  },
  "devDependencies": {
    "jasmine-node": "1.0.x"
  }
}
  </code>
</p>

<p>
  This states that the npm modules <em>amdefine</em> and <em>jasmine-node</em> are needed for our lib to work in the Node.js environment. Run <code>npm install</code> to have them automatically installed.
</p>

<p>
  /lib/multiply.js:
  <code>
if (typeof define !== 'function') { var define = require('amdefine')(module) }

define([], function() {
  var multiply = function(a, b) {
    return a * b;
  };
  return multiply;
});
  </code>
</p>

<p>
  Here, several things happen that are crucial for our library to be universal. Because the module itself is wrapped in RequireJS&#8217; <em>define()</em> function, it need some special treatment to be usable within a Node.js context. This is what the first line does, it makes sure that Node.js uses the <em>define</em> function of the <em>amdefine</em> package. This way, the module becomes usable just like any other npm package, even though RequireJS is not available.
</p>

<p>
 The <em>return multiply;</em> statement has the same effect as <em>module.exports = multiply;</em> would have, making the <em>multiply</em> function available for Node.js scripts that do <em>var multiply = require(&#8220;../lib/multiply.js&#8221;);</em>
</p>

<p>
  /spec/multiply.spec.js:
  <code>
if (typeof define !== 'function') { var define = require('amdefine')(module) }

define(["../lib/multiply.js"], function(multiply) {
  describe("multiply", function() {
    it("multiplies two numbers", function() {
      expect(multiply(2, 5)).toEqual(10);
    });
  });
});
  </code>
</p>

<p>
  /spec/run.sh:
  <code>
#!/bin/bash
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &#038;&#038; pwd )"
node $DIR/../node_modules/jasmine-node/lib/jasmine-node/cli.js $DIR
  </code>
</p>

<p>
  This just a script which enables me to run the Jasmine test suite for the Node.js environment via <em>./spec/run.sh</em> on the command line.
  It&#8217;s probably a totally unnecessary hack, but I never really found the time to find a better solution. Recommendations welcome. Not part of our code, thus it&#8217;s acceptable for now I think.
</p>

<p>
  /spec/SpecRunner.html:
  <code>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Jasmine Test Runner&lt;/title&gt;

    &lt;!-- Jasmine --&gt;
    &lt;link rel="stylesheet" type="text/css" href="../vendor/jasmine/jasmine.css"/&gt;
    &lt;script type="text/javascript" src="../vendor/jasmine/jasmine.js"&gt;&lt;/script&gt;
    &lt;script type="text/javascript" src="../vendor/jasmine/jasmine-html.js"&gt;&lt;/script&gt;

    &lt;!-- RequireJS --&gt;
    &lt;script type="text/javascript" src="../vendor/require.js"&gt;&lt;/script&gt;

  &lt;/head&gt;

  &lt;body&gt;
    &lt;script type="text/javascript"&gt;
      require.config({
        baseUrl: './'
      });

      require([
        '../spec/multiply.spec.js'
      ], function() {
        jasmine.getEnv().addReporter(new jasmine.TrivialReporter());
        jasmine.getEnv().execute();
      });
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
  </code>
</p>

<p>
  The web-based Jasmine spec runner needs several modifications, because the spec files need to be <em>require()</em>d the RequireJS way. However, once this is set up, the workflow for adding new code and specs is straight-forward. For adding new specs to the Node.js spec run, no additional steps are necessary. To include new specs in the web-based spec run, simply add the path to the spec file to the <em>require()</em> array, and that&#8217;s it.
</p>

<p>
  You can find the complete library code on GitHub at <a href="https://github.com/ManuelKiessling/UniversalJavaScriptModulesExample">https://github.com/ManuelKiessling/UniversalJavaScriptModulesExample</a>, with some additional usage examples.
</p>

<p>
  <strong>tl;dr:</strong> Writing JavaScript modules that can be seamlessly included in client-side as well as server-side applications, and providing Jasmine test suites which allow to test these modules in a browser environment as well as a Node.js environment is possible without any dirty workarounds by writing these modules using the AMD pattern, and then using the <em>amdefine</em> package to make them available to Node.js.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/03/30/true-universal-javascript-modules-with-write-once-run-anywhere-jasmine-specs/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/03/30/true-universal-javascript-modules-with-write-once-run-anywhere-jasmine-specs/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Talk: PHP 5.4 &#8211; die wichtigsten Neuerungen im Ãœberblick]]></title>
		<link rel="alternate" type="text/html" href="/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/" />
		<id>http://172.16.233.129/wordpress/?p=600</id>
		<updated>2012-07-28T19:41:45Z</updated>
		<published>2012-03-24T21:42:30Z</published>
		<category scheme="/" term="Deutsche Artikel" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[Am 14. MÃ¤rz 2012 fand das erste Treffen der Symfony User Group Berlin statt. Ich habe dort einen kurzen Talk mit dem Titel <em>PHP 5.4 - die wichtigsten Neuerungen im Ãœberblick</em> gehalten. Folien und Videomitschnitt des Talks nach dem Klick.]]></summary>
		<content type="html" xml:base="/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/"><![CDATA[<p>
  Am 14. MÃ¤rz 2012 fand das erste Treffen der Symfony User Group Berlin statt. Ich habe dort einen kurzen Talk mit dem Titel <em>PHP 5.4 &#8211; die wichtigsten Neuerungen im Ãœberblick</em> gehalten. Die Folien des Talks gibt es <a href="/wp-content/uploads/2012/03/Manuel-Kiessling-PHP-5.4-Die-wichtigsten-Neuerungen-im-Ãœberblick.pdf" title="PrÃ¤sentation 'PHP 5.4 - die wichtigsten Neuerungen im Ãœberblick' von Manuel Kiessling als PDF-Download">hier zum Download als PDF</a> und <a href="http://www.slideshare.net/manuelkiessling/php-54-die-wichtigsten-neuerungen" title="PrÃ¤sentation 'PHP 5.4 - die wichtigsten Neuerungen im Ãœberblick' von Manuel Kiessling online bei SlideShare">hier bei Slideshare</a>. Einen Videomitschnitt gibt es <a href="http://www.ustream.tv/recorded/21104071/highlight/251550" title="Videomitschnitt der PrÃ¤sentation 'PHP 5.4 - die wichtigsten Neuerungen im Ãœberblick' von Manuel Kiessling">bei Ustream</a>.
</p>

<div style="width:510px" id="__ss_12011306"> <strong style="display:block;margin:12px 0 4px"><a href="http://www.slideshare.net/manuelkiessling/php-54-die-wichtigsten-neuerungen" title="PHP 5.4: Die wichtigsten Neuerungen im Ãœberblick" target="_blank">PHP 5.4: Die wichtigsten Neuerungen im Ãœberblick</a></strong> <iframe src="http://www.slideshare.net/slideshow/embed_code/12011306?rel=0" width="510" height="426" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe> <div style="padding:5px 0 12px"> View more <a href="http://www.slideshare.net/thecroaker/death-by-powerpoint" target="_blank">PowerPoint</a> from <a href="http://www.slideshare.net/manuelkiessling" target="_blank">Manuel Kiessling</a> </div> </div>

<iframe src="http://www.ustream.tv/embed/recorded/21104071/highlight/251550" width="608" height="368" scrolling="no" frameborder="0" style="border: 0px none transparent;"></iframe>]]></content>
		<link rel="replies" type="text/html" href="/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/03/24/talk-php-5-4-die-wichtigsten-neuerungen-im-ueberblick/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Object-orientation and inheritance in JavaScript: a comprehensive explanation]]></title>
		<link rel="alternate" type="text/html" href="/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/" />
		<id>http://172.16.233.129/wordpress/?p=568</id>
		<updated>2013-09-25T19:12:09Z</updated>
		<published>2012-03-23T09:43:26Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" /><category scheme="/" term="Tutorial" />		<summary type="html"><![CDATA[<p>
Letâ€™s talk about object-orientation and inheritance in JavaScript.
</p>
<p>
The good news is that itâ€™s actually quite simple, but the bad news is that it works completely different than object-orientation in languages like C++, Java, Ruby, Python or PHP, making it not-quite-so simple to understand.
</p>
<p>
But fear not, we are going to take it step by step.
</p>]]></summary>
		<content type="html" xml:base="/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/"><![CDATA[<p>Let&#8217;s talk about object-orientation and inheritance in JavaScript.</p>
<p>The good news is that it&#8217;s actually quite simple, but the bad news is that it works completely different than object-orientation in languages like C++, Java, Ruby, Python or PHP, making it not-quite-so simple to understand.</p>
<p>But fear not, we are going to take it step by step.</p>
<h2 id="blueprints-versus-finger-pointing">Blueprints versus finger-pointing</h2>
<p>Let&#8217;s start by looking at how &quot;typical&quot; object-oriented languages actually create objects.</p>
<p>We are going to talk about an object called <em>myCar</em>. myCar is our bits-and-bytes representation of an incredibly simplified real world car. It could have attributes like <em>color</em> and <em>weight</em>, and methods like <em>drive</em> and <em>honk</em>.</p>
<p>In a &quot;real&quot; application, <em>myCar</em> could be used to represent the car in a game which is driven by the player of that game &#8211; but we are going to completely ignore the context of this object, because we are going to talk about the nature and usage of this object in a more abstract way.</p>
<p>If you would want to use this <em>myCar</em> object in, say, Java, you need to define the blueprint of this specific object first &#8211; this is what Java and most other object-oriented languages call a <em>class</em>.</p>
<p>If you want to create the object <em>myCar</em>, you tell Java to &quot;build a new object after the specification that is laid out in the class <em>Car</em>&quot;.</p>
<p>The newly built object shares certain aspects with its blueprint. If you call the method <em>honk</em> on your object, like so:</p>
<code>myCar.honk();</code>
<p>the Java interpreter will go to the class of <em>myCar</em> and look up which code it actually needs to execute, which is defined in the <em>honk</em> method of class <em>Car</em>.</p>
<p>Ok, nothing shockingly new here. Enter JavaScript.</p>
<h2 id="a-classless-society">A classless society</h2>
<p>JavaScript does not have classes. But as in other languages, we would like to tell the interpreter that it should built our <em>myCar</em> object following a certain pattern or schema or blueprint &#8211; it would be quite tedious to create every car object from scratch, &quot;manually&quot; giving it the attributes and methods it needs every time we build it.</p>
<p>If we were to create 30 car objects based on the <em>Car</em> class in Java, this object-class relationship provides us with 30 cars that are able to <em>drive</em> and <em>honk</em> without us having to write 30 <em>drive</em> and <em>honk</em> methods.</p>
<p>How is this achieved in JavaScript? Instead of an object-class relationship, there is an object-object relationship.</p>
<p>Where in Java our <em>myCar</em>, asked to honk, says &quot;go look at this class over there, which is my <em>blueprint</em>, to find the code you need&quot;, JavaScript says &quot;go look at that other object over there, which is my <em>prototype</em>, it has the code you are looking for&quot;.</p>
<p>Building objects via an object-object relationship is called <em>Prototype-based programming</em>, versus <em>Class-based programming</em> used in more traditional languages like Java.</p>
<p>Both are perfectly valid implementations of the object-oriented programming paradigm &#8211; it&#8217;s just two different approaches.</p>
<h2 id="creating-objects">Creating objects</h2>
<p>Let&#8217;s dive into code a bit, shall we? How could we set up our code in order to allow us to create our <em>myCar</em> object, ending up with an object that is a <em>Car</em> and can therefore <em>honk</em> and <em>drive</em>?</p>
<p>Well, in the most simple sense, we can create our object completely from scratch, or <em>ex nihilo</em> if you prefer the boaster expression.</p>
<p>It works like this:</p>
<code>var myCar = {}

myCar.honk = function() {
  console.log(&quot;honk honk&quot;);
}

myCar.drive = function() {
  console.log(&quot;vrooom...&quot;);
}</code>
<p>This gives us an object called <em>myCar</em> that is able to honk and drive:</p>
<code>myCar.honk()  // outputs &quot;honk honk&quot;
myCar.drive() // outputs &quot;vrooom...&quot;</code>
<p>However, if we were to create 30 cars this way, we would end up defining the honk and drive behaviour of every single one, something we said we want to avoid.</p>
<p>In real life, if we made a living out of creating, say, pencils, and we don&#8217;t want to create every pencil individually by hand, then we would consider building a pencil-making machine, and have this machine create the pencils for us.</p>
<p>After all, that&#8217;s what we implicitly do in a class-based language like Java &#8211; by defining a class <em>Car</em>, we get the car-maker for free:</p>
<code>Car myCar = new Car();</code>
<p>will built the <em>myCar</em> object for us based on the <em>Car</em> blueprint. Using the <em>new</em> keyword does all the magic for us.</p>
<p>JavaScript, however, leaves the responsibility of building an object creator to us. Furthermore, it gives us a lot of freedom regarding the way we actually build our objects.</p>
<p>In the most simple case, we can write a function which creates &quot;plain&quot; objects that are exactly like our &quot;ex nihilo&quot; object, and that don&#8217;t really share any behaviour &#8211; they just happen to roll out of the factory with the same behaviour copied onto every single one, if you want so.</p>
<p>Or, we can write a special kind of function that not only creates our objects, but also does some behind-the-scenes magic which links the created objects with their creator. This allows for a true sharing of behaviour: functions that are available on all created objects point to a single implementation. If this function implementation changes after objects have been created, which is possible in JavaScript, the behaviour of all objects sharing the function will change accordingly.</p>
<p>Let&#8217;s examine all possible ways of creating objects in detail.</p>
<h3 id="using-a-simple-function-to-create-plain-objects">Using a simple function to create plain objects</h3>
<p>In our first example, we created a plain <em>myCar</em> object out of thin air &#8211; we can simply wrap the creation code into a function, which gives us a very basic object creator:</p>
<code>function makeCar() {
  var newCar = {}
  newCar.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}</code>
<p>For the sake of brevity, the <em>drive</em> function has been omitted.</p>
<p>We can then use this function to mass-produce cars:</p>
<code>function makeCar() {
  var newCar = {}
  newCar.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
  return newCar;
}

myCar1 = makeCar();
myCar2 = makeCar();
myCar3 = makeCar();</code>
<p>One downside of this approach is efficiency: for every <em>myCar</em> object that is created, a new <em>honk</em> function is created and attached &#8211; creating 1,000 objects means that the JavaScript interpreter has to allocate memory for 1,000 functions, although they all implement the same behaviour. This results in an unnecessarily high memory footprint of the application.</p>
<p>Secondly, this approach deprives us of some interesting opportunities. These <em>myCar</em> objects don&#8217;t share anything &#8211; they were built by the same creator function, but are completely independent from each other.</p>
<p>It&#8217;s really like with real cars from a real car factory: They all look the same, but once they leave the assembly line, they are totally independent. If the manufacturer should decide that pushing the horn on already produced cars should result in a different type of honk, all cars would have to be returned to the factory and modified.</p>
<p>In the virtual universe of JavaScript, we are not bound to such limits. By creating objects in a more sophisticated way, we are able to magically change the behaviour of all created objects at once.</p>
<h3 id="using-a-constructor-to-create-objects">Using a constructor to create objects</h3>
<p>In JavaScript, the entities that create objects with shared behaviour are functions which are called in a special way. These special functions are called <em>constructors</em>.</p>
<p>Let&#8217;s create a constructor for cars. We are going to call this function <em>Car</em>, with a capital <em>C</em>, which is common practice to indicate that this function is a constructor.</p>
<p>Because we are going to encounter two new concepts that are both necessary for shared object behaviour to work, we are going to approach the final solution in two steps.</p>
<p>Step one is to recreate the previous solution (where a common function spilled out independent car objects), but this time using a constructor:</p>
<code>function Car() {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}</code>
<p>When this function is called using the <em>new</em> keyword, like so:</p>
<code>var myCar = new Car();</code>
<p>it implicitly returns a newly created object with the honk function attached.</p>
<p>Using <em>this</em> and <em>new</em> makes the explicit creation and return of the new object unnecessary &#8211; it is created and returned &quot;behind the scenes&quot; (i.e., the <em>new</em> keyword is what creates the new, &quot;invisible&quot; object, and secretly passes it to the <em>Car</em> function as its <em>this</em> variable).</p>
<p>You can think of the mechanism at work a bit like in this pseudo-code:</p>
<code>// Pseudo-code, for illustration only!

function Car(this) {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
  return this;
}

var newObject = {}
var myCar = Car(newObject);</code>
<p>As said, this is more or less like our previous solution &#8211; we don&#8217;t have to create every car object manually, but we still cannot modify the <em>honk</em> behaviour only once and have this change reflected in all created cars.</p>
<p>But we laid the first cornerstone for it. By using a constructor, all objects received a special property that links them to their constructor:</p>
<code>function Car() {
  this.honk = function() {
    console.log(&quot;honk honk&quot;);
  }
}

var myCar1 = new Car();
var myCar2 = new Car();

console.log(myCar1.constructor); // outputs [Function: Car]
console.log(myCar2.constructor); // outputs [Function: Car]</code>
<p>All created <em>myCars</em> are linked to the <em>Car</em> constructor. This is what actually makes them a class of related objects, and not just a bunch of objects that happen to have similar names and identical functions.</p>
<p>Now we have finally reached the moment to get back to the mysterious <em>prototype</em> we talked about in the introduction.</p>
<h3 id="using-prototyping-to-efficiently-share-behaviour-between-objects">Using prototyping to efficiently share behaviour between objects</h3>
<p>As stated there, while in class-based programming the class is the place to put functions that all objects will share, in prototype-based programming, the place to put these functions is the object which acts as the prototype for our objects at hand.</p>
<p>But where is the object that is the prototype of our <em>myCar</em> objects &#8211; we didn&#8217;t create one!</p>
<p>It has been implicitly created for us, and is assigned to the</p>
<code>Car.prototype</code>
<p>property (in case you wondered, JavaScript functions are objects that have properties, too).</p>
<p>Here is the key to sharing functions between objects: Whenever we call a function on an object, the JavaScript interpreter tries to find that function within the queried object. But if it doesn&#8217;t find the function within the object itself, it asks the object for the pointer to it&#8217;s prototype, then goes to the prototype, and asks for the function there. If it is found, it is then executed.</p>
<p>This means that we can create <em>myCar</em> objects without any functions, create the <em>honk</em> function in their prototype, and end up having <em>myCar</em> objects that know how to honk &#8211; because everytime the interpreter tries to execute the <em>honk</em> function on one of the <em>myCar</em> objects, it will be redirected to the prototype, and execute the <em>honk</em> function which is defined there.</p>
<p>Here is how this setup can be achieved:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;</code>
<p>Our constructor is now empty, because for our very simple cars, no additional setup is necessary.</p>
<p>Because both <em>myCars</em> are created through this constructor, their prototype points to <em>Car.prototype</em> &#8211; executing <em>myCar1.honk()</em> results in <em>Car.prototype.honk()</em> being executed.</p>
<p>Let&#8217;s see what this enables us to do. In JavaScript, objects can be changed at runtime. This holds true for prototypes, too. Which is why we can change the <em>honk</em> behaviour of all our cars even after they have been created:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;

Car.prototype.honk = function() {
  console.log(&quot;meep meep&quot;);
}

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;meep meep&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;meep meep&quot;</code>
<p>Of course, we can also add additional functions at runtime:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

Car.prototype.drive = function() {
  console.log(&quot;vrooom...&quot;);
}

myCar1.drive(); // executes Car.prototype.drive() and outputs &quot;vrooom...&quot;
myCar2.drive(); // executes Car.prototype.drive() and outputs &quot;vrooom...&quot;</code>
<p>But we could even decide to treat only one of our cars differently:</p>
<code>function Car() {}

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}

var myCar1 = new Car();
var myCar2 = new Car();

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;

myCar2.honk = function() {
  console.log(&quot;meep meep&quot;);
}

myCar1.honk(); // executes Car.prototype.honk() and outputs &quot;honk honk&quot;
myCar2.honk(); // executes myCar2.honk() and outputs &quot;meep meep&quot;</code>
<p>It&#8217;s important to understand what happens behind the scenes in this example. As we have seen, when calling a function on an object, the interpreter follows a certain path to find the actual location of that function.</p>
<p>While for <em>myCar1</em>, there still is no <em>honk</em> function within that object itself, that no longer holds true for <em>myCar2</em>. When the interpreter calls <em>myCar2.honk</em>, there now is a function within <em>myCar2</em> itself. Therefore, the interpreter no longer follows the path to the prototype of <em>myCar2</em>, and executes the function within <em>myCar2</em> instead.</p>
<p>That&#8217;s one of the major differences to class-based programming: while objects are relatively &quot;rigid&quot; e.g. in Java, where the structure of an object cannot be changed at runtime, in JavaScript, the prototype-based approach links objects of a certain class more loosely together, which allows to change the structure of objects at any time.</p>

<p>Also, note how sharing functions through the constructor&#8217;s prototype is way more efficient than creating objects that all carry their own functions, even if they are identical. As previously stated, the engine doesn&#8217;t know that these functions are meant to be identical, and it has to allocate memory for every function in every object. This is no longer true when sharing functions through a common prototype &#8211; the function in question is placed in memory exactly once, and no matter how many <em>myCar</em> objects you create, they don&#8217;t carry the function themselves, they only refer to their constructor, in whose prototype the function is found.</p>

<p>To give you an idea of what this difference can mean, here is a very simple comparison. The first example creates 1,000,000 objects that all have the function directly attached to them:
</p>

<code>var C = function() {
  this.f = function(foo) {
    console.log(foo);
  }
}

var a = [];
for (var i = 0; i < 1000000; i++) {
  a.push(new C());
}
</code>

<p>In Google Chrome, this results in a heap snapshot size of 328 MB. Here is the same example, but now the function is shared through the constructor's prototype:</p>

<code>var C = function() {}

C.prototype.f = function(foo) {
  console.log(foo);
}

var a = [];
for (var i = 0; i < 1000000; i++) {
  a.push(new C());
}
</code>

<p>This time, the size of the heap snapshot is only 17 MB, i.e., only about 5% of the non-efficient solution.</p>

<h2 id="object-orientation-prototyping-and-inheritance">Object-orientation, prototyping, and inheritance</h2>
<p>So far, we haven't talked about inheritance in JavaScript, so let's do this now.</p>
<p>It's useful to share behaviour between a certain class of objects, but there are cases where we would like to share behaviour between different, but similar classes of objects.</p>
<p>Imagine our virtual world not only had cars, but also bikes. Both drive, but where a car has a horn, a bike has a bell.</p>
<p>Being able to <em>drive</em> makes both objects <em>vehicles</em>, but not sharing the <em>honk</em> and <em>ring</em> distinguishes them.</p>
<p>We could illustrate their shared and local behaviour as well as their relationship to each other as follows:</p>
<code>         Vehicle
         &gt; drive

            |
 ----------------------
 |                    |

Car                 Bike
&gt; honk              &gt; ring</code>
<p>Designing this relationship in a class-based language like Java is straightforward: We would define a class <em>Vehicle</em> with a method <em>drive</em>, and two classes <em>Car</em> and <em>Bike</em> which both <em>extend</em> the <em>Vehicle</em> class, and implement a <em>honk</em> and a <em>ring</em> method, respectively.</p>
<p>This would make the car as well as bike objects inherit the drive behaviour through the inheritance of their classes.</p>
<p>How does this work in JavaScript, where we don't have classes, but prototypes?</p>
<p>Let's look at an example first, and then dissect it. To keep the code short for now, let's only start with a car that inherits from a vehicle:</p>
<code>function Vehicle() {}

Vehicle.prototype.drive = function () {
  console.log(&quot;vrooom...&quot;);
}


function Car() {}

Car.prototype = new Vehicle();

Car.prototype.honk = function() {
  console.log(&quot;honk honk&quot;);
}


var myCar = new Car();

myCar.honk()  // outputs "honk honk"
myCar.drive()  // outputs "vrooom..."
</code>
<p>In JavaScript, inheritance runs through a chain of prototypes.</p>
<p>The prototype of the <em>Car</em> constructor is set to a newly created <em>vehicle</em> object, which establishes the link structure that allows the interpreter to look for methods in parent objects.</p>
<p>The prototype of the <em>Vehicle</em> constructor has a function <em>drive</em>. Here is what happens when the <em>myCar</em> object is asked to <em>drive()</em>:</p>
<ul>
<li>The interpreter looks for a <em>drive</em> method within the <em>myCar</em> object, which does not exist</li>
<li>The interpreter then asks the <em>myCar</em> object for its prototype, which is the prototype of its constructor <em>Car</em></li>
<li>When looking at <em>Car.prototype</em>, the interpreter sees a <em>vehicle</em> object which has a function <em>honk</em> attached, but no <em>drive</em> function</li>
<li>Thus, the interpreter now asks this <em>vehicle</em> object for its prototype, which is the prototype of its constructor <em>Vehicle</em></li>
<li>When looking at <em>Vehicle.prototype</em>, the interpreter sees an object which has a <em>drive</em> function attached - the interpreter now knows which code implements the <em>myCar.drive()</em> behaviour, and executes it</li>
</ul>

<h2>A classless society, revisited</h2>

<p>We just learned how to emulate traditional (or classical) inheritance in JavaScript. This understanding is needed to successfully unlearn it and leave it behind, in order to embrace the idea that in JavaScript, you really don't need classes at all, and you therefore don't need to emulate them - plus, that's really a lot of code to express the prototypical idea of <em>"go look at that other object over there, it has the code you are looking for"</em>, isn't it?</p>

<p>It was Douglas Crockford who came up with a very clever solution, which allows to let objects directly inherit from each other, without the need for all the boilerplate code presented in the previous example. The solution is a native part of JavaScript by now - it's the <em>Object.create()</em> function, and it works like this:
</p>

<code>Object.create = function(o) {
  function F() {}
  F.prototype = o;
  return new F();
};
</code>

<p>We learned enough now to understand what's going on. Let's analyze an example:
</p>

<code>var vehicle = {};
vehicle.drive = function () {
  console.log("vrooom...");
}

var car = Object.create(vehicle);
car.honk = function() {
  console.log("honk honk");
}

var myCar = Object.create(car);

myCar.honk()  // outputs "honk honk"
myCar.drive()  // outputs "vrooom..."
</code>

<p>While being significantly more concise and expressive, this code achieves exactly the same behaviour, without the need to write dedicated constructors and attaching functions to their prototype. As you can see, <em>Object.create()</em> handles both behind the scenes, on the fly. A temporary constructor is created, its prototype is set to the object that serves as the role model for our new object, and a new object is created from this setup.
</p>

<p>Conceptually, this is really the same as in the previous example where we defined that <em>Car.prototype</em> shall be <em>new Vehicle();</em>.
</p>

<p>But wait! We created the functions <em>drive</em> and <em>honk</em> within our objects, not on their prototypes - that's memory-inefficient!
</p>

<p>Well, in this case, it's actually not. Let's see why:
</p>

<code>var vehicle = {};
vehicle.drive = function () {
  console.log("vrooom...");
}

var car = Object.create(vehicle);
car.honk = function() {
  console.log("honk honk");
}

var myVehicle = Object.create(vehicle);
var myCar1 = Object.create(car);
var myCar2 = Object.create(car);

myCar1.honk()  // outputs "honk honk"
myCar2.honk()  // outputs "honk honk"

myVehicle.drive()  // outputs "vrooom..."
myCar1.drive()     // outputs "vrooom..."
myCar2.drive()     // outputs "vrooom..."
</code>

<p>We have now created a total of 5 objects, but how often do the <em>honk</em> and <em>drive</em> methods exist in memory? Well, how often have they been defined? Just once - and therefore, this solution is basically as efficient as the one where we built the inheritance manually. Let's look at the numbers:
</p>

<code>c = {};
c.f = function(foo) {
  console.log(foo);
}

var a = [];
for (var i = 0; i < 1000000; i++) {
  a.push(Object.create(c));
}
</code>

<p>Turns out, it's not exactly identical - we end up with a heap snapshot size of 40 MB, thus there seems to be <em>some</em> overhead involved. However, in exchange for much better code, this is probably more than worth it.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/03/23/object-orientation-and-inheritance-in-javascript-a-comprehensive-explanation/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Project: projectile, an HTML5 canvas game]]></title>
		<link rel="alternate" type="text/html" href="/2012/03/19/project-projectile-an-html5-canvas-game/" />
		<id>http://172.16.233.129/wordpress/?p=512</id>
		<updated>2012-07-28T19:43:05Z</updated>
		<published>2012-03-19T00:03:30Z</published>
		<category scheme="/" term="Projects" />		<summary type="html"><![CDATA[Some days ago my son asked me how computer games are made. I couldn't really explain it very well in terms he understood (he's 5 years old), but I wanted to show it to him. Thus I started working with him on a 2D space shooter written in JavaScript, using the canvas element of HTML5.
<br />
<img src="/wp-content/uploads/2012/03/projectile_screenshot.png" alt="" title="projectile Screenshot" width="446" height="239" class="aligncenter size-full wp-image-513" />]]></summary>
		<content type="html" xml:base="/2012/03/19/project-projectile-an-html5-canvas-game/"><![CDATA[<p>
  Some days ago my son asked me how computer games are made. I couldn&#8217;t really explain it very well in terms he understood (he&#8217;s 5 years old), but I wanted to show it to him. Thus I started working with him on a 2D space shooter written in JavaScript, using the canvas element of HTML5. The result is playable at <em><a href="http://manuel.kiessling.net/projectile/">http://manuel.kiessling.net/projectile/</a></em>.
</p>
<p>
  <a href="http://manuel.kiessling.net/projectile/"><img src="/wp-content/uploads/2012/03/projectile_screenshot.png" alt="" title="projectile Screenshot" width="446" height="239" class="aligncenter size-full wp-image-513" /></a>
</p>
<p>
  The project source code is hosted at <a href="https://github.com/ManuelKiessling/projectile">https://github.com/ManuelKiessling/projectile</a>
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/03/19/project-projectile-an-html5-canvas-game/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/03/19/project-projectile-an-html5-canvas-game/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Project: bivouac, an HTML5 web chat with filesharing]]></title>
		<link rel="alternate" type="text/html" href="/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/" />
		<id>http://172.16.233.129/wordpress/?p=486</id>
		<updated>2012-07-28T19:43:24Z</updated>
		<published>2012-03-18T13:33:36Z</published>
		<category scheme="/" term="Projects" />		<summary type="html"><![CDATA[
Some weeks ago I started working on a new open source software, called bivouac.



bivouac provides an open source software package which allows to easily setup and run web-based group chats with dead-simple file-sharing (drag a file into the chat, and it&#8217;s immediately available as a download for all chat members).







Besides these &#8220;feature goals&#8221;, my secondary [...]]]></summary>
		<content type="html" xml:base="/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/"><![CDATA[<p>
Some weeks ago I started working on a new open source software, called <a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac"><em>bivouac</em></a>.
</p>

<p>
<a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac">bivouac</a> provides an open source software package which allows to easily setup and run web-based group chats with dead-simple file-sharing (drag a file into the chat, and it&#8217;s immediately available as a download for all chat members).
</p>

<p>
<img src="/wp-content/uploads/2012/03/bivouac_screenshots.png" alt="" title="bivouac - Screenshots" width="850" height="300" class="alignnone size-full wp-image-488" />
</p>

<p>
Besides these &#8220;feature goals&#8221;, my secondary goal is to learn how to architect JavaScript applications that are relatively complex, with domain-driven design and a strong separation of concerns in mind.
</p>

<p>
bivouac is written in JavaScript, for the Node.js platform. It&#8217;s hosted at <a href="https://github.com/ManuelKiessling/bivouac" title="GitHub page of bivouac">https://github.com/ManuelKiessling/bivouac</a>
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/03/18/project-bivouac-an-html5-web-chat-with-filesharing/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Deploying Symfony2 Apps via Scalarium: Improved Methodology]]></title>
		<link rel="alternate" type="text/html" href="/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/" />
		<id>http://172.16.233.129/wordpress/?p=453</id>
		<updated>2012-05-06T15:04:04Z</updated>
		<published>2012-01-05T08:04:19Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[Some weeks ago I wrote about deploying Symfony2 Applications to Amazon AWS with Scalarium. It turned out that the described methodology can be refined in several ways. Here's how.]]></summary>
		<content type="html" xml:base="/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/"><![CDATA[<p>
Some weeks ago I wrote about <a href="/wordpress/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/">deploying Symfony2 Applications to Amazon AWS with Scalarium</a>. It turned out that the described methodology can be improved in several ways. Here&#8217;s how.
</p>

<p>
First, let&#8217;s discuss what&#8217;s suboptimal with the previously described approach. The basic idea was to provide a custom Chef recipe which is executed on our instances whenever our Symfony2 application gets deployed. This recipe took care of
<ul>
<li>
executing the tasks which need to be done whenever the application is deployed, like installing the Symfony2 vendors or cleaning the application cache
</li>
<li>
configuring Apache to correctly serve the application
</li>
</ul>
</p>

<p>
Well, the problem is that these are really two very different tasks which shouldn&#8217;t be mixed together.
Updating the web server configuration every time you release a new version of your application simply doesn&#8217;t make sense.
</p>

<p>
Thanks to <a href="http://wiki.opscode.com/display/chef/Deploy+Resource">Chef deployment hooks</a>, we can separate these tasks. Whatever needs to be done upon application deployment can be provided within the application itself, making it much more self-contained. This way, application business logic and application deployment logic live in the same source tree.
</p>

<p>
On the other hand, system configuration steps which aren&#8217;t specific to deploying a new version of your application, but are specific to hosting your application in a given system context, shouldn&#8217;t be bundled with your application, but with your system context.
</p>

<p>
Thus, we are going to separate the deployment recipes and the system setup recipes. We will provide the deploy recipes from our application, and the system setup recipes through a custom Chef cookbook, just as we did in <a href="/wordpress/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/">the first version of this tutorial</a>.
</p>


<p>
Let&#8217;s look at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/b325adb91f421ca87eacf8f545339324bfccfc2a/symfony2/recipes/deploy.rb">the old version of our deploy.rb recipe</a>, and decide which parts are related to the deployment of our application, and which parts are related to hosting our application.
</p>

<p>
Well, it&#8217;s actually quite simple &#8211; everything from the beginning of the file through <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/b325adb91f421ca87eacf8f545339324bfccfc2a/symfony2/recipes/deploy.rb#L44">line 44</a> is stuff that needs to be done upon every single deployment, or else we wouldn&#8217;t end up with a working application.
</p>

<p>
Let&#8217;s move this part of the deployment recipe into our application. Where does it belong? When Scalarium&#8217;s Chef deploys our application, it looks for certain scripts in the <em>/deploy</em> directory of our application:
<ul>
<li>/deploy/before_migrate.rb</li>
<li>/deploy/before_symlink.rb</li>
<li>/deploy/before_restart.rb</li>
<li>/deploy/after_restart.rb</li>
</ul>
As their names imply, these scripts are triggered at certain points of the deployment lifecycle. They are closely related to the steps that are necessary when deploying Ruby on Rails applications, and thus not all of them are useful for us when deploying a Symfony2 application.
</p>

<p>
For the steps we want to execute with our recipe (installing the vendors, clearing the app cache, executing db migrations, installing the assets, and chowning app cache and log dirs), the <em>before_symlink.rb</em> is just fine &#8211; it hooks into the deployment process the moment before Chef, after downloading the application source code from Github, changes the symbolic link at <em>/srv/www/symfonyexample/current</em> to the newly downloaded release. At this moment, we have all the source code available, but it is not yet put into production, thus it&#8217;s the most sensible moment for additional setup steps.
</p>

<p>
Moving the deployment-specific parts of our recipe into this hook gives us a <em>before_symlink.rb</em> script as it&#8217;s available in our <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/blob/9bf49a35f352809c06c25b825c8b1f7386eaae2a/deploy/before_symlink.rb">ScalariumExampleSymfony2Application repository on GitHub</a>.
</p>

<p>
The rest of the original recipe is all about configuring Apache in a way that gives us a working vhost for serving our application &#8211; this configuration may change, but it isn&#8217;t related to any specific deployment. Thus, these steps should be done only when our Scalarium/AWS instance is set up, and not on every deployment.
</p>

<p>
And as with the deployment steps, there is a better way to set up Apache for our application, too. Turns out, we don&#8217;t need a recipe at all. The reason is that Scalarium implements a very convenient substitution logic &#8211; if our own cookbook provides a file with the same name at the same location as one from the Scalarium-provided cookbooks, the file from our own cookbook &#8220;wins&#8221; and is used instead.
</p>

<p>
The Scalarium file we are going to substitute is located at <a href="https://github.com/scalarium/cookbooks/blob/master/mod_php5_apache2/templates/default/web_app.conf.erb">/mod_php5_apache2/templates/default/web_app.conf.erb</a>. By providing this file <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/f5850713ef4663175ad326d709f8e03ad8737243/mod_php5_apache2/templates/default/web_app.conf.erb">in our own cookbook repository</a>, it&#8217;s used as the template for our Symfony2 application vhost &#8211; we don&#8217;t even need to define &#8220;symfony2::deploy&#8221; as a <em>Custom Recipe</em> anymore.
</p>

<p>
And that&#8217;s it. As described in my previous post on this topic, we need to configure our Scalarium cloud with the information on our custom cookbook and our application, but instead of manually hooking our custom recipe into the <em>configure</em> and <em>deploy</em> events in Scalarium, our newly created deploy hook now lives in the application itself, and is automatically triggered on every deployment, and our newly created <em>web_app.conf.erb</em> Apache vhost template is automatically used by Scalarium&#8217;s Chef when setting up new PHP application server instances.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Cocoa: What to do if outlineView: objectValue ForTableColumn: byItem never gets called]]></title>
		<link rel="alternate" type="text/html" href="/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/" />
		<id>http://172.16.233.129/wordpress/?p=441</id>
		<updated>2012-01-18T10:52:05Z</updated>
		<published>2011-11-15T21:10:21Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[
If you set up an OutlineView in Interface Builder and connect your Controller as its dataSource and delegate (and provide the methods there accordingly), you will notice that


- (id)outlineView:(NSOutlineView *)outlineView
  objectValueForTableColumn:(NSTableColumn *)tableColumn
  byItem:(id)item


never get&#8217;s called. The reason might very simple: in Interface Builder, in the attributes inspector for your OutlineView, you can define [...]]]></summary>
		<content type="html" xml:base="/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/"><![CDATA[<p>
If you set up an OutlineView in Interface Builder and connect your Controller as its dataSource and delegate (and provide the methods there accordingly), you will notice that
</p>
<p>
<code>- (id)outlineView:(NSOutlineView *)outlineView
  objectValueForTableColumn:(NSTableColumn *)tableColumn
  byItem:(id)item</code>
</p>
<p>
never get&#8217;s called. The reason might very simple: in Interface Builder, in the attributes inspector for your OutlineView, you can define the <em>Content Mode</em> as <em>View Based</em> or <em>Cell Based</em>.
</p>
<p>
If you simply want to display the content of NSStrings, choose <em>Cell Based</em>, and objectValueForTableColumn will be called.
</p>
<p>
Only took me 3 hours to find out.
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/11/15/what-to-do-if-outlineview-objectvaluefortablecolumn-byitem-is-never-called/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[How hosting MyHammer started in a devops way back in the day]]></title>
		<link rel="alternate" type="text/html" href="/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/" />
		<id>http://172.16.233.129/wordpress/?p=433</id>
		<updated>2012-07-28T19:42:25Z</updated>
		<published>2011-11-07T15:47:59Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[
This here is just me, bragging about myself. You have been warned.







This guy with the psychopathic look standing in a room full of rubbish in front of something that vaguely looks like computers is me standing in front of the first MyHammer server cluster, only days before the whole system went into production.



It was 2005, [...]]]></summary>
		<content type="html" xml:base="/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/"><![CDATA[<p>
This here is just me, bragging about myself. You have been warned.
</p>

<p>
<a href="/wp-content/uploads/2011/11/manuel_kiessling_myhammer_fai.jpg"><img src="/wp-content/uploads/2011/11/manuel_kiessling_myhammer_fai-300x225.jpg" alt="Manuel Kiessling in front of the very first MyHammer FAI server cluster" title="Manuel Kiessling in front of the very first MyHammer FAI server cluster" width="300" height="225" class="alignnone size-medium wp-image-434" /></a>
</p>

<p>
This guy with the psychopathic look standing in a room full of rubbish in front of something that vaguely looks like computers is me standing in front of the first MyHammer server cluster, only days before the whole system went into production.
</p>

<p>
It was 2005, and I had set up the whole system in a way I&#8217;m still quite proud of. Being the only systems administrator at the company at that time, I knew that I wouldn&#8217;t face happy times if I would set up the cluster in a traditional way, that is, machine by machine. Although it started small, the system was supposed to grow, especially regarding the web servers, and even I knew that while managing 13 servers individually was doable, it wasn&#8217;t fun, and managing several dozen machines just sounded like total nightmare.
</p>

<p>
Which is why I thought about better ways to setup and manage the whole cluster. I ended up with a central installation server running <a href="http://fai-project.org/">FAI &#8211; Fully Automatic Installation</a>, and hosting a <a href="http://cfengine.com/community">cfengine</a> server. This way, new machines could be automatically provisioned with the operating system (Debian GNU/Linux) they needed by simply booting from their network adaptor, and new as well as existing systems could easily be maintained, configured, and updated via cfengine. No need to ever log in on a specific machine to change configuration or install software. A centralized syslog server completed the picture.
</p>

<p>
cfengine was a major pain in the ass at times, and with Chef and Puppet there a way more sophisticated tools available today, but the overall system ran extremely well and could easily scale over time.
</p>

<p>
Years later, the term Devops was coined, and I couldn&#8217;t help congratulating myself for getting it right so long ago.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/11/07/how-hosting-myhammer-started-in-a-devops-way-back-in-the-day/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Deploying Symfony2 Applications to Amazon AWS with Scalarium]]></title>
		<link rel="alternate" type="text/html" href="/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/" />
		<id>http://172.16.233.129/wordpress/?p=391</id>
		<updated>2012-07-28T19:43:44Z</updated>
		<published>2011-11-01T10:18:46Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[This article describes how to use the cloud-based cluster-management platform Scalarium in order to automatically mass-deploy Symfony2 applications with a MySQL database backend onto clusters of Amazon EC2 virtual machines by creating a special Symfony2 environment, using a custom Chef recipe, and making use of Doctrine migrations.]]></summary>
		<content type="html" xml:base="/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/"><![CDATA[<h2 style="color: red">Important Note</h2>
<p>
The methodology explained here is <span style="color: red">outdated</span>, please read <a href="/wordpress/2012/01/05/deploying-symfony2-apps-via-scalarium-improved-methodology/">Deploying Symfony2 Apps via Scalarium: Improved Methodology</a> for an updated version.
</p>

<h2>About</h2>
<p>
This article describes how to use the cloud-based cluster-management platform Scalarium in order to automatically mass-deploy Symfony2 applications with a MySQL database backend onto clusters of Amazon EC2 virtual machines by creating a special Symfony2 environment, using a custom Chef recipe, and making use of Doctrine migrations.
</p>

<h2>Target audience</h2>
<p>
Current or soon-to-be Scalarium customers, devops interested in deploying Symfony2 applications using centralized configuration management tools like Chef or Puppet, PHP developers.
</p>

<h2>Prerequisites</h2>
<p>
In order to get your Symfony2 application running on Amazon AWS, you need an AWS account, a Scalarium account (correctly set up with your AWS credentials), and a Github account to host your Symfony2 application and your custom Chef recipes. This tutorial assumes that you are already familiar with managing EC2 clusters with Scalarium. It&#8217;s tested to work with Symfony 2.0.4 on Ubuntu 11.04 instances.
</p>

<h2>Overview</h2>
<p>
<a href="/wordpress/wp-content/uploads/2011/11/Logical-overview.png"><img src="/wordpress/wp-content/uploads/2011/11/Logical-overview-300x225.png" alt="" /></a>
</p>

<p>
The above diagram is here to illustrate how the different parts are working together in order to deploy a running application. The virtual machines hosting our Symfony2 application and the database are EC2 instances running in the Amazon AWS cloud. We will use Scalarium to define the layout of our cluster (called a &#8220;cloud&#8221; in Scalarium), to configure the application which will be deployed onto the application servers within this cluster, and to configure the custom Chef recipes which need to be applied to our application server instances in order to finalize the deployment and setup of our application.
</p>

<p>
Looking at the diagram, we could say that Scalarium <em>manages</em>, Amazon <em>runs</em>, and Github <em>provides</em> &#8211; Scalarium uses what Github provides in order to run the application on Amazon.
</p>

<p>
Because Scalarium already provides the setup logic and recipes that allow us to set up basic Apache/PHP application servers and MySQL database servers on AWS, we will only need to provide what is needed in order to finalize the deployment and setup of our specific Symfony2 application &#8211; setting up a basic Ubuntu server, installing software packages like PHP, Apache, MySQL, and checking out our Symfony2 application code from Github is all done by Scalarium without the need to provide additional means.
</p>

<h2>The example application</h2>
<p>
It makes sense to follow this tutorial using your own Symfony2 application. However, an example application hosted at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application">Github.com â†’ MyHammer â†’ ScalariumExampleSymfony2Application</a> is used to illustrate the process &#8211; you are free to use this repository to deploy the application to your Scalarium cloud. The <a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/commits/master">commit history</a> of this repository also shows step by step which changes need to be applied to a Symfony2 application to prepare it for running on a Scalarium cloud.
</p>

<h2>Setting up a Scalarium cloud for our application</h2>
<p>
Our very first step is to set up a new cloud in Scalarium which manages the EC2 instances, server roles, application configuration and custom Chef cookbooks needed to deploy and run our Symfony2 application.
</p>

<p>
In order to do so, click on <em>Add Cloud</em> in the upper right corner of the main Scalarium administration interface. Choose whatever name you like (I will stick with &#8220;Symfony&#8221;), define a region that makes sense for you (Europe in my case), and choose &#8220;Ubuntu 11.04&#8243; as the default operating system. Please choose &#8220;Role Dependent&#8221; as the <em>Hostname Theme</em>, because this will make it much easier to follow my explanations whenever I&#8217;m talking about specific machine instances.
</p>

<p>
Assuming that you have already set up Amazon AWS in your Scalarium account, please choose the AWS credentials and SSH keys accordingly.
</p>

<p>
You now have an empty cloud. Let&#8217;s define these three things in order to host our application:
<ul>
<li>An application</li>
<li>Custom Cookbooks</li>
<li>An application server and a database server role</li>
</ul>
</p>

<p>
Let&#8217;s start with our application. Later, we are going to make some modifications to our application that are necessary for hosting it with Scalarium, but we can already configure it into our cloud. Clicking on <em>Add Application</em> gives us a dialogue in which we can configure where EC2 instances will be able to pull our application from upon bootup or application deployment.
</p>

<p>
I&#8217;m going to choose &#8220;SymfonyExample&#8221; as the name. Please choose &#8220;PHP&#8221; as the <em>Application Type</em>. If you are going to use the sample application that resides on Github, please choose a <em>Repository Type</em> of &#8220;Git&#8221;, and enter the following <em>Repository URL</em>: &#8220;git://github.com/MyHammer/ScalariumExampleSymfony2Application.git&#8221;. All other parameters are not important for now &#8211; however, if you are configuring your own private repository, you will need to provide a deploy SSH key.
</p>

<p>
Although this is sufficient to deploy the code from Github onto our instances, it&#8217;s not enough to actually get a running application. Additional steps are necessary after the code has been pulled from Github, and those steps need to be performed using a custom Chef recipe. We are going to write this recipe later, but we can already configure the cookbook that contains the recipe into our cloud. On the cloud overview page, select <em>Manage Cookbooks</em> from the <em>Actions</em> dropdown.
</p>

<p>
In the new dialogue, check <em>Enable custom cookbooks</em>, and configure as follows:
<ul>
<li><em>Repository Type</em>: &#8220;Git&#8221;</li>
<li><em>Repository URL</em>: &#8220;git://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes.git&#8221;</li>
</ul>
<em>Deploy SSH Key</em> and <em>Branch / Revision</em> don&#8217;t need to be filled out.
</p>

<p>
As a last step, our cloud needs two machine roles: one for our application servers, and one for our database server. We will start with the application server role. On the cloud overview page, select <em>Add role</em>, and choose the <em>PHP Application Server</em> role.
</p>

<p>
Once you have added it, an additional configuration is necessary: In <em>Custom Recipes</em>, please add the recipe &#8220;symfony2::deploy&#8221; for the Scalarium actions <em>configure</em> and <em>deploy</em>. This makes sure that our recipe is run everytime our application is deployed to an instance, or whenever the cloud changes.
</p>

<p>
Last but not least, create an additional role <em>MySQL Master</em>. Additional configuration is not needed here. If you like, you can already add and boot an instance for this role.
</p>

<h2>Modifying the Symfony2 application</h2>
<p>
Our Symfony2 application doesn&#8217;t need to be re-programmed in any way in order to run via Scalarium &#8211; however, its configuration needs to be specifically modified. You can either modify your existing &#8220;prod&#8221; environment configuration, or you may want to create a new environment called &#8220;scalarium&#8221;, which is what I did for the example application.
</p>

<p>
The nice thing about Scalarium and Symfony2 is that the former provides a very nice PHP interface to the layout of the current cluster, and the latter provides an easy way to make use of this interface.
</p>

<p>
This allows us to dynamically configure our application&#8217;s database setup, avoiding the need to hard code e.g. the IP address of our MySQL master into the application. Here is a step by step description on how to achieve this:
</p>

<p>
First, create a new and empty file <em>/app/config/config_scalarium.yml</em>. This is going to be the main configuration file for the new &#8220;scalarium&#8221; environment. Fill the file with the following content:

<code>imports:
    - { resource: parameters_scalarium.php }
    - { resource: config.yml }
</code>

This makes our new environment use 99% of it&#8217;s settings based on those from the &#8220;prod&#8221; environment &#8211; however, an additional file is imported, <em>/app/config/parameters_scalarium.php</em> &#8211; this is where we are going to dynamically define our database settings using the power of Scalarium.
</p>

<p>
Let&#8217;s fill <em>/app/config/parameters_scalarium.php</em> with life:

<code>&lt;?php

include_once(__DIR__ . '/../../scalarium.php');

$scalarium = new Scalarium();
$container-&gt;setParameter('database_driver',  'pdo_mysql');
$container-&gt;setParameter('database_host',     $scalarium->db->host);
$container-&gt;setParameter('database_port',     '3306');
$container-&gt;setParameter('database_name',     $scalarium->db->database);
$container-&gt;setParameter('database_user',     $scalarium->db->username);
$container-&gt;setParameter('database_password', $scalarium->db->password);
$container-&gt;setParameter('database_path',     null);

unset($scalarium);</code>

What happens here is that the database parameter placeholders like <em>%database_host%</em>, which are used in <em>/app/config/config.yml</em> to configure the database for our application, are programmatically defined based on the current state of the Scalarium cloud. Leveraging the power of the <em>scalarium.php</em> script, which is available in the root directory of our application per default (a symlink is created whenever our application is deployed on an instance), this makes our database config dynamic and therefore always up to date.
</p>

<p>
<blockquote>
The creators of Scalarium point out that using <em>scalarium.php</em> is just a convenience. The full-stack alternative would be to write a Chef recipe that generates the configuration file and register this recipe on the <em>configure</em> and <em>deploy</em> events. Inside the Chef recipe Scalarium provides you with all necessary information like which hosts exist in the cloud and what their configuration is (e.g. IP addresses, roles, etc.). See <a href="http://support.scalarium.com/kb/custom-instance-setup">http://support.scalarium.com/kb/custom-instance-setup</a> for further information on how to use the Scalarium cloud structure information in your own recipes.
</blockquote>
</p>

<p>
One important thing to consider here is that the settings in <em>/app/config/parameters.ini</em> overwrite settings with the same name in <em>/app/config/parameters_scalarium.php</em> &#8211; we therefore need to remove all lines containing &#8220;%database&#8221; from <em>parameters.ini</em>!
</p>

<p>
The last step is to create a Frontend Controller for our new &#8220;scalarium&#8221; environment at <em>/web/app_scalarium.php</em>:

<code>&lt;?php

require_once __DIR__.'/../app/bootstrap.php.cache';
require_once __DIR__.'/../app/AppKernel.php';
//require_once __DIR__.'/../app/AppCache.php';

use Symfony\Component\HttpFoundation\Request;

$kernel = new AppKernel('scalarium', false);
$kernel-&gt;loadClassCache();
//$kernel = new AppCache($kernel);
$kernel-&gt;handle(Request::createFromGlobals())-&gt;send();
</code>
</p>

<p>
You can look at a visual before-and-after comparison of all these changes on Github:
<br />
<a href="https://github.com/MyHammer/ScalariumExampleSymfony2Application/compare/d2fad2a06b82471c173bd211a59223075b87ab98...0392bed4c1bf5e08b98a48aff3fd71760c938a93"><img src="/wp-content/uploads/2011/11/github_comparison_symfony2_scalarium_environment-249x300.png" width="249" height="300"/></a>

<h2>Putting the parts together</h2>
<p>
Our cloud now has everything in place it needs. Its roles are defined, our application servers know where to pull our application from, instances of role &#8220;PHP Application Server&#8221; are configured to use our custom Symfony2 deploy recipe, and our application has been modified to make use of the dynamic cloud configuration in a new environment called &#8220;scalarium&#8221;.
</p>

<p>
Let&#8217;s take a moment to look at <a href="https://github.com/MyHammer/ScalariumExampleSymfony2ChefRecipes/blob/master/symfony2/recipes/deploy.rb#L1">our custom Chef recipe</a>.
</p>

<p>
What this recipe does is it checks for each application that is going to be deployed if it&#8217;s actually a Symfony2 application, and if yes, it starts by updating the vendors (or installing them from scratch if none exist yet), clears the application&#8217;s cache, applies database migrations (if any), makes sure that cache and log files and directories are read and writable for those users that need it, installs and configures an Apache vhost file for the app, and disables the default vhost.
</p>

<p>
Ok, let&#8217;s deploy our application! All you need to do is to add an instance of role <em>PHP Application Server</em> and boot it up. Everything else happens automatically:
<ul>
<li>Scalarium requests an EC2 instance from Amazon AWS which is booted with a fresh Ubuntu 11.04 image</li>
<li>Scalarium sets this machine up with some basic packages like PHP, Apache etc., and executes its basic setup routines (which are Chef recipes, too)</li>
<li>Our Symfony2 application is pulled from Github and deployed to <em>/srv/www/symfonyexample/current</em></li>
<li>The instance executes our custom Chef recipe <em>symfony2::deploy</em> which installs the vendors, clears the app cache, applies the database migrations, installs the web assets, sets users and permissions on cache and log files, and configures an Apache vhost for our application</li>
</ul>
</p>

<p>
Once this is done, you can browse to the public IP of the booted application server instance, which will display the Symfony2 welcome page.
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/11/01/deploying-symfony2-applications-to-amazon-aws-with-scalarium/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Behaviour-driven node.js development with stubbing by combining Vows and node-gently]]></title>
		<link rel="alternate" type="text/html" href="/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/" />
		<id>http://172.16.233.129/wordpress/?p=374</id>
		<updated>2012-05-06T15:04:44Z</updated>
		<published>2011-04-13T18:35:16Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[
I&#8217;m about 3.5 hours into node.js development, I guess that qualifies me to give advice on it on this Internet thing.



Being the BDD fanatic that I am, I wanted to start off behaviour-driven right from the beginning, and Vows looked like a good choice.



However, I quickly came to the point where I needed to stub [...]]]></summary>
		<content type="html" xml:base="/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/"><![CDATA[<p>
I&#8217;m about 3.5 hours into node.js development, I guess that qualifies me to give advice on it on this Internet thing.
</p>

<p>
Being the BDD fanatic that I am, I wanted to start off behaviour-driven right from the beginning, and <a href="http://vowsjs.org/">Vows</a> looked like a good choice.
</p>

<p>
However, I quickly came to the point where I needed to stub out a dependency in one of my modules, and as far as I can see, Vows doesn&#8217;t provide mocking/stubbing. But <a href="https://github.com/felixge/node-gently">https://github.com/felixge/node-gently</a> does, and here is my approach at combining these two:
</p>

<p>
This is the Vows spec:
<code>var gently = global.GENTLY = new (require("gently"));

var vows = require("vows"),
    assert = require("assert");

var myModule = require("MyModule");

vows.describe("My Module").addBatch({
  "when calling its foo() method": {
    topic: myModule,
    "it triggers a console message": function (topic) {
      gently.expect(gently.hijacked.sys, "puts", function(str) {
        assert.equal(str, "Hello World");
      });
      topic.foo("Hello World");
    }
  }
}).export(module);
</code>
</p>

<p>
And this is the implementation of MyModule:
<code>if (global.GENTLY) require = GENTLY.hijack(require);

var sys = require("sys");

function foo(message) {
  sys.puts(message);
  return true;
}

exports.foo = foo;
</code>
</p>
<p>
No idea if this makes any sense in the long run &#8211; I will tell you when I&#8217;m about 14 hours or so into BDD node.js development&#8230;
</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/04/13/behaviour-driven-node-js-development-with-stubbing-by-combining-vows-and-node-gently/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[The only job application form that makes sense]]></title>
		<link rel="alternate" type="text/html" href="/2011/04/11/the-only-job-application-form-that-makes-sense/" />
		<id>http://172.16.233.129/wordpress/?p=370</id>
		<updated>2011-04-11T10:01:04Z</updated>
		<published>2011-04-11T10:00:14Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Other" />		<summary type="html"><![CDATA[It just dawned on me that this is really the only job application form that makes sense...]]></summary>
		<content type="html" xml:base="/2011/04/11/the-only-job-application-form-that-makes-sense/"><![CDATA[<p>
It just dawned on me that this is really the only job application form that makes sense, isn&#8217;t it?
</p>
<p>
<img src="http://manuel.kiessling.net/images/job_application_form.png" alt="Job Application Form" />
</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/04/11/the-only-job-application-form-that-makes-sense/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/04/11/the-only-job-application-form-that-makes-sense/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Why developing without tests is like driving a car without brakes]]></title>
		<link rel="alternate" type="text/html" href="/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/" />
		<id>http://172.16.233.129/wordpress/?p=367</id>
		<updated>2011-04-07T07:57:07Z</updated>
		<published>2011-04-07T07:37:55Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[The following roots in something I heard from Jon Jagger at QCon London 2011 after his fantastic talk about Deliberate Practice. It was related to Test Driven Development. He asked "Why do cars have brakes?". It made us think "To stop!", but he said "No - to go faster".]]></summary>
		<content type="html" xml:base="/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/"><![CDATA[<blockquote>
<p>
The following roots in something I heard from <a href="http://twitter.com/JonJagger">Jon Jagger</a> at QCon London 2011 after his fantastic talk about <a href="http://qconlondon.com/london-2011/presentation/Deliberate+Practice">Deliberate Practice</a>. It was related to Test Driven Development. He asked &#8220;Why do cars have brakes?&#8221;. It made us think &#8220;To stop!&#8221;, but he said &#8220;No &#8211; to go faster&#8221;.
</p>
<p>
(Jon points out he didn&#8217;t invent it, he heard it from <a href="http://twitter.com/KevlinHenney">Kevlin Henney</a>).
</p>
<p>
I have been thinking about this ever since, and here is what I came up with.
</p>
</blockquote>

<p>
Imagine I would give you the keys to my car. I would tell you &#8220;here are the keys, you can drive wherever you want, including the highway, have fun!&#8221;
</p>

<p>
How fast would go? My car is not exactly a sports car, but it can do around 200 km/h. I guess we both agree that you would drive around 50 km/h within cities (the maximum allowed in Germany), and probably up to 200 km/h on the highway, as long as there is no limit.
</p>

<p>
Fine. Now image this: I would give you the keys to my car. I would tell you &#8220;here are the keys, you can drive wherever you want, including the highway, have fun! Oh, just one thing, <strong>the brakes don&#8217;t work</strong>.&#8221;
</p>

<p>
Now let&#8217;s forget for a moment that in reality, you probably wouldn&#8217;t start at all, if you <em>had</em> to drive, then how fast would you go? 10 km/h, maybe 20? Driving extremely cautious, always trying to look as far ahead as possible if you are going to need to halt? Yeah, I thought so.
</p>

<p>
But why is that? The brakes don&#8217;t have anything to do with the speed of my car &#8211; it&#8217;s still capable of doing 200 km/h just fine!
</p>

<p>
It&#8217;s because the ability to stop is what enables you to go real fast. With only a bit of exaggaration you could say that having a brake allows for a very &#8220;iterative&#8221; way of driving &#8211; no cars within the next 300 meters, let&#8217;s accelerate a bit &#8211; oh, there&#8217;s a car coming over from the right, let&#8217;s brake a bit &#8211; ok, now I can accelerate again &#8211; ah, there is a signal that suddenly turned red, no problem, I will stop here.
</p>

<p>
For me, this metaphor is the best I could find by now to explain to myself (and in the future, to others), why I <em>really</em> want to develop test-driven, and why it actually makes me faster, not slower, although I&#8217;m doing more.
</p>

<p>
Just as the brake doesn&#8217;t directly influence your driving speed, but does so indirectly, your tests won&#8217;t influence your coding speed directly, but indirectly. It&#8217;s because once they are in place, they allow you to iterate over your code and refactor it at what I, from my own experience, can only describe as the speed of light compared to conventional programming.
</p>

<p>
With tests in place, it&#8217;s like: Mh, what if I would split this rather long method into two? &#8211; ok, works; What if I put a bit more of dependency injection into this class? &#8211; ah, now this test here fails, no problem, I will have it back to green within minutes, I know exactly where to go to fix this; Hey, I could give this method here a better name &#8211; ok, still green; There&#8217;s this performance bottleneck deep inside this one class that is heavily used by a lot of other classes, let&#8217;s see if I can fix this &#8211; my tests will tell me if I accidently changed behaviour.
</p>

<p>
Compare this to conventional programming: You will never know for sure what breaks somewhere else if you change something. If you want to find out, you need huge amounts of manpower to have your webpage or UI tested for regression. What really happens is that you slow down to a near halt: because you don&#8217;t know what&#8217;s around the next corner when developing, and you know there is nothing that will immediately stop you and save you from harm if you take that next corner, you will drive, err, code so cautious, you won&#8217;t make any real progress.
</p>

<blockquote>
On which Jon commented: &#8220;Yes. As the pragmatic programmers say, paraphrasing &#8211; <em>you don&#8217;t know why it&#8217;s broken because you didn&#8217;t know why it worked in the first place.</em>&#8221;
</blockquote>
]]></content>
		<link rel="replies" type="text/html" href="/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/04/07/why-developing-without-tests-is-like-driving-a-car-without-brakes-2/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Visualization: Why working iterative works]]></title>
		<link rel="alternate" type="text/html" href="/2011/03/10/visualization-why-working-iterative-works/" />
		<id>http://172.16.233.129/wordpress/?p=299</id>
		<updated>2011-03-10T17:38:05Z</updated>
		<published>2011-03-10T16:48:18Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[
I&#8217;m really into visualizations. More often then not I can only really &#8220;get&#8221; something (a complex system, an abstract idea, a process etc.) when I see it visualized. You could call this the transformation of gut feelings into images.



So, I had this (rather obvious) gut feeling that working iterative in software projects makes a lot [...]]]></summary>
		<content type="html" xml:base="/2011/03/10/visualization-why-working-iterative-works/"><![CDATA[<p>
I&#8217;m really into visualizations. More often then not I can only really &#8220;get&#8221; something (a complex system, an abstract idea, a process etc.) when I see it visualized. You could call this the transformation of gut feelings into images.
</p>

<p>
So, I had this (rather obvious) gut feeling that working iterative in software projects makes a lot of sense, I&#8217;ve heard all the arguments and explanations and examples and stuff like that, and I probably already &#8220;got it&#8221;, but I thought it could make sense to clearly work out why exactly it makes sense, by visualizing it. Surely it&#8217;s no rocket surgery what happens here, but I kind of like it and would like to share it.
</p>

<p>
So here&#8217;s my approach:
</p>

<p>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Start.png" width="450" height="800" border="0" style="border: none" align="center" />
<br/>
<br />
The customer tells a product manager what he wants him to build. Let&#8217;s see how a waterfall approach leads to the product team failing at this:
<br />
<br />
</p>

<p>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Waterfall.png" width="450" height="800" border="0" style="border: none" align="center" />
<br />
The reason why what is finally delivered isn&#8217;t what the customer expected is that the project goes through the hands of different people and different stages, and every time the project is given to another person or team, the amount of misunderstanding grows. That&#8217;s only natural because we cannot copy ideas from one brain to another in a 1:1 manner.
</p>

<p>
Here is why an iterative approach makes sense:
<br />
<br/>
<img src="http://manuel.kiessling.net/images/Why%20working%20iterative%20pays%20out%20-%20Iterations.png" width="450" height="800" border="0" style="border: none" align="center" />
<br />
The project really starts the same: There is a certain amount of misunderstanding, and the team does things wrong. But due to the regular feedback from the customer, this wrong direction can be corrected. It might then move into another direction which is still a bit wrong, but then comes the next correction, and finally everything is on track.
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2011/03/10/visualization-why-working-iterative-works/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/03/10/visualization-why-working-iterative-works/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Interview mit mir auf dem O&#8217;Reilly Blog]]></title>
		<link rel="alternate" type="text/html" href="/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/" />
		<id>http://172.16.233.129/wordpress/?p=289</id>
		<updated>2011-01-31T12:33:12Z</updated>
		<published>2011-01-31T12:26:43Z</published>
		<category scheme="/" term="Other" />		<summary type="html"><![CDATA[<p>
Das Blog von O'Reilly Deutschland hat in seiner Reihe "Karriere(n) in der IT" ein <a href="http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/">kurzes Interview</a> mit mir gefÃ¼hrt.
</p>]]></summary>
		<content type="html" xml:base="/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/"><![CDATA[<p>
Das Blog von O&#8217;Reilly Deutschland hat in seiner Reihe &#8220;Karriere(n) in der IT&#8221; ein kurzes Interview mit mir gefÃ¼hrt:
</p>
<p>
<a href="http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/">http://community.oreilly.de/blog/2011/01/31/job-portrait-softwarenentwickler-bei-myhammer-de/</a>
</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/01/31/interview-mit-mir-auf-dem-oreilly-blog/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Empfehlung: Literatur fÃ¼r Manager]]></title>
		<link rel="alternate" type="text/html" href="/2011/01/21/literatur-fuer-manager/" />
		<id>http://172.16.233.129/wordpress/?p=272</id>
		<updated>2011-01-21T13:56:26Z</updated>
		<published>2011-01-21T13:52:04Z</published>
		<category scheme="/" term="Recommendations" />		<summary type="html"><![CDATA[
Ich werde oft gefragt, wie ich so ein hervorragender Manager geworden bin. Haha, quatsch, kein Mensch fragt mich das. Ich stelle mir nur manchmal vor, dass es so wÃ¤re. Dann weine ich mich langsam in den Schlaf&#8230; Wo war ich? Ach ja.



Also, ich wurde nach Literatur zum Thema Management bzw. MitarbeiterfÃ¼hrung gefragt. Spontan fiel mir [...]]]></summary>
		<content type="html" xml:base="/2011/01/21/literatur-fuer-manager/"><![CDATA[<p>
Ich werde oft gefragt, wie ich so ein hervorragender Manager geworden bin. Haha, quatsch, kein Mensch fragt mich das. Ich stelle mir nur manchmal vor, dass es so wÃ¤re. Dann weine ich mich langsam in den Schlaf&#8230; Wo war ich? Ach ja.
</p>

<p>
Also, ich wurde nach Literatur zum Thema Management bzw. MitarbeiterfÃ¼hrung gefragt. Spontan fiel mir dazu erst mal nicht viel ein, weil ich keinen Kanon dazu im Hinterkopf habe, und weil FÃ¼hrungsqualitÃ¤ten sowieso nur bedingt anlesbar sind. Dann sind mir aber doch ein paar Quellen eingefallen die mir vieles klarer gemacht haben und auf deren Erkenntnisse ich in der Praxis immer wieder zurÃ¼ckgreifen konnte:
</p>

<p>
<strong>Samy Molcho &#8211; Alles Ã¼ber KÃ¶rpersprache</strong>

<p>
Kein Buch Ã¼ber Management im engeren Sinne. Ist aber eine gute Anleitung fÃ¼r den Umgang mit Menschen, also das, was man in 99,9% der Zeit als Manager tun muss. Hat mir Ã¼brigens vor allem geholfen, mich selbst einzuschÃ¤tzen, der Aspekt &#8220;ich lese jetzt die KÃ¶rpersprache anderer Menschen und kann praktisch ihre Gedanken lesen&#8221; ist denke ich Ã¼berbewertet. Ãœbrigens das vielleicht wichtigste Buch als Vorbereitung auf ein BewerbungsgesprÃ¤ch.
</p>

<p>
Link zu Amazon: <a href="http://www.amazon.de/gp/product/3442390478?ie=UTF8&#038;tag=thelogbooofma-21&#038;linkCode=as2&#038;camp=1638&#038;creative=6742&#038;creativeASIN=3442390478">Alles Ã¼ber KÃ¶rpersprache: sich selbst und andere besser verstehen</a><img src="http://www.assoc-amazon.de/e/ir?t=thelogbooofma-21&#038;l=as2&#038;o=3&#038;a=3442390478" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
</p>
</p>


<p>
<strong>Project Management lessons from NASA</strong>

<p>
<a href="http://www.mariosalexandrou.com/blog/project-management-lessons-from-nasa/">http://www.mariosalexandrou.com/blog/project-management-lessons-from-nasa/</a>
</p>

<p>
Jeder einzelne Satz ist Gold wert und sollte in Bezug auf die eigene Situation reflektiert werden. Das ist schon eine Zusammenfassung, mÃ¼sste ich es noch weiter runterdestillieren, wÃ¤ren das meine wichtigsten Punkte:

<ul>
<li>Most managers succeed on the strength and skill of their staff.</li>
<li>Sometimes the best thing to do is nothing. It is also occasionally the best help you can give. Just listening is all that is needed on many occasions. You may be the boss but, if you constantly have to solve someone&#8217;s problems, you are working for him.</li>
<li>Never assume someone knows something or has done something unless you have asked them. </li>
<li>Never ask management to make a decision that you can make. Assume you have the authority to make decisions unless you know there is a document that states unequivocally that you cannot.</li>
</ul>
</p>
</p>





<p>
<strong>Dale Carnegie &#8211; Wie man Freunde gewinnt: Die Kunst, beliebt und einflussreich zu werden</strong>

<p>
In Teilen etwas angestaubtes, aber sehr lehrreiches Buch, das vor allem sehr menschlich und berÃ¼hrend vermittelt, wie man andere Menschen behandeln sollte. Ich konnte fÃ¼r meine Arbeit als Manager unglaublich viel davon ableiten.
</p>

<p>
Link zu Amazon: <a href="http://www.amazon.de/gp/product/3596170699?ie=UTF8&#038;tag=thelogbooofma-21&#038;linkCode=as2&#038;camp=1638&#038;creative=19454&#038;creativeASIN=3596170699">Wie man Freunde gewinnt: Die Kunst, beliebt und einflussreich zu werden</a><img src="http://www.assoc-amazon.de/e/ir?t=thelogbooofma-21&#038;l=as2&#038;o=3&#038;a=3596170699" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
</p>
</p>


<p>
<strong>Michael Lopp &#8211; Managing Nerds</strong>

<p>
Zu guter Letzt noch ein lesenswerter Aufsatz fÃ¼r alle, die nicht nur irgendwen, sondern ein Team von Nerds managen dÃ¼rfen: <a href="http://www.randsinrepose.com/archives/2011/01/17/managing_nerds.html">http://www.randsinrepose.com/archives/2011/01/17/managing_nerds.html</a>
</p>

</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/01/21/literatur-fuer-manager/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/01/21/literatur-fuer-manager/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Neues Projekt: Platform Health Viewer]]></title>
		<link rel="alternate" type="text/html" href="/2011/01/11/neues-projekt-platform-health-viewer/" />
		<id>http://172.16.233.129/wordpress/?p=247</id>
		<updated>2011-01-11T09:18:49Z</updated>
		<published>2011-01-11T08:16:53Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[
Platform Health Viewer (kurz PHV) ist mein aktuelles Ruby on Rails Hobbyprojekt.


Sobald es einen stabilen Zustand erreicht, wird dieses Tool das Sammeln und Visualisieren verschiedener statistischer Daten, wie sie typischerweise von Internetplattformen erzeugt werden, schnell und leichtgewichtig ermÃ¶glichen. Beispiele fÃ¼r diese Daten sind Dinge wie die CPU Last einzelner Systeme, Benutzerlogins, Anzahl der Seitenaufrufe usw.


Die [...]]]></summary>
		<content type="html" xml:base="/2011/01/11/neues-projekt-platform-health-viewer/"><![CDATA[<p>
Platform Health Viewer (kurz PHV) ist mein aktuelles Ruby on Rails Hobbyprojekt.
</p>
<p>
Sobald es einen stabilen Zustand erreicht, wird dieses Tool das Sammeln und Visualisieren verschiedener statistischer Daten, wie sie typischerweise von Internetplattformen erzeugt werden, schnell und leichtgewichtig ermÃ¶glichen. Beispiele fÃ¼r diese Daten sind Dinge wie die CPU Last einzelner Systeme, Benutzerlogins, Anzahl der Seitenaufrufe usw.
</p>
<p>
Die Applikation basiert in erster Linie auf <a href="http://rubyonrails.org/">Rails</a>, der HTTP Server fÃ¼r die Datenanlieferung ist in <a href="http://nodejs.org/">node.js</a> geschrieben, die WeboberflÃ¤che nutzt sehr ausgiebig <a href="http://jquery.com/">jQuery</a> und fÃ¼r die Erzeugung von SVG Graphen die <a href="http://raphaeljs.com/">RaphaÃ«l</a> Bibliothek. Massendaten werden in SQL gespeichert, weitere Daten liegen in einer <a href="http://couchdb.apache.org/">CouchDB</a>.
</p>
<p>
Der Projektcode ist auf Github abrufbar unter <a href="https://github.com/ManuelKiessling/PlatformHealthViewer">https://github.com/ManuelKiessling/PlatformHealthViewer</a>.
</p>
<p>
Das folgende Video ist eine kurze EinfÃ¼hrung zur aktuellen Alphaversion des Projekts. Es enthÃ¤lt auÃŸerdem eine lustige Sprecherstimme und eine sehr kreative Interpretation der englischen Grammatik.
</p>

<object width="640" height="385"><param name="movie" value="http://www.youtube.com/v/HI6SRqz_3D0?fs=1&amp;hl=de_DE"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/HI6SRqz_3D0?fs=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="640" height="385"></embed></object>

<br />
<p />
<p>

Im Folgenden das ins Deutsche Ã¼bersetzte Transskript des Videos:

<blockquote>
Hi. Platform Health Viewer &#8211; oder PHV &#8211; ist mein aktuellen Hobbyprojekt.
<p/>
Ich brauche eine einfache und leichtgewichtige MÃ¶glichkeit, die verschiedenen Kennzahlen der Webseite, fÃ¼r die ich verantwortlich bin, zu sammeln und zu visualisieren. Sachen wie die CPU Perfomance wichtiger Systeme, Userlogins, HTTP Anfragen.
<p/>
Deshalb habe ich angefangen mit Ruby on Rails, jQuery, CouchDB und node.js zu experimentieren, und hier ist eine frÃ¼he Alphaversion, die ich demonstrieren mÃ¶chte.
<p/>
Mein Hauptziel ist es, den Prozess von der Einspeisung der Daten in das System bis hin zu ihrer grafischen Visualisierung so einfach wie mÃ¶glich zu gestalten.
<p/>
Um Daten in das System zu bekommen, benÃ¶tigt man lediglich einen HTTP Aufruf, was es sehr einfach macht, die Daten der unterschiedlichsten Quellen zu sammeln.
<p/>
Probieren wir ein Beispiel aus. I mÃ¶chte die CPU Performance meiner lokalen Maschine visualisieren.
<p/>
Ich werde diese Daten bekommen, indem ich einen Standard Unix Befehl verwende: sar.
<p/>
Dies ist ein wichtiger Aspekt meines Ansatzes: FÃ¼r den Platform Health Viewer spielt es Ã¼berhaupt keine Rolle, woher die Daten stammen &#8211; man ist bei den Mitteln der Datenbeschafung vÃ¶llig frei. Dadurch kann man wirklich alles in das System Ã¼bermitteln, angefangen bei allgemeinen Daten wie der CPU Last bis hin zu Ã¤uÃŸerst individuellen Sachen wir den Logins auf der eigenen Webseite.
<p/>
Ok, so bekomme ich den CPU &#8220;usr&#8221; Wert auf meiner OS X Kommandozeile:
<p/>
sar 1 1| grep Average| cut -b 14-15
<p/>
Fein, das wird&#8217;s tun.
<p/>
Wie liefern wir diese Werte in das System? Es reicht ein simpler HTTP Post Aufruf mithilfe von curl:
<p/>
curl &#8211;data &#8220;event[value]=`sar 1 1| grep Average| cut -b 14-15`&#038;event[source]=macbook&#038;event[name]=cpu_usr_percentage&#8221; http://localhost:3000/queue_event
<p/>
Wie man sieht, die Nutzdaten des Aufrufs bestehen aus lediglich drei Parametern: Der Quelle des Events, dem Namen des Events, und seinem Wert.
<p/>
Noch mal: Man ist an dieser Stelle komplett flexibel, man ist nicht gezwungen Eventnamen und -quellen zuvor im PHV zu konfigurieren &#8211; man definiert diese einfach im Moment der Dateneinlieferung, das das System akzeptiert diese. Wir werden gleich sehen, wie man mit den verschiedenen Events, die in das System geschrieben werden, sinnvoll umgeht.
<p/>
Ok, ich verwende jetzt ein kleines Hilfsskript das ich geschrieben habe, um die CPU sys, idle, usr und nice Werte in das System zu liefern:
<p/>
cat script/agents/macosx/cpu_overview_percent.sh
<p/>
Wie man sieht, geschieht alles unter der Verwendung normaler Unixbefehle.
<p/>
Starten wir also das Skript:
<p/>
bash ./script/agents/macosx/cpu_overview_percent.sh http://localhost:3000/ macbook
<p/>
Ich Ã¼bergebe hier nur zwei Parameter, die URL zum Platform Health Viewer (der fÃ¼r diese Demonstration auf demselben Host lÃ¤uft), und den Namen meiner Eventquelle, die ich &#8220;macbook&#8221; nenne. Eventnamen und -werten kommen direkt aus dem Hilfsskript.
<p/>
Man sieht, wie das Skript alle vier CPU Kennzahlen in das System liefert. Schauen wir uns diese Daten innerhalb des Platform Health Viewer an.
<p/>
Nun, das Dashboard ist nach wie vor leer, da wir noch keinerlei Visualisierungen definiert haben. Aber der &#8220;Tageditor&#8221; zeigt ebenfalls noch keinerlei Events an. Das liegt daran, dass die in das System eingelieferten Events noch nicht zu sogenannten Eventtypen normalisiert wurden.
<p/>
Dies ist bewusst ein zusÃ¤tzlicher Schrit, denn es erlaubt dem System, eingehende Events so schnell wie mÃ¶glich in die Datenbank speichern zu kÃ¶nnen, ohne sie in Hinblick auf Name und Quelle normalisieren zu mÃ¼ssen. Diese Normalisierung erledigt ein Rake Task:
<p/>
rake queue:convert
<p/>
Dieser Task liest die Events aus der Anlieferungs-Queue, erzeugt bei Bedarf neue Eventtypen, oder verbindet die Events mit bereits existierenden Eventtypen, falls diese bereits existieren. Abschliessend wird die Anlieferungsqueue geleert.
<p/>
ZurÃ¼ck im Tageditor, kÃ¶nnen wir die vier Eventtypen nun sehen.
<p/>
Ein Eventtyp ist die Kombination einer Eventquelle und eines Eventnamen, also ist &#8220;macbook &#8211; cpu_idle_percentage&#8221; ein Eventtyp.
<p/>
Schauen wir nun, wie wir den Tageditor verwenden kÃ¶nnen um etwas sinnvolles zu basteln. Das Gruppieren von einem oder mehreren Eventtypen unter einem Tag macht unsere eingelieferten Daten visualisierbar. Ich bin Ã¼brigens nicht ganz glÃ¼cklich mit dem Begriff &#8220;Tag&#8221;, vielleicht fÃ¤llt mir da noch etwas besseres ein.
<p/>
Wie auch immer, erzeugen wir nun einen einfachen Tag den wir benutzen kÃ¶nnen, um genau einen Wert zu visualisieren.
<p/>
Ich werde diesen Tag &#8220;macbook_cpu_usr&#8221; nennen. In diesen laufen dann alle Events, deren Quelle &#8220;macbook&#8221; und deren Name &#8220;cpu_usr_percentage&#8221; lautet. Ich kÃ¶nnte diese Parameter auch in die Textbox eintippen, aber es ist einfacher sie schlicht per Drag&#038;Drop in das Formular zu ziehen.
<p/>
Ok, fÃ¼gen wir diesen Tag also hinzu.
<p/>
Wir haben nun also einen ersten Tag, und um zu sehen, ob er wie erwartet funktioniert, kann ich die Werte der zugeordneten Events in einer Vorschau kontrollieren.
<p/>
Liefern wir jetzt ein paar weitere Werte in das System und schauen, ob sie dann hier angezeigt werden.
<p/>
Ok, ich starte also mein Hilfsskript erneut um neue Werte in den Server zu liefern, und starte dann wiederum den Rake Task um die Werte zu normalisieren.
<p/>
Ein erneuter Klick auf &#8220;Show latest events&#8221; zeigt diese neuen Werte nun an.
<p/>
Ich starte die Datenanlieferung jetzt in einer Schleife, um viele Werte zu erhalten.
<p/>
Ok, wir haben nach wie vor keine Datenvisualisierung, also gehen wir das jetzt an. Ich wechsle in&#8217;s Dashboard und fÃ¼ge einen Frame hinzu, dies ist ein Container der unsere Graphen enthalten wird.
<p/>
Ein Frame ist die Visualisierung aller Werte die mit einem Tag verbunden sind, also muss ich den Namen des Tags angeben, das ich mit diesem Frame visualisieren mÃ¶chte.
<p/>
&#8220;Add frame&#8221;, und los geht&#8217;s. Ein einfacher Liniengraph, der einen meiner CPU Werte reprÃ¤sentiert. Der Graph ist im Ãœbrigen ein SVG, erzeugt mit Raphael, einer fantastischen JavaScript Bibliothek.
<p/>
Und dank jQuery kann ich diesen Graphen frei bewegen und skalieren.
<p/>
Erzeugen wir nun einen Graphen fÃ¼r alle meine CPU Werte. ZurÃ¼ck im Tageditor ziehe ich nun alle meine Eventtypen zusammen.
<p/>
Ich kann Ã¼brigens Tags erzeugen, indem ich Eventquellen und -namen mit bereits existierenden Tags kombiniere, wie man hier sieht.
<p/>
Ich Ã¼berprÃ¼fe alle Werte die meinem neuen Tag zugeordnet sind, und dort sieht man alle CPU Werte, die mein Skript gesammelt hat.
<p/>
Wieder zurÃ¼ck im Dashboard gehe ich her und erzeuge einen weiteren Frame, fÃ¼r meinen neuen Tag. Wie man sieht, enthÃ¤lt dieser Frame vier Liniengraphen und zeigt mir so eine hÃ¼bsche Ãœbersicht meiner kompletten CPU Performance. NatÃ¼rlich wird hier eine Legende benÃ¶tigt, etwas das bisher noch nicht implementiert ist.
<p/>
Nun, das ist es, das ist der aktuelle Stand des Projekts. Ich wÃ¼rde mich sehr Ã¼ber Feedback freuen, der Code kann auf Github geforkt werden oder schreibt mir eine Mail.
<p/>
Danke fÃ¼r&#8217;s Interesse.
</blockquote>

</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/01/11/neues-projekt-platform-health-viewer/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/01/11/neues-projekt-platform-health-viewer/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[New project: Platform Health Viewer]]></title>
		<link rel="alternate" type="text/html" href="/2011/01/11/platform-health-viewer/" />
		<id>http://172.16.233.129/wordpress/?p=233</id>
		<updated>2011-01-11T09:14:10Z</updated>
		<published>2011-01-11T07:39:48Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Software" />		<summary type="html"><![CDATA[
Platform Health Viewer is my current Ruby on Rails pet project.


Once stable, it will allow users to easily collect and visualize different types of statistical data which is typically produced by internet platforms, like CPU performance, user logins, HTTP requests etc.


The main application is build on Rails, the server used for data collection is written [...]]]></summary>
		<content type="html" xml:base="/2011/01/11/platform-health-viewer/"><![CDATA[<p>
Platform Health Viewer is my current Ruby on Rails pet project.
</p>
<p>
Once stable, it will allow users to easily collect and visualize different types of statistical data which is typically produced by internet platforms, like CPU performance, user logins, HTTP requests etc.
</p>
<p>
The main application is build on <a href="http://rubyonrails.org/">Rails</a>, the server used for data collection is written in <a href="http://nodejs.org/">node.js</a>, the web interface makes heavy use of <a href="http://jquery.com/">jQuery</a> and uses <a href="http://raphaeljs.com/">RaphaÃ«l</a> to create SVG graphs. Mass data is saved in a SQL db, other data is stored using <a href="http://couchdb.apache.org/">CouchDB</a>.
</p>
<p>
The project&#8217;s code is hosted on Github at <a href="https://github.com/ManuelKiessling/PlatformHealthViewer">https://github.com/ManuelKiessling/PlatformHealthViewer</a>.
</p>
<p>
This video is a short introduction to the current alpha version of the project. A funny voice and lots of grammatical shortcomings are included for free:
</p>

<object width="640" height="385"><param name="movie" value="http://www.youtube.com/v/HI6SRqz_3D0?fs=1&amp;hl=de_DE"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/HI6SRqz_3D0?fs=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="640" height="385"></embed></object>

<br />
<p />
<p>

Transcription of the video:

<blockquote>
Hi. Platform Health Viewer &#8211; or PHV &#8211; is my current pet project.
<p/>
I need an easy and lightweight way to collect and visualize the different key performance indicators of the web platform I&#8217;m responsible for &#8211; stuff like CPU performance of important systems, user logins, http requests.
<p/>
So I started to play around with Ruby on Rails, jQuery, CouchDB and node.js, and here is an early alpha I would like to demonstrate.
<p/>
My primary goal was to make the process from feeding data into the system to visualizing that data as simple as possible.
<p/>
In order to get data into the system, all you need to make is an HTTP call, which makes it very easy to collect data from very different sources.
<p/>
Let&#8217;s try an example. I would like to visualize the cpu usage of my local machine.
<p/>
I&#8217;m going to collect this data using a standard unix command, sar.
<p/>
That&#8217;s an important aspect of my approach: It doesn&#8217;t play any role for the Platform Health Viewer where the data comes from, you&#8217;re completely free to choose how to collect it.
This way you can feed really anything into the system, from generic data like CPU load to highly individual stuff like the user logins of your specific web site.
<p/>
Ok, here is how I can get my cpu &#8220;usr&#8221; value on my OS X command line:
<p/>
sar 1 1| grep Average| cut -b 14-15
<p/>
Great, that will do.
<p/>
How do we push these values into the system? It&#8217;s a simple http post request using curl:
<p/>
curl &#8211;data &#8220;event[value]=`sar 1 1| grep Average| cut -b 14-15`&#038;event[source]=macbook&#038;event[name]=cpu_usr_percentage&#8221; http://localhost:3000/queue_event
<p/>
As you can see, the payload of the post requests is just 3 parameters: the source of the event, the name of the event, and its value.
<p/>
Again, you&#8217;re completely free here, you don&#8217;t need to configure event names and sources inside PHV &#8211; just define them when pushing data into the system, it will happily accept it. We will see in a moment how to make sense of different events that were pushed into the system.
<p/>
Ok, let&#8217;s use a small helper script I wrote in order to feed the CPU sys, idle, usr and nice values into my system:
<p/>
cat script/agents/macosx/cpu_overview_percent.sh
<p/>
As you can see, this is all done using only standard unix commands.
<p/>
Let&#8217;s start the script:
<p/>
bash ./script/agents/macosx/cpu_overview_percent.sh http://localhost:3000/ macbook
<p/>
I&#8217;m just providing two parameters here, the URL to my platform health viewer installation, which resides on the same host for this demo, and the source name, which I call &#8220;macbook&#8221;.
<p/>
As you can see, my script pushes all four CPU usage values into the system. Now let&#8217;s have a look at this data within platform health viewer.
<p/>
Well, the Dashboard is still empty, because we did not yet define any visualizations. But the &#8220;Tageditor&#8221; doesn&#8217;t show any events, too. This is because the events I pushed into the system have not yet been normalized to event-types.
<p/>
This is an additional step, because it will allow the system to push incoming events into the database as quickly as possible without the need to normalize those events regarding their name and source. This normalization is done using a rake task:
<p/>
rake queue:convert
<p/>
This task reads the events from the incoming queue, creates new event-types as needed, or connects the event values with existing event types if they already exists. It then deletes the incoming queue.
<p/>
Getting back to our tageditor, we can now see our 4 event types.
<p/>
An event type is the combination of an event source and an event name, so &#8220;macbook &#8211; cpu_idle_percentage&#8221; is one event type.
<p/>
Let&#8217;s see how we can use the tag editor to create something useful. Grouping one or more event types into a tag is what makes our data suitable for visualization. I&#8217;m not quite happy with the term &#8220;tag&#8221; by the way, maybe I will come up with something better.
<p/>
Anyway, let&#8217;s create a very simple tag which we can use to visualize exactly one value.
<p/>
I&#8217;m going to name my tag &#8220;macbook_cpu_usr&#8221;. It will hold all events whose source matches &#8220;macbook&#8221;, and whose name matches &#8220;cpu_usr_percentage&#8221;. I could type those parameters into the text box, but it&#8217;s easier to just drag&#8217;n'drop them there.
<p/>
Ok, let&#8217;s add this tag.
<p/>
Now we have this first tag, and to check if it works as expected, I can preview the values of the matching events.
<p/>
Let&#8217;s push some new values into our system and check if they are visible here.
<p/>
Ok, I&#8217;m starting my helper skript again in order to post new values to the server, and I start my rake task in order to normalize these values.
<p/>
Clicking again on &#8220;Show latest events&#8221; now shows these values.
<p/>
I will now start data push and normalization in a loop in order to get a lot of values.
<p/>
Ok, we still have no data visualization, so let&#8217;s do this now. Let&#8217;s switch to the Dashboard and add a frame, which is a container that will hold our graph.
<p/>
A frame is the visualization of all values connected to a tag, so I need to provide the name of the tag I want to visualize with this frame.
<p/>
&#8220;Add frame&#8221;, and here we go. A simple line graph representing one of my CPU values. The graph is actually an SVG, created using Raphael, an awesome JavaScript library.
<p/>
And thanks to jQuery, I can freely move and resize the graph.
<p/>
Let&#8217;s create a graph with all my CPU values in it. Back to the Tageditor, I&#8217;m going to drag all my values together.
<p/>
I can also create tags by combining event-sources and -names with already existing tags, as you can see here.
<p/>
Let&#8217;s check the values of my new tag, and there are all the different CPU values my script collects.
<p/>
Back to the Dashboard, I&#8217;m going to create another frame for my new tag. As you can see, this one contains 4 linegraphs and gives me a nice overview of my system&#8217;s CPU performance. Of course, a graph legend is needed, something that&#8217;s not yet implemented.
<p/>
Well, that&#8217;s it, that&#8217;s the current state of this project, I would love to hear your feedback, you can fork the code on github and drop me an e-mail.
<p/>
Thanks for your interest.
</blockquote>

</p>]]></content>
		<link rel="replies" type="text/html" href="/2011/01/11/platform-health-viewer/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2011/01/11/platform-health-viewer/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Testgetriebene Administration &#8211; test driven administration]]></title>
		<link rel="alternate" type="text/html" href="/2010/09/01/testgetriebene-administration-test-driven-administration/" />
		<id>http://172.16.111.147/wordpress/?p=188</id>
		<updated>2010-09-02T12:01:28Z</updated>
		<published>2010-09-01T22:57:25Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[
Ich hatte tatsÃ¤chlich einmal eine ganz eigene Idee. Und sie war gut, auch nachdem ich sie mehrmals durchgekaut und von allen Seiten beleuchtet hatte.



Wieso eigentlich sollte man die Prinzipien und Methodiken von testgetriebener Softwareentwicklung nicht auch auf den Bereich der IT-Systemadministration Ã¼bertragen? Also in aller KÃ¼rze: Ich definiere Tests, die das vom noch zu implementierenden [...]]]></summary>
		<content type="html" xml:base="/2010/09/01/testgetriebene-administration-test-driven-administration/"><![CDATA[<p>
Ich hatte tatsÃ¤chlich einmal eine ganz eigene Idee. Und sie war gut, auch nachdem ich sie mehrmals durchgekaut und von allen Seiten beleuchtet hatte.
</p>

<p>
Wieso eigentlich sollte man die Prinzipien und Methodiken von testgetriebener Softwareentwicklung nicht auch auf den Bereich der IT-Systemadministration Ã¼bertragen? Also in aller KÃ¼rze: Ich definiere Tests, die das vom noch zu implementierenden System erwartete Verhalten prÃ¼fen, sehe zu wie diese Tests fehlschlagen, und erfÃ¼lle dann schrittweise diese Tests, indem ich das System aufbaue. Test driven administration &#8211; TDA.
</p>

<p>
Da war ich ganz alleine drauf gekommen, und ich war sehr stolz.
</p>

<p>
Dann habe ich gegoogelt. Die Idee existiert seit mindestens 2006.
</p>

<p>
Aber hey, gut ist die Idee trotzdem, also beschreibe ich sie hier.
</p>

<p>
Warum mÃ¶chte man testgetrieben administrieren? Die GrÃ¼nde sind dieselben wie bei testgetriebener Entwicklung: Habe ich Tests, bin ich gegen Regression geschÃ¼tzt, d.h. Ã¤ndert ein StÃ¼ck Code / ein System sein Verhalten aufgrund von Ã„nderungen, weisen mich die Tests darauf hin.
</p>

<p>
Gehe ich test<em>getrieben</em> vor, sind die Tests nicht irgendwas, das ich ganz unbedingt machen sollte, das aber doch am Ende runterfÃ¤llt, sondern sie sind garantiert vorhanden. Mit den bekannten angenehmen Begleiterscheinungen, dass die Tests einen zwingen, sich Gedanken darÃ¼ber zu machen, wie das Ziel eigentlich beschaffen sein soll, und automatisch dazu fÃ¼hren, die LÃ¶sung schlank und elegant umzusetzen.
</p>

<p>
Code und IT-Systeme sind aber nicht dasselbe, wie wÃ¼rde man also in der Praxis konkret vorgehen? Hier mein Vorschlag.
</p>

<p>
Zuerst benÃ¶tigt man ein Testwerkzeug. Um in der Softwareentwicklung Unittests zu bauen, benutzt man Tools aus der xUnit Familie wie JUnit oder phpUnit. Das Ã„quivalent zu diesen Tools in der Systemadministration sind Monitoringsysteme wie Nagios oder Zabbix.
</p>

<p>
In der Softwareentwicklung formuliert man Unittest so, dass man eine kleine Einheit des Gesamtsystems, also in der Regel die einzelnen Methoden einer Klasse, mit einer gewissen Erwartungshaltung (&#8220;wenn ich diese Parameter reingebe, erwarte ich jenen RÃ¼ckgabewert&#8221;) aufruft, und dann die erwartete RÃ¼ckgabe mit der tatsÃ¤chlichen vergleicht.
</p>

<p>
Was wÃ¤re dementsprechend &#8220;erwartetes Verhalten&#8221; bei einem IT-System? Nehmen wir an, die Anforderungen lauten wie folgt:
</p>

<blockquote>
BenÃ¶tigt wird ein Linux-System, welches unter der IP 123.456.789.000 einen Webserver bereitstellt, und die FestplattengrÃ¶ÃŸe des Systems soll 100 GB betragen.
</blockquote>

<p>
In der RealitÃ¤t wÃ¤ren die Anforderungen natÃ¼rlich umfangreicher, aber ich halte das Beispiel einfach.
</p>

<p>
Aus den Anforderungen lÃ¤sst sich das gewÃ¼nschte Verhalten ableiten:

<ul>
	<li>Bei einem Ping auf 123.456.789.000 muss eine Antwort erfolgen</li>
	<li>Die Abfrage des Betriebssystems unter dieser IP muss &#8220;Linux&#8221; ergeben</li>
	<li>Ein HTTP Request gegen diese IP unter Port 80 muss eine HTTP Antwort zur Folge haben</li>
	<li>Bei der Abfrage der FestplattengrÃ¶ÃŸe muss ein Wert von 100 GB zurÃ¼ckgeliefert werden</li>
</ul>
</p>

<p>
Daraus wiederum kann man im Monitoringsystem Tests formulieren. Diese lÃ¤sst man einmalig laufen, um zu verifizieren, dass sie tatsÃ¤chlich fehlschlagen. Und dann beginnt man damit, ein System aufzusetzen, das die Testbedingungen erfÃ¼llt, bis schliesslich alle Tests &#8220;grÃ¼n&#8221; sind.
</p>

<p>
Das ist der Kern der Idee. Im weiteren Verlauf Ã¼berwacht man die Tests regelmÃ¤ÃŸig (was man mit einem Monitoringsystem ja eh tut), und hat damit das Thema Continuous Integration gleich mit erschlagen. Ansonsten geht man genauso wie auch beim TDD vor: MÃ¶chte man Ã„nderungen an einem System vornehmen, passt man zuerst die Tests an, verifiziert dass sie fehlschlagen, und Ã¤ndert dann das System, um die Tests wieder zu erfÃ¼llen.
</p>
</p>]]></content>
		<link rel="replies" type="text/html" href="/2010/09/01/testgetriebene-administration-test-driven-administration/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/09/01/testgetriebene-administration-test-driven-administration/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[&lt;angular/&gt; &#8211; ein radikal neuer Weg, Ajax Applikationen zu schreiben]]></title>
		<link rel="alternate" type="text/html" href="/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/" />
		<id>http://172.16.111.147/wordpress/?p=158</id>
		<updated>2012-01-18T11:41:44Z</updated>
		<published>2010-08-25T09:21:13Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[<em>&#60;angular/&#62;</em> bringt JavaScript-Logik und das dazugehÃ¶rige HTML Dokument deutlich nÃ¤her zueinander als bestehende Frameworks wie beispielsweise <em>jQuery</em>. Es entfernt gleich mehrere Ebenen an Abstraktion, die ein StÃ¼ck JavaScript-Code und das DOM-Element, auf welchem der Code operieren mÃ¶chte, voneinander trennen.]]></summary>
		<content type="html" xml:base="/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/"><![CDATA[<p>
JavaScript, Ajax und DHTML sind nicht wirklich meine Welt. Zum einen, weil ich einfach grundsÃ¤tzlich eher mit dem Backend einer Software als mit dem Frontend zu tun habe, zum anderen, weil ich immer schon das ungute GefÃ¼hl hatte, in diesem Bereich muss man einfach deutlich zu viel Code produzieren um damit dann gefÃ¼hlt deutlich zu wenig zu erreichen.
</p>

<p>
Umso mehr hat <em>&lt;angular/&gt;</em> mein Interesse geweckt. Die Autoren versprechen:
</p>

<blockquote>Write less code. A lot less. Forget about writing all that extra JavaScript to handle event listeners, DOM updates, formatters, and input validators. <angular/> comes with autobinding and built-in validators and formatters which take care of these. And you can extend or replace these services at will. With these and other services, youâ€™ll write about 10x less code than writing your app without <angular/>.</blockquote>

<p>
In einem Video versucht MiÅ¡ko Hevery zu erklÃ¤ren, was <em>&lt;angular/&gt;</em> eigentlich ist, und stellt fest dass diese ErklÃ¤rung schwierig ist:
</p>

<p align="center">
<object width="270" height="176"><param name="movie" value="http://www.youtube.com/v/0iQCLlu1dko?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube.com/v/0iQCLlu1dko?fs=1&amp;hl=de_DE&amp;rel=0&amp;hd=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="270" height="176"></embed></object>
</p>

<p>
Mein VerstÃ¤ndnis ist in erster Linie: <em>&lt;angular/&gt;</em> bringt JavaScript-Logik und das dazugehÃ¶rige HTML Dokument deutlich nÃ¤her zueinander als bestehende Frameworks wie beispielsweise <em>jQuery</em>. Es entfernt gleich mehrere Ebenen an Abstraktion, die ein StÃ¼ck JavaScript-Code und das DOM-Element, auf welchem der Code operieren mÃ¶chte, voneinander trennen.
</p>

<p>
WÃ¤hrend man bei traditioneller JavaScript-Programmierung stets gezwungen ist, explizit das HTML Dokument mit JavaScript-Code zu manipulieren, macht <em>&lt;angular/&gt;</em> die Verbindung zwischen Logik und HTML-ReprÃ¤sentation implizit &#8211; etwas, das mich Ã¼brigens stark an die Mechanik erinnert, die Max Winde fÃ¼r <a href="http://172.16.111.147/wordpress/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/">siqqel</a> einsetzt.
</p>

<p>
Spielen wir ein einfaches Beispiel durch, welches ich dank der rein clientseitigen Arbeitsweise von <em>&lt;angular/&gt;</em> problemlos direkt hier im Post zum laufen bringen kann.
</p>

Zuerst binde ich die <em>&lt;angular/&gt;</em> JavaScript Bibliothek ein:

<code>&lt;script type="text/javascript"
 src="http://angularjs.org/ng/js/angular-debug.js" ng:autobind&gt;
&lt;/script&gt;
</code>

Nun definiere ich ein Input Feld sowie einen <em>&lt;angular/&gt;</em>-Platzhalter, welche beide in einer Beziehung zueinander stehen:

<code>Dein Name: &lt;input type="text" name="deinname" value="Manuel"/&gt;
&lt;br /&gt;
Hallo {<span class="nospace" />{deinname}<span class="nospace" />}!
</code>

<p>
Wodurch entsteht diese Beziehung? Sie ist dank Autobinding implizit, und mappt alleine aufgrund des Formularfeldnamens und des Platzhalternamens beide zusammen.
</p>

<p>
Das Ergebnis sieht man hier &#8211; einfach den Inhalt des Textfeldes Ã¤ndern:
</p>

<p>
<script type="text/javascript" src="http://angularjs.org/ng/js/angular-debug.js" ng:autobind>
</script>

Eingabe: <input type="text" name="yourname" value="Welt"/> &#8211; Hallo {{yourname}}!
</p>

<p>
Dieses Beispiel geht natÃ¼rlich maximal als Spielerei durch. Es zeigt aber schon, wieviel weniger Code nÃ¶tig ist, als dies mit einem klassischen Framework der Fall wÃ¤re.
</p>

Ein etwas praxisnÃ¤heres Beispiel findet man unter <a href="http://angularjs.org/Cookbook:BasicForm">http://angularjs.org/Cookbook:BasicForm</a>. In diesem Beispiel geht es um den klassischen Fall, Eingaben in ein Textfeld per JavaScript clientseitig zu validieren &#8211; dies ist einfach mÃ¶glich durch folgende schlanke und ausdrucksstarke Syntax:

<code>&lt;input type="text" name="user.address.state" size="2"
 ng:required ng:validate="regexp:/^\w\w$/"/&gt;
</code>

FÃ¼r weitere Informationen verweise ich auf <a href="http://angularjs.org/Overview">http://angularjs.org/Overview</a>.]]></content>
		<link rel="replies" type="text/html" href="/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/08/25/angular-ein-radikal-neuer-weg-ajax-applikationen-zu-schreiben/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Tutorial: Testgetriebene Entwicklung mit PHP]]></title>
		<link rel="alternate" type="text/html" href="/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/" />
		<id>http://172.16.111.147/wordpress/?p=107</id>
		<updated>2012-03-19T01:07:13Z</updated>
		<published>2010-08-23T16:24:50Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[Testgetriebene Entwicklung (test driven development) ist eine Arbeitsmethodik, die Softwareentwickler dabei unterstÃ¼tzt, wichtige QualitÃ¤tsprinzipien bei der Erstellung von Code zu erreichen. Dieses Tutorial beschreibt Schritt fÃ¼r Schritt die Motivation, einem testgetriebenen Ansatz zu folgen, und stellt die notwendigen Werkzeuge und Techniken vor.]]></summary>
		<content type="html" xml:base="/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/"><![CDATA[<h3>Einleitung</h3>
Testgetriebene Entwicklung (test driven development) ist eine Arbeitsmethodik, die Softwareentwickler dabei unterstÃ¼tzt, wichtige QualitÃ¤tsprinzipien bei der Erstellung von Code zu befolgen:

<ul>
	<li><strong>Lose Kopplung (loose couping)</strong> &#8211; weil man beim Schreiben von Unittests, dem zentralen Werkzeug der Methodik, ganz automatisch dazu verfÃ¼hrt wird, innerhalb der Tests von Codeunits (Klassen, Methoden usw.) auszugehen, die mÃ¶glichst wenige AbhÃ¤ngigkeiten zu anderen Modulen haben &#8211; einfach deshalb, weil das Schreiben der Tests dann zu nervig wird.</li>
	<li><strong>Saubere Trennung von Verantwortlichkeiten (separation of concerns)</strong> &#8211; aus ganz Ã¤hnlichen GrÃ¼nden wie der erste Punkt: Jeder Test testet genau ein gewÃ¼nschtes Verhalten, und dies fÃ¼hrt ganz automatisch dazu, dass man spÃ¤ter den Code, der die Tests erfÃ¼llen muss, in sauber voneinander getrennte und logisch strukturierte Einheiten teilt.</li>
	<li><strong>Schlanke LÃ¶sungen</strong> &#8211; testgetrieben bedeutet eben auch, dass man von den Tests getrieben ist, im besten Sinne: Man tut alles, um einen noch fehlschlagenden Test zu erfÃ¼llen; aber eben auch nur genau das und nicht mehr. Salopp gesagt: Man programmiert nicht mehr &#8220;einfach rum&#8221;, sondern arbeitet Ã¤uÃŸerst zielgerichtet und erzeugt Code, der nur genau das tut was er tun muss, was ganz automatisch zu einer schlanken und damit eleganten LÃ¶sung fÃ¼hrt, in der sich zum Beispiel Bugs sehr viel schlechter verstecken kÃ¶nnen.</li>
</ul>

DarÃ¼ber hinaus hat der testgetriebene Ansatz weitere nÃ¼tzliche Nebeneffekte:

<ul>
	<li>Die im Laufe der Zeit aufgebaute Sammlung von Unittests kann man benutzen, um die mit Tests versehenen Units automatisiert immer wieder testen zu kÃ¶nnen, zum Beispiel um beim Mergen eines Entwicklungszweigs mit einem anderen Zweig (oder auch nach jedem einzelnen Commit in ein Versionskontrollsystem) sicherzustellen, dass sich alle Units auch nach der ZusammenfÃ¼hrung zweier Entwicklungslinien noch so verhalten wie erwartet. Das Stichwort fÃ¼r weiterfÃ¼hrende LektÃ¼re ist hier die <em>Kontinuierliche Integration</em> (continuous integration).</li>
	<li>Ein Unittest ist in der Praxis nicht nur ein StÃ¼ck Code, sondern immer auch Dokumentation des erwarteten Verhaltens eines Systems &#8211; zumindest in einer fÃ¼r Programmierer lesbaren Form. Um als Unbeteiligter ein StÃ¼ck Code oder ganze Teile eines Systems kennen zu lernen, ist es hÃ¤ufig effizienter, die dazugehÃ¶rigen Tests zu lesen, als den Code selbst.</li>
	<li>Hat man erst einmal die Tests komplett geschrieben, welche die noch zu erzeugenden Units testen sollen, ist es sehr einfach, die Arbeit am eigentlichen Code einfach mittendrin auch fÃ¼r lÃ¤ngere Zeit zu unterbrechen &#8211; die Tests geben einem sofort einen Anhaltspunkt, wo man &#8220;weiterprogrammieren&#8221; muss, selbst wenn man gedanklich lÃ¤ngst aus dem Thema war.</li>
	<li>Testgetrieben zu entwickeln, erzeugt ein gutes GefÃ¼hl. Das mag banal klingen, aber es ist ein realer und wichtiger Faktor. Irgendwo habe ich mal eine sehr gute Definition des Begriffs &#8220;legacy code&#8221; gelesen: &#8220;legacy code&#8221; ist Code, vor dem man sich fÃ¼rchtet &#8211; weil man nicht genau weiss was er tut, und deshalb Angst hat, ihn zu verÃ¤ndern. Testgetriebene Entwicklung ist die beste Vorsorge gegen legacy code &#8211; man weiss, es gibt eine Instanz die Ã¼berwacht und aussagt, was der Code tun soll. Es wÃ¤chst das Vertrauen in den eigenen Code und damit auch in die eigenen FÃ¤higkeiten.</li>
</ul>

Die Unterteilung in zentrale Effekte und Nebeneffekte ist subjektiv. Ich habe die ErhÃ¶hung der CodequalitÃ¤t an sich fÃ¼r mich als wichtiger erlebt als zum Beispiel die Tatsache, dank der sich entwickelnden Testsammlung Regressionstests durchfÃ¼hren zu kÃ¶nnen. Geschadet hat mir jedenfalls noch kein einziger durch testgetriebene Entwicklung entstandener Effekt.

<h3>Voraussetzungen</h3>
Was benÃ¶tigt man nun, um in PHP testgetrieben zu entwickeln? Im Wesentlichen vier Dinge:

<ul>
	<li>Eine <strong>Arbeitsmethodik</strong>, um effizient zu testgetrieben entwickeltem Code zu kommen</li>
	<li>Ein <strong>Organisationsprinzip</strong>, um Tests und zu testenden Code sinnvoll strukturieren zu kÃ¶nnen</li>
	<li>Ein PHP <strong>Framework</strong>, um TestfÃ¤lle schreiben zu kÃ¶nnen</li>
	<li>Ein <strong>Tool</strong>, um TestfÃ¤lle ausfÃ¼hren und auswerten zu kÃ¶nnen</li>
</ul>

Beginnen wir mit den letzten beiden Punkten, denn dank der MaÃŸstÃ¤be setzenden Arbeit von Sebastian Bergmann (<a href="http://sebastian-bergmann.de/">http://sebastian-bergmann.de/</a>) existiert ein Softwareprojekt, welches beide Anforderungen hervorragend erfÃ¼llt und lÃ¤ngst der de-facto Standard fÃ¼r Unittesting unter PHP ist: PHPUnit.

<p />

Unter <a href="http://www.phpunit.de/manual/current/en/installation.html">http://www.phpunit.de/manual/current/en/installation.html</a> befindet sich eine ausfÃ¼hrliche Anleitung fÃ¼r die in der Regel sehr einfache Installation.

<p />

PHPUnit ist sowohl ein Framework aus PHP Klassen, die es erlauben, Unittests fÃ¼r den eigenen PHP Code zu schreiben, als auch Kommandzeilen-Werkzeug, um die eigenen Tests auszufÃ¼hren und in verschiedenen Formaten die Testergebnisse darzustellen.

<p />

Im weiteren Verlauf des Tutorials gehe ich davon aus, dass PHPUnit installiert und funktionsfÃ¤hig ist.

<p />

Im Mittelpunkt von testgetriebener Entwicklung stehen aber nicht die Werkzeuge, sondern der Arbeitsprozess. Dieser folgt stets diesem Muster:
<ul>
	<li>Schreiben des Tests fÃ¼r eine neu zu implementierende FunktionalitÃ¤t</li>
	<li>ErfÃ¼llen des Tests mit so wenig Aufwand wie mÃ¶glich, so dass dieser fehlerfrei durchlÃ¤uft</li>
	<li>Ãœberarbeiten des Codes, der den Test erfÃ¼llt, so dass dieser keine Duplizierungen enthÃ¤lt, sauber abstrahiert ist, und dem eigenen Code-Style entspricht &#8211; und dabei immer noch den Test erfÃ¼llt</li>

</ul>
Diese Schritte werden immer wieder wiederholt, bis man keine neuen sinnvollen Tests mehr findet fÃ¼r die neue FunktionalitÃ¤t.

<p />

MÃ¶chte man bereits vorhandene FunktionalitÃ¤t Ã¤ndern, die bereits mit Tests versehen ist, bedeutet testgetriebene Entwicklung, dass man zuerst die Tests Ã¤ndert, um das neue erwartete Verhalten widerzuspiegeln, sicherstellt, dass die verÃ¤nderten Tests fehlschlagen, und dann erst den Code anpasst, um die verÃ¤nderten Tests wieder zu erfÃ¼llen.

<p />

WÃ¤re noch die Frage der Testorganisation zu klÃ¤ren &#8211; einfacher ausgedrÃ¼ckt: Wohin mit den Tests? Meiner Meinung nach ist der einzig wirklich sinnvolle Ansatz, Code und Tests identisch zu strukturieren. Das bedeutet, der Test fÃ¼r die Klasse <em>DefaultUser</em> in

<code>lib/core/user/default_user.php</code>

sollte in der Datei

<code>tests/core/user/default_user_test.php</code>

in der Testklasse <em>DefaultUserTest</em> liegen.

<p />

Aber solange wir noch kein Beispiel fÃ¼r einen Unittest durchgespielt haben, bleibt vieles sehr abstrakt, also beginnen wir den praktischen Teil des Tutorials.

<h3>Ein erstes Beispiel</h3>
Angenommen, wir mÃ¶chten mithilfe von PHP ein Forum programmieren. Auf die ein oder andere Art und Weise wird diese Software eine Unit enthalten mÃ¼ssen, die eine E-Mail Adresse auf GÃ¼ltigkeit prÃ¼ft. Wir haben also eine Erwartungshaltung, was der Code spÃ¤ter einmal tun soll. Der Einfachheit halber definieren wir diese Erwartungshaltung in diesem Beispiel so:

<p />
<em>Wenn eine E-Mail Adresse ohne @-Zeichen Ã¼bergeben wird, dann liefere mir FALSE zurÃ¼ck, sonst TRUE</em>
<p />

Diese Erwartungshaltung gieÃŸen wir nun in Form von PHP Code in einen Unittest. Da wir testgetrieben arbeiten, existiert noch keinerlei Code der diese Erwartungen erfÃ¼llen kÃ¶nnte.

<p />

Wir geben der Unit, die spÃ¤ter einmal unsere formulierte Erwartung erfÃ¼llen soll, den Namen <em>Verify</em>. Daraus leitet sich als Klassenname fÃ¼r den Unittest die Bezeichnung <em>VerifyTest</em> ab.

<p />

Wir erzeugen daher folgende Datei:

<p />

<em>tests/verify_test.php</em>

<p />

Und fÃ¼llen sie mit folgendem GrundgerÃ¼st:

<code>&lt;?php
require_once('/usr/lib/php/PHPUnit/Framework.php');

class VerifyTest extends PHPUnit_Framework_TestCase {}
</code>

Dieser Code reprÃ¤sentiert einen Testcase, der noch keine Tests enthÃ¤lt. Wir inkludieren das PHP-Klassen Framework von PHPUnit, da wir unsere Testcase-Klassen von einer Klasse dieses Frameworks ableiten mÃ¼ssen. Je nach Plattform liegt die zu inkludierende Framework.php auch schon mal unter <em>/usr/share/php/PHPUnit/Framework.php</em>.

<p />

Den Testcase selbst formulieren wir, indem wir eine Klasse definieren, deren Name auf <em>Test</em> endet, und die von <em>PHPUnit_Framework_TestCase</em> erbt.

<p />

Dieser Testcase kann nun mithilfe des PHPUnit Kommandozeilentools ausgefÃ¼hrt werden. Dazu starten wir folgenden Befehl an der Kommandozeile:

<p />

<em>phpunit tests/verify_test.php</em>

<p />

Dadurch erhalten wir die folgende Ausgabe:

<code>PHPUnit 3.4.13 by Sebastian Bergmann.

F

Time: 0 seconds, Memory: 7.25Mb

There was 1 failure:

1) Warning
No tests found in class "VerifyTest".
</code>

PHPUnit wertet den Testlauf als nicht erfolgreich (&#8220;Failure&#8221;), da keinerlei Tests innerhalb des Testcases gefunden wurden. Als nÃ¤chstes fÃ¼gen wir daher einen Test hinzu:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

}
</code>

Einen Test innerhalb eines Testcase formuliert man, indem man der Testcase-Klasse eine Methode hinzufÃ¼gt, deren Name mit <em>test</em> beginnt.
<br />
Innerhalb der Methode schreibt man nun den Code, der notwendig ist, um den oder die Werte von der zu testenden Unit zu bekommen, mithilfe derer man das erwartete Verhalten verifizieren kann.
<br />
Die von der Unit erhaltenen Werte testet man nun gegen eine Behauptung, einen <em>assert</em>: Wir drÃ¼cken hier also aus, dass der Test erwartet, dass der zu testende Wert FALSE ist.

<p />

Letztendlich muss man sich aber immer bewusst machen: Man mÃ¶chte Verhalten testen, nicht Daten. Daten drÃ¼cken nur das Ergebnis eines Verhaltens aus. Entsprechen die tatsÃ¤chlichen (actual) Daten den erwarteten (expected) Daten, dann entspricht das tatsÃ¤chliche Verhalten dem im Test erwarteten.

<p />

Nun lassen wir den neu formulierten Testcase erneut durchlaufen, mit folgendem Ergebnis:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

PHP Fatal error:  Class 'Verify' not found in tests/verify_test.php on line 8
</code>

Wenig Ã¼berraschend beschwert sich PHP (nicht PHPUnit!), dass wir eine Klasse verwenden, die nirgends definiert wurde. Tun wir dies also, indem wir eine Datei <em>lib/verify.php</em> erzeugen und mit folgendem Inhalt fÃ¼llen:

<code>&lt; ?php

class Verify {}

</code>

Dann muss im Testcase noch sichergestellt werden, dass die Datei mit dieser Klasse auch inkludiert wird:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');
require_once('lib/verify.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

}
</code>

Lassen wir den Testcase nun laufen, Ã¤ndert sich das Bild:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

PHP Fatal error:  Call to undefined method Verify::checkEmail() in tests/verify_test.php on line 9
</code>

Wir rufen eine Methode auf, die noch nicht existiert, also muss diese implementiert werden:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {}

}
</code>

Nun steht zumindest die Codestruktur komplett, so dass PHPUnit ohne Fatals durchlaufen kann:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

F

Time: 0 seconds, Memory: 7.00Mb

There was 1 failure:

1) VerifyTest::test_falseIfNoAtSign
Failed asserting that  is false.

tests/verify_test.php:10

FAILURES!
Tests: 1, Assertions: 1, Failures: 1.
</code>

Eine Zwischenbemerkung: Das Vorgehen ist hier natÃ¼rlich sehr kleinschrittig &#8211; ob man die offensichtlichen Dinge wie das Anlegen der benÃ¶tigten Klassen und Methoden nicht gleich in einem Rutsch macht, bleibt Geschmackssache. Ich persÃ¶nlich habe Gefallen gefunden an dem Vorgehen, meine ganze Energie in die Tests zu stecken, und dann in einen anderen Modus zu schalten und ganz stupide Schritt fÃ¼r Schritt immer wieder die Implementierung anzupassen und den Testlauf neu zu starten, bis keinerlei Fehler mehr auftreten.

<p />

Wie auch immer, PHPUnit lÃ¤uft nun wieder ohne PHP Fehler durch, bestÃ¤tigt aber wenig Ã¼berraschend, dass die nunmehr vorhandene Code-Unit nicht das Verhalten zeigt, welches wir laut Test von ihr erwarten. Wechseln wir nun also auf die inhaltliche Ebene der Implementierung und sorgen dafÃ¼r, dass unser Code sich wie gewÃ¼nscht verhÃ¤lt:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {
    if (!strstr($email, '@')) return FALSE;
  }

}
</code>

Nun besteht unser Testcase alle Tests:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

.

Time: 0 seconds, Memory: 7.00Mb

OK (1 test, 1 assertion)
</code>

Damit wÃ¤re der erste Testzyklus komplett. Stellt sich die Frage, ob uns noch weitere Verhaltensweisen fÃ¼r unsere Unit einfallen, die wir von ihr erwarten. Es liegt auf der Hand, dass wir den Positivfall ebenfalls testen wollen, nÃ¤mlich dass eine E-Mail Adresse mit @-Zeichen als valide erkannt wird. NatÃ¼rlich wÃ¼rde man in der RealitÃ¤t noch viel mehr AnsprÃ¼che an die Validierung einer E-Mail Adresse stellen, aber in diesem Beispiel bleibe ich der Einfachheit halber unrealistisch.

<p />

Eine Faustregel der testgetriebenen Entwicklung lautet, immer nur ein Verhalten pro Test zu Ã¼berprÃ¼fen, anders ausgedrÃ¼ckt &#8220;ein assert pro Test&#8221;. Dies hilft, die einzelnen Tests Ã¼bersichtlich und nachvollziehbar zu halten, und hat auch ganz praktischen Nutzen, da PHPUnit bei der Ausgabe eines Failures innerhalb eines Tests nicht darauf hinweist, welcher assert genau nicht erfÃ¼llt wurde, sondern immer den gesamten Test als fehlgeschlagen zu melden &#8211; hat man einen Test mit 20 asserts geschrieben, wird die Fehlersuche aufwendig.

<p />

Formulieren wir also einen weiteren Test:

<code>&lt;?php

require_once('/usr/lib/php/PHPUnit/Framework.php');
require_once('lib/verify.php');

class VerifyTest extends PHPUnit_Framework_TestCase {

  public function test_falseIfNoAtSign() {
    $actual = Verify::checkEmail('manuel.kiessling.net');
    $this-&gt;assertFalse($actual);
  }

  public function test_trueIfAtSign() {
    $actual = Verify::checkEmail('manuel@kiessling.net');
    $this-&gt;assertTrue($actual);
  }

}
</code>

Danach sollte man allerdings, obwohl kleinschrittig, auf jeden Fall den Testcase einmal durchlaufen lassen und ihm beim Fehlschlagen zusehen: Auch beim Schreiben von Tests kÃ¶nnen Fehler passieren, und es kommt vor, dass man einen neuen Test formuliert, der wegen eines Fehlers in der Implementation oder im Test sofort erfÃ¼llt wird &#8211; geht man nach dem Schreiben des Tests sofort an die Implementation, ohne zuvor den Test einmal fehlschlagen gesehen zu haben, Ã¼bersieht man mÃ¶glicherweise einen Bug in der Implementation oder im Test, wenn man erst dann den Test laufen lÃ¤sst und dieser dann ohne Fehler durchlÃ¤uft.
<br />
Dann sorgt gar nicht die eigene Ã„nderung an der Implementation fÃ¼r das funktionieren des Tests, sondern ein Bug, den man aber eben nicht bemerkt.

<p />

Also stellen wir sicher, dass unser neuer Test fehlschlÃ¤gt:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

.F

Time: 0 seconds, Memory: 7.00Mb

There was 1 failure:

1) VerifyTest::test_trueIfAtSign
Failed asserting that  is true.

tests/verify_test.php:15

FAILURES!
Tests: 2, Assertions: 2, Failures: 1.
</code>

Und nun Ã¤ndern wir die Implementation, um ihn zu erfÃ¼llen:

<code>&lt;?php

class Verify {

  public static function checkEmail($email) {
    if (!strstr($email, '@')) return FALSE;
    return TRUE;
  }

}
</code>

Nun laufen beide Tests im Testcase erfolgreich durch:

<code>bash$ phpunit tests/verify_test.php
PHPUnit 3.4.13 by Sebastian Bergmann.

..

Time: 0 seconds, Memory: 7.00Mb

OK (2 tests, 2 assertions)
</code>

Das hier beschriebene Beispiel ist natÃ¼rlich banal, aber im Grunde ist alles Wichtige zur Methodik der testgetriebenen Entwicklung gesagt.

<p />

Aber auch in der eigenen Praxis, auch bei spannenden Projekten, wird man aber immer wieder dem GefÃ¼hl begegnen, dass der einzelne Test im Grunde trivial ist. Aber das ist auch vÃ¶llig in Ordnung: Selbst komplexeste Softwareprojekte sind letztendlich die VerknÃ¼pfung kleiner und fÃ¼r sich betrachtet trivialer Funktionseinheiten &#8211; aber aus dem Zusammenspiel dieser vielen einfachen Module ergibt sich die LÃ¶sung komplexer Probleme fÃ¼r den Anwender.]]></content>
		<link rel="replies" type="text/html" href="/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/08/23/tutorial-testgetriebene-entwicklung-mit-php/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Empfehlung: Barbecue Sauce &#8220;Bone Suckin&#8217; regular thicker style&#8221; von Ford&#8217;s Food]]></title>
		<link rel="alternate" type="text/html" href="/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/" />
		<id>http://172.16.111.147/wordpress/?p=98</id>
		<updated>2010-08-24T13:53:48Z</updated>
		<published>2010-07-13T11:17:46Z</published>
		<category scheme="/" term="Recommendations" />		<summary type="html"><![CDATA[Phil Ford war Immobilienmakler und hat aus einem Hobby heraus angefangen, diese Sauce zu entwickeln.]]></summary>
		<content type="html" xml:base="/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/"><![CDATA[Klar, Ã¼ber Geschmack lÃ¤sst sich immer streiten, aber Ã¼ber den Geschmack von Barbecue Saucen wahrscheinlich am meisten.
<p/>
Die beste Sauce, die zu probieren ich bisher das VergnÃ¼gen hatte, ist jedenfalls vÃ¶llig zweifelsfrei die &#8220;Bone Suckin&#8217; Sauce regular&#8221; von Ford&#8217;s Food.
<p/>
Phil Ford war Immobilienmakler und hat aus einem Hobby heraus angefangen, diese Sauce zu entwickeln. Sie hat nur wenig Raucharoma und ist nicht scharf, im Vordergrund steht vor allem ein fantastisches Tomatenaroma und eine gewisse SÃ¼ÃŸe, die es aber auch nicht Ã¼bertreibt. Man muss sich sehr beherrschen, sie nicht direkt auszulÃ¶ffeln.
<p/>
Die offizielle Homepage zur Sauce ist <a href="http://www.bonesuckin.com/">www.bonesuckin.com</a>, zu beziehen ist sie in Deutschland zum Beispiel Ã¼ber BOS FOOD in Meerbusch:
<p/>
<a href="https://www.bosfood.de/Bone_Suckin%B4_Sauce_regular_Barbecue_Sauce_dickfluessig_Ford%B4s_Food_454_g_11;20504;bone+suckin%2C%2C;0.html">Bone SuckinÂ´ Sauce regular, Barbecue-Sauce (dickflÃ¼ssig), FordÂ´s Food, 454 g</a> bei bosfood.de.]]></content>
		<link rel="replies" type="text/html" href="/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/07/13/empfehlung-barbeque-soss-bone-suckin-sauce-regular/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Empfehlung: &#8220;Der Kuchenladen&#8221; in Berlin]]></title>
		<link rel="alternate" type="text/html" href="/2010/07/13/empfehlung-der-kuchenladen-in-berlin/" />
		<id>http://172.16.111.147/wordpress/?p=89</id>
		<updated>2010-08-24T13:32:02Z</updated>
		<published>2010-07-13T11:08:46Z</published>
		<category scheme="/" term="Recommendations" />		<summary type="html"><![CDATA[Die Konditorei "Der Kuchenladen" ist sogar die nervige Parkplatzsuche auf der KantstraÃŸe wert. Handgemachte Torten, Kuchen und Tarts, die klasse aussehen und einfach gut schmecken.]]></summary>
		<content type="html" xml:base="/2010/07/13/empfehlung-der-kuchenladen-in-berlin/"><![CDATA[Die Konditorei &#8220;Der Kuchenladen&#8221; ist sogar die nervige Parkplatzsuche auf der KantstraÃŸe wert. Handgemachte Torten, Kuchen und Tarts, die klasse aussehen und einfach gut schmecken.
<p />
Das Wichtigste: Auf Ã¼bertriebene effekthascherische ZuckerguÃŸ-UngetÃ¼me wird verzichtet &#8211; die Torten zeichnet neben der spÃ¼rbaren handwerklichen QualitÃ¤t vor allem aus, dass sie nicht zu sÃ¼ÃŸ sind.
<p />
Als Schwiegersohn einer Konditoreimeisterin bin ich sehr verwÃ¶hnt, aber der Kuchenladen hat mich noch nie enttÃ¤uscht.
<p />
<address>
Der Kuchenladen<br/>
KantstraÃŸe 138<br/>
10623 Berlin<br/>
Telefon: 030 / 310 184 24<br/>
E-Mail: uwe_gundelach@yahoo.de<br/>
<a href="http://der-kuchenladen.com/">http://der-kuchenladen.com/</a><br/>
</address>
]]></content>
		<link rel="replies" type="text/html" href="/2010/07/13/empfehlung-der-kuchenladen-in-berlin/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/07/13/empfehlung-der-kuchenladen-in-berlin/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Project: PHPRestfulSubversion]]></title>
		<link rel="alternate" type="text/html" href="/2010/05/21/project-phprestfulsubversion/" />
		<id>http://172.16.233.129/wordpress/?p=517</id>
		<updated>2012-03-19T00:24:15Z</updated>
		<published>2010-05-21T00:14:15Z</published>
		<category scheme="/" term="English articles only" /><category scheme="/" term="Projects" />		<summary type="html"><![CDATA[
  PHPRestfulSubversion&#8230;
  
    &#8230;provides a RESTful JSON webservice API to access information in your Subversion repository,
    &#8230;provides tools to cache your Subversion repository in order to make it searchable through the webservice in a fast and simple manner,
    &#8230;is a library of PHP classes [...]]]></summary>
		<content type="html" xml:base="/2010/05/21/project-phprestfulsubversion/"><![CDATA[<p>
  <a href="https://github.com/ManuelKiessling/PHPRestfulSubversion">PHPRestfulSubversion</a>&#8230;
  <ol>
    <li>&#8230;provides a RESTful JSON webservice API to access information in your Subversion repository,</li>
    <li>&#8230;provides tools to cache your Subversion repository in order to make it searchable through the webservice in a fast and simple manner,</li>
    <li>&#8230;is a library of PHP classes which you can use to implement more complex use cases.</li>
  </ol>
</p>
<p>
  A secondary goal of this project is to explore how to create very clean PHP code using a 100% test-driven approach.
</p>
<p>
  The source code of this project is hosted at <a href="https://github.com/ManuelKiessling/PHPRestfulSubversion">https://github.com/ManuelKiessling/PHPRestfulSubversion</a>
</p>
]]></content>
		<link rel="replies" type="text/html" href="/2010/05/21/project-phprestfulsubversion/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/05/21/project-phprestfulsubversion/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Alte Homepage wieder verfÃ¼gbar]]></title>
		<link rel="alternate" type="text/html" href="/2010/04/30/alte-homepage-wieder-verfugbar/" />
		<id>http://172.16.111.147/wordpress/?p=26</id>
		<updated>2010-07-08T22:08:02Z</updated>
		<published>2010-04-30T12:36:52Z</published>
		<category scheme="/" term="Other" />		<summary type="html"><![CDATA[Meine alte Homepage (2000-2005) ist wiederauferstanden und unter <a href="http://old.manuel.kiessling.net/">http://old.manuel.kiessling.net/</a> erreichbar.]]></summary>
		<content type="html" xml:base="/2010/04/30/alte-homepage-wieder-verfugbar/"><![CDATA[Meine alte Homepage (2000-2005) ist wiederauferstanden und unter <a href="http://old.manuel.kiessling.net/">http://old.manuel.kiessling.net/</a> erreichbar.]]></content>
		<link rel="replies" type="text/html" href="/2010/04/30/alte-homepage-wieder-verfugbar/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/04/30/alte-homepage-wieder-verfugbar/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[siqqel: SQL-Abfragen direkt aus HTML heraus ausfÃ¼hren und darstellen]]></title>
		<link rel="alternate" type="text/html" href="/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/" />
		<id>http://localhost/wordpress/?p=6</id>
		<updated>2010-07-09T15:22:55Z</updated>
		<published>2010-04-07T23:15:47Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[Ein Kollege von mir, Max Winde, hat in den vergangenen Wochen ein Tool geschrieben welches sich innerhalb kÃ¼rzester Zeit zu einem Renner in den verschiedensten Abteilungen entwickelt hat, und schon jetzt aus dem Arbeitsalltag kaum noch wegzudenken ist.]]></summary>
		<content type="html" xml:base="/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/"><![CDATA[Ein Kollege von mir, Max Winde, hat in den vergangenen Wochen ein Tool geschrieben welches sich innerhalb kÃ¼rzester Zeit zu einem Renner in den verschiedensten Abteilungen entwickelt hat, und schon jetzt aus dem Arbeitsalltag kaum noch wegzudenken ist: <strong>siqqel</strong>.
<p />
<h3>Welchen Zweck erfÃ¼llt siqqel?</h3>
<p />
Die verschiedensten Leute in einem Unternehmen mÃ¼ssen aus den verschiedensten GrÃ¼nden auf relationale Datenbanken zugreifen. Klassischerweise gibt es zwei Szenarien:
<p />
<ul>
<li><strong>Ich brauche eine einfache und kurze Information</strong></li>
<li>Beispiel: &#8220;Wie war noch gleich der &#8216;name&#8217; des &#8216;product&#8217; mit der Id 12345?&#8221; oder &#8220;Wieviele EintrÃ¤ge waren doch gleich in der &#8216;city&#8217; Tabelle?&#8221;</li>
</ul>
<p />
Ãœblicherweise schmeisst man dafÃ¼r direkt die SQL Kommandozeile an, oder man benutzt ein Tool wie Toad, phpMyAdmin, oder irgend einen anderen Query-Browser.
<p />
<ul>
<li><strong>Ich benÃ¶tige eine komplexe Auswertung wichtiger Kennzahlen, inklusive historischer Betrachtung und Querverweisen, und diese brauche ich langfristig und regelmÃ¤ÃŸig</strong></li>
<li>Beispiel: &#8220;Wir mÃ¼ssen die Conversions unserer User auswerten&#8221; oder &#8220;Ich brauche eine tÃ¤glich aktualisierte Auswertung unserer ProduktverkÃ¤ufe&#8221;</li>
</ul>
<p />
Ãœblicherweise werden hierfÃ¼r komplexe und spezialisierte Enterprise-Tools wie Data Warehouses benutzt und manchmal auch selbst implementiert.
<p />
Das ist auch alles fein, und die Tools fÃ¼r beide Szenarien sind vielfÃ¤ltig und ausgereift. In der Praxis gibt es aber ein weiteres Szenario, welches sozusagen &#8220;dazwischen&#8221; liegt: Hier ein paar Beispiele:
<p />
<ul>
<li>Die QA Abteilung soll einem Bug nachspÃ¼ren und muss dafÃ¼r Ã¼ber einen Zeitraum von einigen Tagen einige mittelkomplexe Datenanalysen fahren und diese regelmÃ¤ÃŸig aktualisieren (&#8220;zu welcher Tageszeit kommt es vor dass User aus Gruppe X auf Seite Y Aktion Z durchfÃ¼hren, und dann die Kombination der Daten aus Tabelle A, B, und C gleich D ergibt?&#8221;)</li>
<li>Ein Produktmanager soll ein neues Feature konzeptionieren und benÃ¶tigt dafÃ¼r Ã¼ber einen sehr begrenzten Zeitraum eine Auswertung Ã¼ber verschiedene Business-Kennzahlen. Da die Analyse auf ganz neuen Annahmen beruht, helfen die im Data Warehouse vorhandenen Reports nicht weiter.</li>
<li>Ein Softwareentwickler arbeitet an der Anbindung eines externen Webservice, und mÃ¶chte wÃ¤hrend der Implementations- und Testphase alle Tabellen und die zusammengehÃ¶renden Daten, die aus Webservice-Calls resultieren, im Blick haben, ohne jedes Mal 30 einzelne Queries abfeuern und miteinander in Verbindung bringen zu mÃ¼ssen.</li>
<li>Ein Business Analyst soll einen grÃ¶ÃŸeren Report vorbereiten, mÃ¶chte aber erst mal ein GefÃ¼hl dafÃ¼r bekommen welche Daten er benÃ¶tigt und wie er diese sinnvoll miteinander verknÃ¼pfen kann.</li>
</ul>
<p />
Alle diese Beispiele haben eines gemeinsam: Die &#8220;kleine&#8221; LÃ¶sung, direkt einzelne Queries nacheinander an die DB zu schicken und die Ergebnisse dann hÃ¤ndisch zusammenzutragen und miteinander in Verbindung zu bringen, ist <em>zu</em> klein, damit zu anstrengend und ineffektiv. Man kennt das, man fÃ¤ngt dann an sich die Queries in irgendein Textfile zu pasten damit sie nicht verloren gehen, oder man hat in Tools wie phpmyadmin plÃ¶tzlich 15 Browsertabs auf und wird langsam wahnsinnig.
<p />
Die &#8220;groÃŸe&#8221; LÃ¶sung ist aber wiederum <em>zu</em> groÃŸ: Â Es lohnt in der Regel nicht, einen Business-Analysten mehrere Stunden oder Wochen mit dem Bau eines Reports aus dem Data Warehouse zu beauftragen, nur weil man wenige Tage lang etwas beobachten oder nur vorÃ¼bergehend Daten debuggen muss.
<p />
Der Kompromiss sieht dann hÃ¤ufig so aus, dass man anfÃ¤ngt eine Zwischen-NotlÃ¶sung auf irgend einer Insel zu bauen: Man fÃ¤ngt an, mit irgendwelchen ODBC Kontrukten und Excel. Schick mir so eine Excel Datei, und ich sehe nichts, denn ich habe ODBC gerade nicht richtig eingerichtet. Oder der Produktmanager, der seine temporÃ¤re, aber komplexe Auswertung braucht, bekommt eine virtuelle Maschine mit einer Basisinstallation von PHP, ein Developer gibt ihm einen Crash-Kurs in PHP-Entwicklung, und los geht das Gefrickel. Irgendwo fliegen dann diese Skripte rum, nach ein paar Monaten, wo sie vielleicht fÃ¼r eine neue, Ã¤hnliche Analyse noch mal nÃ¼tzlich gewesen wÃ¤ren, findet sie dann auch keiner mehr. Der PM schlÃ¤gt sich mit Programmierung rum, Sysops meckert zu Recht, dass sie jetzt auch noch diese Spielkiste managen mÃ¼ssen, alle sind unglÃ¼cklich, und irgendwo in der Ferne fÃ¤ngt ein kleines Kind an zu weinen.
<p />
Das muss nicht sein!
<p />
Denn genau diese Nische zwischen &#8220;einfach mal ein Query&#8221; und &#8220;das groÃŸe bÃ¶se komplette Data Warehouse&#8221; besetzt <em>siqqel</em> exzellent, ohne die Probleme der frickeligen InsellÃ¶sungen einzufÃ¼hren.

<h3>Wie funktioniert siqqel?</h3>

Die MÃ¤chtigkeit von siqqel liegt darin, dass es den Applikationsstack, der benÃ¶tigt wird, um Anfragen an die Datenbank zu Ã¼bermitteln, die Antwort entgegenzunehmen und die empfangenen Daten darzustellen, auf etwas recht bekanntes und verbreitetes beschrÃ¤nkt: den Browser.
<p />
SQL Queries werden direkt in einer statischen HTML Datei notiert. Per Ajax werden diese an ein zentral abgelegtes Backend-Skript Ã¼bermittelt. Das Result Set wird an den Browser zurÃ¼ckgeliefert und direkt dort per DHTML dargestellt. Mit (D)HTML Bordmitteln, JavaScript und CSS kann man direkt innerhalb des HTML Dokuments dann beliebig flexibel mit den Result Sets arbeiten.

Richtig, ganz ohne PHP geht es nicht. Es braucht einen Punkt im Backend, welcher den SQL Query vom Browser entgegennimmt, an die DB Ã¼bermittelt, und das Result Set als JSON an den Browser zurÃ¼ckliefert. Aber man beachte die Vorteile zur vorhin beschriebenen InsellÃ¶sung:
<ul>
	<li>Das Skript wird einmalig an zentraler Stelle in der Serverlandschaft hinterlegt &#8211; zum Beispiel an dieselbe Location, an der bereits der phpMyAdmin lÃ¤uft; dann hat man vielleicht sogar gleich die Frage der Zugriffsrechte erschlagen, denn (Ã¼blicherweise) haben nur die richtigen Personen im Unternehmen Zugriff auf diese Ressource, und Sicherheitsmechanismen, die fÃ¼r die Zugriffsicherung des phpMyAdmin bereits implementiert wurden (wie .htaccess, SSL Public Keys etc.), greifen ohne zusÃ¤tzlich notwendige Handgriffe auch fÃ¼r das PHP Backend von siqqel.</li>
	<li>Nun kann jeder sofort anfangen, Reports auf Basis von siqqel zu bauen &#8211; alles was er braucht: Zugriffsrecht auf die HTTP Location des PHP Backend Skripts &#8211; und einen Browser!</li>
</ul>
<p />
Wie funktioniert das nun im Detail?
<p />
Angenommen, es gibt im Intranet eine MySQL Datenbank mit einer Tabelle, in der stehen alle Produkte des Unternehmens. Nennen wir sie &#8216;product&#8217;, und nehmen an sie befindet sich im Schema &#8216;data&#8217;. Nehmen wir weiterhin an, es gibt einen Server, auf dem wurde phpmyadmin installiert, damit man Ã¼ber diese Datenbank browsen kann. Dieses ist erreichbar unter unter <em>http://intranet/secure/phpmyadmin</em>. <em>/secure</em> ist der mit Zugriffsrechten versehene Teil des Servers.

Nun muss ein Systemadministrator die PHP Backendskripte unter <em>http://intranet/secure/siqqel/</em> hinterlegen, und die Konfiguration anpassen um dem siqqel PHP Code Zugriff auf die genannte Datenbank zu ermÃ¶glichen.

Ein siqqel User muss dann nur eine HTML Datei erzeugen (auf seinem Desktop oder wo auch immer, ein LAMP Kontext wird ja nicht benÃ¶tigt), die folgendes enthÃ¤lt:
<code>
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://intranet/secure/siqqel/siqqel.js.php&quot;&gt; &lt;/script&gt;

&lt;/head&gt;
&lt;body&gt;

&lt;table sql=&quot;SELECT * FROM data.product&quot;&gt;&lt;/table&gt;

&lt;/body&gt;
</code>

Ã–ffnet er diese Datei lokal in seinem Browser, wird das SQL Statement im Attribut der Table an das Backend Skript Ã¼bermittelt, das Result Set als JSON zurÃ¼ckgegeben, und der Inhalt der Datenbanktabelle automatisch in das table Element gerendert.

<p />

Von hier aus hat man alle MÃ¶glichkeiten: Man mÃ¶chte mehrere Tabellen auf einmal anzeigen? Man erzeugt einfach mehrere table Elemente mit den entsprechenden Queries. Man mÃ¶chte alle Zeilen im Result Set, bei denen die Spalte <em>name</em> mit &#8220;a&#8221; beginnt in der HTML Tabelle hervorheben? Kein Problem, jede Tabelle, Zeile und Spalte liefert ein &#8220;loaded&#8221; Event, also hat man mit einem JavaScript-Konstrukt wie

<code>
$('td.name').live('loaded', function(name) {
  // do something useful.
});
</code>

alle MÃ¶glichkeiten. Der Client Teil von siqqel basiert auf jQuery, also kann man schnell und einfach Reports bauen mit allen sinnvollen und sinnlosen MÃ¶glichkeiten, die jQuery bietet.

<p />

Was sind die weiteren Vorteile? Nun, die HTML Datei ist nicht nur der View des Reports, die HTML Datei <em>IST</em> der Report. Man kann ihn in die vielleicht vorhandenen Coderepositories im Unternehmen packen, man kann ihn per Mail verschicken, man, wenn die Wikisoftware es zulÃ¤sst, seine Reports sogar direkt nativ in eine Wikiseite packen und so besonders effizient mit den Kollegen im Unternehmen teilen.

Die Projektseite von siqqel ist <a href="http://github.com/MyHammerOpenSource/siqqel">http://github.com/MyHammerOpenSource/siqqel</a>. Nicht wundern, bis vor kurzem hieÃŸ das Projekt noch &#8220;sqlHammer&#8221;, der Begriff mag noch an verschiedenen Stellen auftauchen.

Bei Fragen zu siqqel empfehle ich, ein <a href="http://github.com/MyHammerOpenSource/siqqel/issues">Issue Ticket bei github zu Ã¶ffnen</a>, oder wendet euch an <a href="mailto:opensource@myhammer.com">opensource@myhammer.com</a>.]]></content>
		<link rel="replies" type="text/html" href="/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/04/08/siqqel-ein-sehr-nutzliches-tool-fur-entwickler-business-analysten-produktmanager-und-qaler/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Database Change Management mithilfe von VCS: Teil 1]]></title>
		<link rel="alternate" type="text/html" href="/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/" />
		<id>http://172.16.111.147/wordpress/?p=28</id>
		<updated>2010-07-13T10:29:39Z</updated>
		<published>2010-02-26T12:39:01Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[Dieses Dokument beschreibt Werkzeuge und Prozesse, um DatenbankÃ¤nderungen innerhalb von groÃŸen Softwareprojekten einfach, fehlerfrei und nachvollziehbar durchzufÃ¼hren und zu managen.]]></summary>
		<content type="html" xml:base="/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/"><![CDATA[<strong>Dieser Artikel ist Work in Progress!</strong>

<h3>VorÃ¼berlegungen</h3>
  
Dieses Dokument beschreibt Werkzeuge und Prozesse, um DatenbankÃ¤nderungen innerhalb von groÃŸen Softwareprojekten einfach, fehlerfrei und nachvollziehbar durchzufÃ¼hren und zu managen.

Zentraler Ansatz dieser LÃ¶sung ist: DatenbankÃ¤nderungen und CodeÃ¤nderungen sind <strong>prinzipiell genau dasselbe</strong>. Denn DatenbankÃ¤nderungen haben genau wie CodeÃ¤nderung die folgenden Eigenschaften:

<ul>
 <li>Sie Ã¤ndern das Verhalten des Softwaresystems</li>
 <li>Sie entwickeln sich verteilt in verschiedenen Projekten bzw. Branches, und mÃ¼ssen fÃ¼r Abnahme und Rollout/Release zusammengefÃ¼hrt werden</li>
 <li>Beim ZusammenfÃ¼hren kann es Ãœberschneidungen und Konflikte geben, die man mitbekommen und lÃ¶sen kÃ¶nnen mÃ¶chte</li>
 <li>Man mÃ¶chte sie auch spÃ¤ter noch nachvollziehen kÃ¶nnen, also sehen wer wann was gemacht hat</li>
 <li>Man mÃ¶chte diese Ã„nderungen ggf. einem Reviewprozess unterziehen</li>
</ul>

Wenn wir DatenbankÃ¤nderungen in diesem Sinne genau wie CodeÃ¤nderungen <em>verstehen</em>, macht es auch Sinn, DatenbankÃ¤nderungen genau wie CodeÃ¤nderungen zu <em>behandeln</em>. Und das bedeutet, diese innerhalb des bereits vorhandenen Entwicklungsprozesses zu managen und im selben VCS Repository zu verwalten.

<h3>Abbildung der DatenbankÃ¤nderungen im VCS</h3>

Unter <em>DatenbankÃ¤nderungen</em> mÃ¼ssen wir verstehen: Alle SQL Statements, welche die Strukturen oder Inhalte einer Datenbank verÃ¤ndern.

Eine DatenbankÃ¤nderung im Zuge eines Projekts, Bugfixes oder sonstigen Tickets ist daher folgerichtig eine Sammlung von SQL Statements, welche zusammen mit den CodeÃ¤nderungen des zugehÃ¶rigen Tickets im selben Branch vom Entwickler hinterlegt wird. Hinzu kommt, dass es eine klar definierte <em>LokalitÃ¤t</em> fÃ¼r diese Ã„nderung geben muss, damit ein Raum geschaffen ist, in dem Konflikte entstehen (und gelÃ¶st werden) kÃ¶nnen. So wie die gleichzeitige Ã„nderung an der Datei <em>myFile.txt</em> in zwei verschiedenen, zu mergenden Branches zu einem Konflikt fÃ¼hrt &#8211; da in beiden Branches die Datei den selben Speicherort, also dieselbe LokalitÃ¤t besitzt &#8211; mÃ¼ssen auch Ã„nderungen an derselben Tabelle in zwei Branches innerhalb derselben LokalitÃ¤t des jeweiligen Branches stattfinden. Der vorgeschlagene Ansatz ist daher, die Struktur der Datenbank, also die Databases mit den darunterliegenden Tables, in einer analog aufgebauten Ordner-Datei-Struktur abzubilden.

Die LokalitÃ¤t  fÃ¼r die Tabelle <em>users.hobbies</em> wÃ¤re beispielsweise die Datei <em>/dbchanges/users/hobbies.sql</em> innerhalb des VCS. Abgebildet wird die gesamte DB Struktur, also alle Databases mit allen ihren Tables:

<code>
/dbchanges/users/hobbies.sql
/dbchanges/users/contact.sql
...
/dbchanges/products/colors.sql
/dbchanges/products/forms.sql
...
</code>

und so weiter. Gerade bei komplexen Datenbanken macht es natÃ¼rlich Sinn, diese Struktur mit einem Skript zu erzeugen, fÃ¼r MySQL kann man dazu in einem beliebigen Verzeichnis auf dem DB Server folgenden Code ausfÃ¼hren (geht davon aus, dass die MySQL Daten unterhalb /var/lib/mysql liegen):

<code>
find /var/lib/mysql -type f -name *.frm -exec dirname {} \;| cut -d "/" -f 5| xargs mkdir -pfind /var/lib/mysql -type f -name *.frm | cut -d "/" -f 5,6 | sed "s/.frm/.sql/g" | xargs touch
</code>

Diese Dateien nenne ich im folgenden <em>DB Change Container</em>.

<h3>Prozessbeschreibung</h3>

<h4>WÃ¤hrend der Produktion eines neuen Release</h4>

Wichtig ist, dass sÃ¤mtliche DB Change Container nach einem Release, nachdem diese Ã„nderungen also auf dem Produktivsystem angewendet wurden, wieder leer sind &#8211; denn zum Start der Produktion eines neuen Releases liegen noch keine neuen Ã„nderungen fÃ¼r die DB vor.

Nun beginnen die Entwickler, Tickets (Feature Requests, Bugs etc.) umzusetzen, einige gemeinsam in einem Branch, einige in eigenen Branches. Sind im Zuge einer Implementation DatenbankÃ¤nderungen notwendig, hinterlegt der Entwickler innerhalb des zugehÃ¶rigen Branches diese Ã„nderungen nach folgendem Muster:

<ul>
 <li><u><strong>Case 1:</strong> Die Tabelle user.hobbies soll verÃ¤ndert werden (neues Feld, Feld lÃ¶schen, Index anlegen oder lÃ¶schen, einfÃ¼gen, lÃ¶schen oder Ã¤ndern von EintrÃ¤gen etc.)</u>
   
  Der Entwickler legt alle benÃ¶tigten Statements in der Datei <em>/dbchanges/users/hobbies.sql</em> ab:

  <code>
USE users;
ALTER TABLE hobbies ADD newfield1 INT NOT NULL AFTER userId;
ALTER TABLE hobbies DROP oldfield;
ALTER TABLE hobbies ADD newfield2 TINYINT NOT NULL;
ALTER TABLE hobbies ADD INDEX (newfield2);
INSERT INTO hobbies ( id, name, value ) VALUES (1234, 'hobbyname', 'hobbyvalue');
  </code>
 </li>

 <li><u><strong>Case 2:</strong> Der Entwickler legt eine komplett neue Tabelle pets im vorhandenen Schema users an</u>
    
  Er erzeugt dazu eine neue Datei <em>/dbchanges/users/pets.sql</em> und fÃ¼llt sie mit dem CREATE Statement (sowie ggf. INSERT Statements):

  <code>
USE users;
CREATE TABLE pets(
 id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
 petname VARCHAR( 64 ) NOT NULL,
 FULLTEXT ( petname )
);
  </code>
 </li>

 <li><u><strong>Case 3:</strong> Der Entwickler legt eine neue Database products und darin eine neue Tabelle colors an</u>
 
   Er erzeugt einen neuen Ordner <em>/dbchanges/products</em> und darin eine Datei <em>colors.sql</em> mit folgendem Inhalt:
   <code>
CREATE DATABASE products;
USE products;
CREATE TABLE colors (
 id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
 colorname VARCHAR( 24 ) NOT NULL);
   </code>
 </li>

 <li><u><strong>Case 4:</strong> Der Entwickler lÃ¶scht die Tabelle colors in der Database products</u>
    
   Er fÃ¼llt die Datei <em>/dbchanges/products/colors.sql</em> mit folgendem Inhalt:
   
  <code>
USE products;
DROP TABLE colors;
  </code>
 </li>
</ul>
  
Ansonsten lÃ¤uft der Entwicklungsprozess wie gewohnt.

<h4>Merge aller Tickets fÃ¼r den Release</h4>

Werden nun verschiedene Tickets fÃ¼r den Release gebÃ¼ndelt, werden die einzelnen Branches wie gehabt gemerged. In Hinblick auf die DB Ã„nderungen passiert nun folgendes:

SÃ¤mtliche Ã„nderungen in den einzelnen Branches unterhalb von <em>/dbchanges</em> werden naturgemÃ¤ÃŸ unterhalb <em>/dbchanges</em> im Merge zusammengefÃ¼hrt. Hierbei greifen die bekannten VCS Mechanismen: Wurden Ã„nderungen in einer Datei nur in einem einzigen Branch oder Commit vorgenommen, werden diese Ã„nderungen einfach angewendet. Wurden Ã„nderungen an einer Datei (also innerhalb derselben LokalitÃ¤t) in mehreren Branches vorgenommen, kommt es zu einem Konflikt.

Dies ist der erste wichtige Mechanismus der hilft, die drei Anforderungen &#8211; einfach, fehlerfrei und nachvollziehbar &#8211; zu gewÃ¤hrleisten: Da der Konflikt garantiert eintritt, ist auch garantiert, dass der Vorgang vÃ¶llig automatisch die notwendige Aufmerksamkeit erzeugt und nicht Ã¼bersehen werden kann.

Nun muss, wie auch bei Codekonflikten, gelÃ¶st werden: Machen beide Ã„nderungen Sinn, oder widersprechen sie sich? Wie genau kann man sie am sinnvollsten zusammenfÃ¼hren? Relevant ist hier nur, dass am Ende ein Set an Ã„nderungsanweisungen in den Approval committet wird, welches in sich rund ist. Falls es eine eigene Test oder QA Datenbank gibt auf die diese Ã„nderungen angewendet werden mÃ¼ssen, wird dies gemacht nachdem alle Tickets fertig gemerged wurden.

<h4>DurchfÃ¼hrung des Release</h4>

Wurde im Vorfeld alles richtig gemacht, muss im Zuge des Rollout oder Release nur noch das zusammengefasste Set an Ã„nderungen ermittelt werden, und diese mÃ¼ssen dann, entsprechend ihrer jeweiligen Eigenschaft, ausgefÃ¼hrt werden. Die Summe der Ã„nderungen ergibt sich aus der Summe aller Anweisungen in den DB Change Containern unterhalb <em>/dbchanges</em> &#8211; hier macht es natÃ¼rlich Sinn, dass man diese mithilfe eines Skripts &#8220;zusammensammelt&#8221;, aber ich gehe hier nicht nÃ¤her darauf ein.

Nach dem Rollout/Release, und vor dem Erzeugen neuer Branches, mÃ¼ssen dann im Trunk sÃ¤mtliche Datenbank-Ã„nderungsanweisungen aus den DB Change Containern entfernt werden (auch hier macht ein Skript wie z.B.

<code>
for f in `find . -type f -name *.sql`; do echo -n "" &gt; $f; done
</code>

Sinn, um diesen Schritt zu vereinfachen), und dies muss in den Trunk (oder von wo aus auch immer neue Branches gebildet werden) committet werden &#8211; denn sonst wÃ¼rden dieselben Ã„nderungen beim nÃ¤chsten Rollout erneut angewendet werden.
]]></content>
		<link rel="replies" type="text/html" href="/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2010/02/26/database-change-management-mithilfe-von-vcs-teil-1/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Wie man Replikationsunterbrechung durch Deadlocks bei INSERT INTO â€¦ SELECT verhindert]]></title>
		<link rel="alternate" type="text/html" href="/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/" />
		<id>http://172.16.111.147/wordpress/?p=36</id>
		<updated>2010-07-13T10:29:47Z</updated>
		<published>2007-08-07T13:04:17Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[Der My-Hammer Auftragsradar, der unsere Auftragnehmer auf Wunsch regelmÃ¤ssig per E-Mail Ã¼ber neu eingestellte Auktionen anhand einstellbarer Filterkriterien informiert, baut bei jedem Durchlauf eine eigene Suchtabelle auf. Diese wird gefÃ¼llt mit einer Untermenge der Daten unserer Haupt-Auktionstabelle, nÃ¤mlich nur den derzeit laufenden Auktionen.

Die Verwendung von INSERT INTO â€¦ SELECT ist hier naheliegend, zum Beispiel so:

INSERT [...]]]></summary>
		<content type="html" xml:base="/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/"><![CDATA[Der <a href="http://web.archive.org/web/20080407200839/http://www.my-hammer.de/showPage.php?id=auftragservice">My-Hammer Auftragsradar</a>, der unsere Auftragnehmer auf Wunsch regelmÃ¤ssig per E-Mail Ã¼ber neu eingestellte Auktionen anhand einstellbarer Filterkriterien informiert, baut bei jedem Durchlauf eine eigene Suchtabelle auf. Diese wird gefÃ¼llt mit einer Untermenge der Daten unserer Haupt-Auktionstabelle, nÃ¤mlich nur den derzeit laufenden Auktionen.

Die Verwendung von <em>INSERT INTO â€¦ SELECT</em> ist hier naheliegend, zum Beispiel so:

<p class="wp_syntax"><p class="code"><pre class="sql"><span style="color: #993333; font-weight: bold">INSERT</span> <span style="color: #993333; font-weight: bold">INTO</span> Suchtabelle
&nbsp;<span style="color: #993333; font-weight: bold">SELECT</span> a, b, c <span style="color: #993333; font-weight: bold">FROM</span> Auktionstabelle <span style="color: #993333; font-weight: bold">WHERE</span> x = y</pre></p></p>


Es ergab sich folgendes Problem: Der Query wird wie jeder andere auch auf die Datenbankslaves repliziert. Dort wurde er auch korrekt ausgefÃ¼hrt. Jedoch nicht immer auf dem Master: hier kam es regelmÃ¤ÃŸig zu Deadlocks auf der Auktionstabelle, da dies eine InnoDB Tabelle ist (bei MyISAM Tabellen kÃ¶nnen Deadlocks nicht auftreten).

Wenn ein MySQL Slave jedoch feststellt, dass beim gleichen Query auf dem Master und auf dem Slave unterschiedliche Fehler auftreten (Slave: no error; Master: deadlock), unterbricht dieser die Replikation. Es hilft dann nur ein <em>SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1; START SLAVE;</em>.

Ich habe mich daraufhin nach LÃ¶sungen umgeschaut. Erste Anlaufstelle ist das Kapitel <a href="http://web.archive.org/web/20080407200839/http://dev.mysql.com/doc/refman/5.1/de/innodb-deadlocks.html"><em>Vom Umgang mit Deadlocks</em></a> im MySQL Handbuch.

Mein erster Versuch war, den 4. Tipp dieses Kapitels zu befolgen: Das Einstellen eines niedrigeren Isolationslevels. Da perfekte Datenkonsistenz fÃ¼r die benÃ¶tigte Suchtabelle nicht nÃ¶tig ist (<em>dirty reads</em> also akzeptabel sind), verwendete ich gleich den niedrigsten Level <em>READ UNCOMMITED</em>. Das Ergebnis war gelinde gesagt verheerend, es traten noch mehr Deadlocks auf als zuvor.

Deshalb bin ich dazu Ã¼bergegangen, die beteiligten Tabellen explizit mit einem <em>READ LOCK</em> zu sperren &#8211; viele Artikel zu diesem Thema haken diese Vorgehensweise sofort als nicht gangbar ab, da die Performance darunter leide. Da es sich beim Auftragsradar jedoch um einen Cronjob handelt, der nur alle paar Minuten einmal lÃ¤uft, und der <em>INSERT INTO â€¦ SELECT</em> Query sehr schnell durchlÃ¤uft, erschien mir das Risiko, eine unserer wichtigsten Tabellen fÃ¼r diesen Query zu sperren, als gering.

Wie sich zeigte, brachte dies den gewÃ¼nschten Erfolg: Seitdem sind an dieser Stelle keinerlei Deadlocks mehr aufgetreten, und der Rest der Plattform zeigt sich von den seltenen und kurzen Locks vÃ¶llig unbeeindruckt.]]></content>
		<link rel="replies" type="text/html" href="/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2007/08/07/wie-man-replikationsunterbrechung-durch-deadlocks-bei-insert-into-select-verhindert/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
		<entry>
		<author>
			<name>Manuel Kiessling</name>
					</author>
		<title type="html"><![CDATA[Recycelter Artikel: &#8220;My-Hammer, das Fernsehen und die Serverlast&#8221;]]></title>
		<link rel="alternate" type="text/html" href="/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/" />
		<id>http://localhost/wordpress/?p=13</id>
		<updated>2010-09-02T12:29:30Z</updated>
		<published>2007-07-16T23:22:28Z</published>
		<category scheme="/" term="Software" />		<summary type="html"><![CDATA[
Vor mittlerweile auch schon wieder einer halben Ewigkeit hatte ich mal eine kurze Artikelserie zum Thema Serverlast-ProblemlÃ¶sungen bei MyHammer online, die ich nun wieder ausgegraben habe. Vieles entspricht gar nicht mehr den aktuell bei MyHammer eingesetzten LÃ¶sungen, aber verwahrenswert finde ich den Schrieb allemal. Leider fehlen die Grafiken, vielleicht finde ich die noch mal irgendwo.



Hier [...]]]></summary>
		<content type="html" xml:base="/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/"><![CDATA[<p>
Vor mittlerweile auch schon wieder einer halben Ewigkeit hatte ich mal eine kurze Artikelserie zum Thema Serverlast-ProblemlÃ¶sungen bei MyHammer online, die ich nun wieder ausgegraben habe. Vieles entspricht gar nicht mehr den aktuell bei MyHammer eingesetzten LÃ¶sungen, aber verwahrenswert finde ich den Schrieb allemal. Leider fehlen die Grafiken, vielleicht finde ich die noch mal irgendwo.
</p>

<p>
Hier der Artikel:
</p>

<p>
Vergangenen Donnerstag zeigte das ProSieben Magazin <em>Galileo</em> einen ca. 10-minÃ¼tigen Beitrag Ã¼ber My-Hammer (<a href="http://www.prosieben.de/wissen/galileo/themen/artikel/40712/">kurze Infos zur Sendung hier</a>). Vom Ansatz her ging es um â€œBranchenbuch vs. My-Hammerâ€, aber fÃ¼r die Betrachtung hier ist das gar nicht so sehr von Interesse &#8211; es ist noch nichtmal von Interesse, ob so ein Beitrag positiv oder negativ fÃ¼r uns ist (in dem Fall warâ€™s wie fast immer positiv) &#8211; sobald das Magazin, in dem Ã¼ber uns berichtet wird, genug Reichweite hat, schieÃŸen die Zugriffe in die HÃ¶he. Die wichtigste Erkenntnis, die wir immer wieder machen: zumindest bei den Privaten scheinen die Zuschauer sprichwÃ¶rtlich mit dem Laptop auf den Knien vorm Fernseher zu sitzen. Die Zugriffe kommen extrem schnell und gebÃ¼ndelt (beim Galileo-Beitrag war aber interessant, dass die Zugriffe wieder auf einen Schlag kamen, aber erst in dem Moment, in dem der Beitrag vorbei war).
</p>

<p>
Genau dieses plÃ¶tzliche Auftreten so vieler Zugriffe ist natÃ¼rlich die Herausforderung &#8211; dieselbe Anzahl User auf nur 15 Minuten verteilt wÃ¤ren kein Problem, aber TV sorgt dafÃ¼r, dass das meiste innerhalb der ersten 5 Minuten passiert. Und es ist wirtschaftlich natÃ¼rlich ziemlich unvernÃ¼nftig, die fÃ¼r diese 5 Minuten benÃ¶tigte Rechenpower anzuschaffen, nur damit sie die anderen 525.595 Minuten im Jahr vor sich hindÃ¼mpelt.
</p>

<p>
Trotzdem kann man eine Webseite auch auf solche Extremsituationen vorbereiten &#8211; My-Hammer hat am Donnerstag perfekt standgehalten, lediglich eine leichte VerzÃ¶gerung in den Ladezeiten war wÃ¤hrend der kritischen Phase spÃ¼rbar.
</p>

<p>
Um kurz die Dimensionen klarzumachen, erstmal eine Grafik, welche den ein- und ausgehenden IP Traffic fÃ¼r unser Netzwerk anzeigt. Man sieht sehr deutlich den Sprung auf das gut 2,5-fache des normalen Werts. Der Faktor selbst klingt vielleicht erstmal nicht so dramatisch, aber wie erwÃ¤hnt geht es nicht um die Masse an sich, sondern das extrem gebÃ¼ndelte Auftreten dieser Masse an Zugriffen:
</p>

<p>
(Die Grafik ist leider nicht mehr auffindbar)
</p>

<p>
Ich behaupte mal, man erkennt recht gut, wann die Sendung liefâ€¦
</p>

<p>
Also, wie kann man die Serversysteme auf so etwas vorbereiten? Klar: mehr Server kaufen. Das ist durchaus ein Aspekt, aber nicht das Allheilmittel. Vor allen Dingen kann das sehr ineffektiv und unwirtschaftlich sein. Angenommen, man hat Server A mit einer gewissen LeistungsfÃ¤higkeit. Nun kann man sich Server B mit doppelt so schnellem Prozessor, doppeltem Arbeitsspeicher und doppelt so schnellen Festplatten kaufen. Dann hat man schon Unmengen von Geld ausgegeben, und hat gerade mal eine Steigerung der LeistungsfÃ¤higkeit von 100% (mal davon abgesehen, dass die Rechnung â€œdoppelt so schnelle Hardware, doppelt so viel Leistungâ€ in der Praxis auch nicht wirklich hinhaut). Dagegen kann ein einziger geschickt gesetzter Index in der Datenbank manchmal 1000% bessere Performance bringen, ohne dass man etwas an der Hardware tut.
</p>

<p>
Wenn man den Anschaffungspreis neuer Hardware mal auf den Stundenlohn eines Entwicklers umrechnet, wird man schnell zu dem Schluss kommen, dass es sich auch finanziell durchaus rechnen kann, diesen einige Tage lang auf die Datenbank anzusetzen um zu schauen, ob nicht doch irgendwo ein wichtiger Index vergessen wurde oder einige Tabellenstrukturen besser ganz anders aufgebaut sein sollten.
</p>

<p>
Das sind nur ein paar grundsÃ¤tzliche Ãœberlegungen. SpÃ¼rbaren Erfolg wird man nur haben, wenn man ein ganzes BÃ¼ndel an Massnahmen ergreift und vor allem immer das Gesamtsystem vom Code Ã¼ber die Datenbank bis hin zu den Servern und dem Netzwerk im Ãœberblick hat. Die vielleicht wichtigste Faustregel, wenn man Ã¼ber Performanceoptimierung von Webseiten spricht, scheint mir daher zu sein: Coder und Admins an einen Tisch! Es hilft nichts, wenn die Entwickler meinen, die Geschwindigkeit des Systems sei doch schliesslich Sache des Admins. Umgekehrt ist es extrem hilfreich, wenn die Programmierer auch einen gewissen Sysadmin-Background haben, und die Admins umgekehrt auch Programmiererfahrung haben; was bei uns glÃ¼cklicherweise sogar sehr ausgeprÃ¤gt der Fall ist.
</p>

<p>
Die weiteren Teile befassen sich mit den konkreten Massnahmen, die man ergreifen kann um sich auf einen TV Beitrag vorzubereiten.
<strong>Hinweis:</strong> Thematisch durchaus verwandt berichtet Tom Bachem <a href="http://blog.thomasbachem.com/2007/05/28/die-sevenload-systemarchitektur/">Ã¼ber die Systemarchitektur von sevenload</a>.
</p>

<p>
Welche Massnahmen kann man nun konkret ergreifen, um sich auf einen TV Beitrag Ã¼ber die eigene Webseite vorzubereiten? Ich versuche so allgemein wie mÃ¶glich zu bleiben, aber da es um konkrete RatschlÃ¤ge gehen soll und ich holprige Umschreibungen vermeiden mÃ¶chte, wird das Vokabular ab jetzt etwas LAMP-lastig; bitte entsprechend auf die eigene Technik ummÃ¼nzen.
</p>

<h3>Massnahme 1: Datenbankoptimierung</h3>
<p>
Wurde ja schon erwÃ¤hnt: die Indizes. Ich verrate wahrscheinlich nicht einmal DB AnfÃ¤ngern etwas neues, wenn ich betone, dass dies essentiell ist. Wenn man die Indizes nicht im Griff hat, braucht man sich die anderen Punkte noch gar nicht anschauen. Deshalb: Ins Slow-Log gucken. Vor allem: Immer wieder. Einen Status Quo gibt es nicht! Immer wieder <a href="http://dev.mysql.com/doc/refman/5.0/en/explain.html">EXPLAIN</a> bemÃ¼hen, vom stumpf auf die Strukturen in phpMyAdmin gucken findet man die Performancefresser nicht.
</p>

<p>
Es gibt diese missverstÃ¤ndliche Formel â€œBraucht man Geschwindigkeit, nimmt man MyISAM, braucht man Sicherheit, InnoDBâ€. InnoDB ist nicht nur einen Blick wert, wenn man Transaktionssicherheit braucht. Im Gegensatz zu MyISAM lockt InnoDB bei schreibenden Queries immer nur die betreffenden Zeilen, MyISAM dagegen grundsÃ¤tzlich die gesamte Tabelle. InnoDB hat zwar aufgrund der grÃ¶ÃŸeren KomplexitÃ¤t etwas mehr â€œGrundoverheadâ€, aber das intelligentere Locking kann immens wertvoll sein in bestimmten Szenarien und das mehr als wettmachen. Wenn man eine Tabelle hat die man hinsichtlich Struktur und Indices schon perfekt durchoptimiert hat (genau das aber wiederum erstmal sicherstellen!), und trotzdem tauchen Queries auf diese Tabelle immer noch im Slow Log auf, dann sollte man prÃ¼fen, ob diese Queries vielleicht immer auf einen Lock warten. In diesem Fall InnoDB auf jeden Fall eine Chance geben. Das hat bei uns konkret bei den Session und Cachetabellen (dazu spÃ¤ter mehr) enorm viel gebracht, weil dort die Lese- und Schreibzugriffe ein ausgewogenes VerhÃ¤ltnis haben.
</p>

<p>
Ein Aspekt, der wenig berÃ¼cksichtigt wird, ist die GrÃ¶ÃŸe der Felder, auf die man Indices setzt. Es kann sich lohnen, hier sparsam zu sein, denn ein kleinerer Spaltentyp bedeutet auch weniger Speicherplatzverbrauch fÃ¼r den Index auf diese Spalte, und das kann im Zweifel nur gut (= schneller) sein. Man ist halt geneigt, seine Primary IDs immer als INT anzulegen. Aber nehmen wir mal den Klassiker Benutzertabelle: Wird man wirklich in nÃ¤chster Zeit 4 Milliarden User haben? Das dÃ¼rfte selbst bei eBay noch ein bisschen dauern. Erstmal tut es also auch ein MEDIUMINT, setzt man diesen UNSIGNED, ist das Limit bei 16 Millionen. Hat man soviele User, bewegt man sich wohl eh in vÃ¶llig anderen Dimensionen.
</p>

<p>
Zumal das Umwandeln einer Spalte in einen Typ mit grÃ¶ÃŸerem Wertbereich (also z.B. von MEDIUMINT nach INT) unproblematisch ist. Wichtig ist allerdings auch, dass man sÃ¤mtliche Felder, die einen FremdschlÃ¼ssel auf ein MEDIUMINT Feld darstellen, ebenfalls als MEDIUMINT anlegt, sonst hat man bei Joins nichts gewonnen.
</p>

<p>
Was bei der Skalierung von MySQL immer enorm hilft ist Replikation. Dazu wurde schon so viel geschrieben, dass ich mir die Wiederholung spare, nur dies: Wir fahren bisher sehr gut damit, das Balancing der Nur-Lese Zugriffe direkt in unserer Applikation zu regeln, und nicht Ã¼ber einen eigenen Software- oder Hardware-Loadbalancer. Da bei fast jedem Seitenaufruf der Master sowieso frÃ¼her oder spÃ¤ter konnektiert werden muss, kann man diese Verbindung auch nutzen, um MASTER STATUS und SLAVE STATUS zu vergleichen, um so ein Fallback auf den Master zu realisieren, falls alle Slaves einmal mehr als 0 Sekunden hinter dem Master zurÃ¼ckhÃ¤ngen. Was sich Ã¼brigens ziemlich gut vermeiden lÃ¤sst, wenn man Master und Slaves per Gigabit statt Fast Ethernet anbindet.
</p>

<p>
Ein oft nicht wahrgenommener Vorteil von Replikation: Man kann einen Slave fÃ¼râ€™s Backup bereitstellen, auf dem man die Datenbank stoppen und auf Dateisystemebene wegkopieren kann (oder man hÃ¤lt nur den Slave Thread an und macht einen Dump), so dass man einen sauberen Snapshot der Datenbank hat, ohne das Gesamtsystem anhalten zu mÃ¼ssen.
</p>

<p>
Ein weiterer wichtiger Hebel fÃ¼r die Skalierung ist es, fÃ¼r spezielle Aufgaben jeweils eigene DB Server bereitzustellen, z.B. ein oder mehrere Maschinen nur fÃ¼r die Sessiontabellen, nur fÃ¼r Tabellen mit Cache-Inhalten, nur fÃ¼r Logtabellen; prinzipiell kann jede Tabelle, die nicht in Form von Joins oder Subselects zusammen mit anderen Tabellen gleichzeitig abgefragt werden muss, auch getrennt von den anderen Tabellen auf einem eigenen Server liegen. DarÃ¼ber hinaus macht die Trennung von sehr verschiedenen Tabellen wie Session- und Logtabellen alleine deshalb schon Sinn, weil man dann die Datenbanksoftware fÃ¼r diese speziellen Aufgaben optimieren kann.
</p>

<p>
Eine praxisnahe Zusammenstellung der Massnahmen, die sich bei <a href="http://www.my-hammer.de/">My-Hammer.de</a> bewÃ¤hrt haben:
<p>

<h4>InnoDB vs MyISAM</h4>
<p>
Ich schrieb bereits, dass man InnoDB nicht nur dann in ErwÃ¤gung ziehen sollte, wenn man Transaktionssicherheit benÃ¶tigt. Eine Tabelle von MyISAM auf InnoDB umzustellen kann unter UmstÃ¤nden Geschwindigkeitsvorteile bringen, nÃ¤mlich dann, wenn das zweite wichtige Feature von InnoDB neben der Transaktionssicherheit, das Row Level Locking, effektiv zum Zug kommen kann. Um herauszufinden, ob dies der Fall ist, kann man wie folgt vorgehen:
</p>

<h4>Mitloggen aller Queries</h4>
<p>
Wenn man fÃ¼r einen bestimmten Zeitraum (bei einer gut besuchten Seite reichen wenige Minuten) einmal alle Abfragen, die an die Datenbank gestellt werden, mitschreibt, kann man aus diesem Log eine Menge interessanter Informationen ziehen. Um festzustellen, ob eine Tabelle vom Row Level Locking profitieren kÃ¶nnte, muss man die lesenden (SELECT) und schreibenden (INSERT, UPDATE, DELETE etc.) Abfragen gegenÃ¼berstellen.
</p>

<p>
Wird aus einer Tabelle sehr hÃ¤ufig gelesen, die Daten in der Tabelle aber nur sehr selten verÃ¤ndert, dann macht das Table Level Locking von MyISAM in der Regel keine Probleme: Zwar wird bei einem UPDATE, INSERT oder DELETE die gesamte Tabelle fÃ¼r nachfolgende Lesezugriffe gesperrt (d.h. diese mÃ¼ssen warten), bis der Schreibprozess abgeschlossen ist. Aber da dies nur selten geschieht, kommt es auch selten vor, dass ein Leseprozess warten muss, so dass daraus keine spÃ¼rbare VerzÃ¶gerung im Gesamtsystem resultiert.
</p>

<p>
Gleiches gilt im umgekehrten Fall: Wird in eine Tabelle praktisch nur geschrieben, aber selten daraus gelesen (wie es z.B. bei Logtabellen hÃ¤ufig der Fall ist), dann kollidieren auch hier die â€œInteressenâ€ nur so selten, dass nicht mit PerformanceeinbuÃŸen zu rechnen ist.
</p>

<h4>Slow Log</h4>
<p>
Interessant sind also jene Tabellen, bei denen Schreib- und Lesezugriffe in einem ausgeglicheneren VerhÃ¤ltnis stehen. In welcher Relation die beiden Zugriffsarten dabei mindestens stehen mÃ¼ssen, damit es sich â€œlohntâ€ InnoDB einzusetzen, ist schwer zu sagen. Ein Blick ins Slow-Log von MySQL hilft hier weiter: Wenn man immer wieder bei denselben Tabellen auf langsame Queries stÃ¶sst, die nicht wegen des Queries selbst langsam waren, sondern weil sie auf ein Lock warten mussten, hat man auf jeden Fall aussichtsreiche Kandidaten.
</p>

<h4>SHOW PROCESSLIST</h4>
<p>
Eine weitere Methode ist, sich einmal fÃ¼r einige Minuten immer wieder die Liste der laufenden Prozesse in MySQL auflisten zu lassen (SHOW PROCESSLIST). Wenn man dort immer wieder dieselben Queries sieht, deren Status <em>Locked</em> ist, dann weiss man wo das Problem liegt. Diese Methode mag zwar auf den ersten Blick wie ein GlÃ¼cksspiel wirken, aber gerade weil man immer nur die Prozesse sieht, die zufÃ¤llig gerade laufen wenn man den Befehl absetzt, fallen die problematischen Prozesse erst recht auf, die immer wiederkehren und oft vielleicht sogar wÃ¤hrend zwei oder mehr SHOW Aufrufen immer noch laufen. Meiner Meinung nach die schnellste Methode, FlaschenhÃ¤lse zu finden.
</p>

<p>
Mehr zum Thema Locking gibt es im <a href="http://dev.mysql.com/doc/refman/5.0/en/internal-locking.html">Kapitel â€˜Internal Locking Methodsâ€™ des MySQL Handbuchs</a>.
</p>

<p>
Nehmen wir also an, man hat einige Tabellen identifiziert, bei denen Queries Ã¶fter als gesund ist auf einen Lock warten mÃ¼ssen. Dies kÃ¶nnte beispielsweise eine Sessiontabelle sein (falls man z.B. PHP nutzt und die Sessionfunktionen so angepasst hat, dass diese eine MySQL Datenbank als Storage nutzen, ein ziemlich klassisches Szenario). Diese Tabelle wird bei jedem Seitenaufruf zu Beginn einmal gelesen, um die Session des aufrufenden Benutzers zu laden, und am Ende des Skripts wird der Sessioninhalt dieses Benutzers wieder geschrieben. Also ein sehr ausgewogenes VerhÃ¤ltnis zwischen lesenden und schreibenden Zugriffen &#8211; jeder Seitenaufruf, der gerade an dem Punkt angelangt ist, an dem die Session geschrieben wird, wÃ¼rde also die Tabelle sperren fÃ¼r sÃ¤mtliche anderen Seitenaufrufe, die in diesem Moment aus der Sessiontabelle lesen mÃ¶chten &#8211; das Performanceproblem ist ab einer bestimmten Anzahl von gleichzeitigen Benutzern vorprogrammiert.
</p>

<p>
Klassischerweise geht man nun so vor, dass man die Tabelle in InnoDB umwandelt und wieder einige Zeit das Slow Log oder die Prozessliste beobachtet &#8211; sinkt die Lock_Time der Abfragen deutlich, hat man einen Flaschenhals erfolgreich eliminiert.
</p>

<p>
Nun, es wÃ¤re freilich zu schÃ¶n, wenn es nicht doch den ein oder anderen Haken bei der Sache gibt; zum GlÃ¼ck lassen sich die meisten aber zumindest einigermassen elegant umschiffen.
</p>

<p>
Eine EinschrÃ¤nkung von InnoDB ist beispielsweise, dass der FULLTEXT Index nicht unterstÃ¼tzt wird. Dies war bei My-Hammer ein Problem, weil wir eine Tabelle, die ziemlich eindeutiger Kandidat fÃ¼r eine Umstellung von MyISAM auf InnoDB war, in einem Teil unserer Applikation auch durchsuchen mussten, und zwar eben gerade einige TEXT-Felder, was ohne FULLTEXT Index nicht wirklich Spass macht.
</p>

<p>
Die LÃ¶sung war, die Tabelle umzuwandeln und damit in der Tabelle selbst auf die FULLTEXT Indizes zu verzichten, per cronjob aber eine weitere Tabelle regelmÃ¤ssig mit den Daten der Ursprungstabelle zu fÃ¼llen. Geschrieben wurde in diese Tabelle nur durch besagten Crobjobs, ansonsten fanden ausschliesslich Lesezugriffe statt, womit MyISAM wieder die perfekte Wahl war &#8211; und wir hatten unsere FULLTEXTs wieder. SchÃ¶ner Nebeneffekt: durchsucht werden mÃ¼ssen eh nur eine Untermenge aller Zeilen der Ursprungstabelle, und es mÃ¼ssen auch nicht alle der (recht zahlreichen) Spalten in die Suchtabelle Ã¼bertragen werden.
</p>

<p>
Dadurch konnten wir nicht nur das Lockingproblem der ursprÃ¼nglichen Tabelle lÃ¶sen, sondern aufgrund der schlankeren Datenbasis in der Suchtabelle die Suche deutlich beschleunigen.
</p>

<p>
Wichtig ist jedoch: diese LÃ¶sung ist nur mÃ¶glich, weil wir in diesem Fall darauf verzichten kÃ¶nnen, auf absoluten Livedaten zu suchen.
</p>

<p>
Meiner Erfahrung nach kann man zusammenfassend sagen: Es gibt nur eine einzige Massnahme, die mehr Performance bringt als Caching, und das ist noch mehr Caching. Das gilt, um mal zum Haupthema zurÃ¼ckzukehren, vor allem in Bezug auf Performance bei plÃ¶tzlichen BesucheranstÃ¼rmen.
</p>

<h3>Statische Inhalte</h3>
<p>
Wenn man einen TV Beitrag Ã¼ber die eigene Plattform Ã¼berleben will, dann gibt es nichts, aber auch wirklich gar nichts Wichtigeres als dies hier: <em>Die Startseite der Plattform ist eine statische HTML Seite</em>. Und zwar in aller Konsequenz, was heissen soll, dass der Aufruf der Seite nicht nur keine Datenbankverbindung zur Folge hat, sondern dass noch nicht einmal der PHP Interpreter auch nur gestartet wird. Die Startseite von My-Hammer ist eine .html Seite, die im Gegensatz zu den .php Seiten per Apache-Konfiguration mod_php noch nicht mal von Weitem zu sehen bekommt. Selbiges sollte konsequenterweise auch fÃ¼r alle JavaScript und CSS Dateien, die von der Startseite eingebunden werden, gelten. Ob man hierfÃ¼r nun mit ProxylÃ¶sungen arbeitet oder Seiten regelmÃ¤ÃŸig vorgeneriert, ist Geschmackssache.
</p>

<p>
Man darf nie den Performancevorteil reinen HTMLs unterschÃ¤tzen &#8211; selbst wenn sich die Datenbanken schon alle verabschiedet haben und die Webserver bereits richtig unter Dampf sind: Eine HTML Seite auszuliefern schafft sogar ein Webserver, der schon ziemlich am Ende ist. Und man wahrt vor allem noch am ehesten sein Gesicht, wenn die ganzen neuen Benutzer, die aufgrund des TV Beitrages neugierig geworden sind, zumindest die Startseite zu sehen bekommen. Was immer man neben der Startseite noch an Seiten statisch vorgenerieren kann, ohne dass der angebotene Dienst selber â€œstatischâ€ wird, sollte man natÃ¼rlich machen (denn wie oft Ã¤ndern sich schon Seiten wie <em>Ãœber uns</em>?). Hierbei macht es Sinn, sich mithilfe der Zugriffsstatistiken einmal anzuschauen, welchen Weg neue Benutzer in der Regel auf der Plattform nehmen, um so auch wirklich jene Seiten zu cachen, die bei einem Ansturm am ehesten angesurft werden.
</p>

<p>
Eine dynamische Seite als statische Seite vorzugenerieren ist dabei natÃ¼rlich die konsequenteste Version von Caching, aber nicht immer praktikabel. My-Hammer nutzt eine zweite Stufe des Cachings, bei dem zwar weiterhin dynamische Seiten ausgeliefert werden, diese aber ganz oder teilweise in dedizierten Cache-Backends (wir nutzen dazu <a href="http://www.danga.com/memcached/">memcached</a>) abgelegt sind, um Ergebnisse teurer Datenbankabfragen, die nicht immer absolut live zur VerfÃ¼gung stehen mÃ¼ssen, zwischenzuspeichern. Diese zwischengespeicherten EintrÃ¤ge kÃ¶nnen zum einen nach einer gewissen Zeit ablaufen und werden dann neu aus der Datenbank erzeugt, oder kÃ¶nnen gezielt als â€œdirtyâ€ markiert werden, wenn die DatenbestÃ¤nde die sie widerspiegeln sich Ã¤ndern.
</p>

<p>
Wie oben erwÃ¤hnt kann es sich auÃŸerordentlich lohnen, diese Cacheinhalte auf dedizierten Maschinen bereitzustellen &#8211; was wiederum deutlich zeigt, dass manche Massnahmen zur Performancesteigerung bestenfalls halbgar sind, wenn Admins und Programmierer nicht zusammenarbeiten.
</p>

<h3>Everybody needs a 304</h3>
<p>
(oder: Wie ich dem Browser des Users helfe, optimal zu cachen)
</p>

<p>
Bisher bin ich lediglich auf ein Ziel von Performanceoptimierung eingegangen &#8211; zu verhindern, dass die eigenen Server zusammenbrechen, wennâ€™s mal brenzlig wird. Man muss sich aber unbedingt bewusst machen, dass Optimierungen auf dem Server erstmal keinen Wert an sich darstellen, sondern nur dem eigentlichen Ziel dienen: dem User die Benutzung der eigenen Seite so schnell und angenehm wie mÃ¶glich zu machen &#8211; indem die Seite grundsÃ¤tzlich erreichbar bleibt, und indem die Seite sich so schnell wie mÃ¶glich aufbaut.
</p>

<p>
Wenn man sich das erstmal bewusst gemacht hat, ist auch klar dass es sich sogar lohnen kann, etwas Rechenzeit auf dem Server zu investieren, um sie dem Client (also Browser) abzunehmen.
</p>

<p>
Aber der Reihe nach. Es gibt ein wichtiges Hilfsmittel, um den Aufbau einer Webseite im Browser deutlich zu beschleunigen (abgesehen von den Ã¼blichen Massnahmen wie geringer DateigrÃ¶ÃŸe, mÃ¶glichst wenig eingebetteten Objekten etc.), und das ist die Verwendung des HTTP Status <em>304 Not Modified</em>. Diesen kann der Server senden, wenn er anhand der Anfrage des Clients erkennt, dass exakt der Inhalt, den der Browser bereits in seinem Cache hat, nochmal Ã¼ber die Leitung wandern wÃ¼rde &#8211; in diesem Fall sendet der Server diesen Inhalt dann eben nicht nochmal, sondern teil dem Browser nur mit, er mÃ¶ge auf den Inhalt seines Caches zurÃ¼ckgreifen.
</p>

<p>
Dies kann zu erheblichen Performancesteigerungen auf Seiten des Clients fÃ¼hren, denn die Zeit die zum Download des Inhalts einer Seite benÃ¶tigt wird, entfÃ¤llt.
</p>

<p>
Es gibt nun zwei Faktoren, die das Status 304 Handling beeinflussen und spezielle Anpassungen erfordern, um optimales Clientcaching zu ermÃ¶glichen: Die Auslieferung von Seiten Ã¼ber PHP Skripte (gilt prinzipiell auch fÃ¼r andere Skriptsprachen) und der Betrieb einer Plattform in einem Webserver-Cluster.
</p>

<p>
Zuerst zu letzterem: Um in der gegenseitigen Kommunikation festzustellen, ob ein Inhalt vom Server neu ausgeliefert werden muss oder der Browser den Inhalt aus dem eigenen Cache lÃ¤dt, gibt es den sogenannten <em>Etag</em>. Ein ganz kurzer Abriss, wie die Verwendung ablÃ¤uft. Der Client fragt eine Ressource beim Server an. Es ist der erste Zugriff innerhalb dieser Browsersitzung, deshalb schickt der Client kein Etag mit. Der Server sendet daraufhin die Inhalte aus, und schickt in den Headern den Etag des aktuellen Inhalts dieser Ressource mit, sagen wir â€œ12345â€³ (der Server schickt dazu den Header <em>Etag: â€œ12345â€³</em>).
</p>

<p>
Fragt der Client nun erneut dieselbe Ressource beim Server an, schickt er in seinen Headern wiederum die Information mit, dass er in seinem Cache bereits die Inhalte mit dem ETag â€œ12345â€³ gespeichert hat, und der Server ihn informieren mÃ¶ge falls sich die Inhalte nicht geÃ¤ndert haben (der Client schickt dazu den Header <em>If-None-Match: â€œ12345â€³</em>). Der Server kann dann schauen, ob die Inhalte die er ausliefern wÃ¼rde immer noch das ETag â€œ12345â€³ haben, und in diesem Fall den erwÃ¤hnten HTTP Status 304 senden, oder, falls Inhalt und ETag nicht mehr zueinander passen, den neuen Inhalt schicken.
</p>

<p>
Die Frage ist nun: Wie genau ist denn definiert, was im Etag steht? Nun, im Prinzip gar nicht. Es gibt kein vorgeschriebenes Format, wichtig ist nur die Definition des Etag an sich: dass nÃ¤mlich ein eindeutiges Etag zu einem eindeutigen Inhalt einer bestimmten Ressource gehÃ¶rt, und deshalb festgestellt werden kann ob sich der Inhalt einer Ressource zwischen zwei Requests geÃ¤ndert hat oder nicht. Man kann sich den Etag deshalb der Einfachheit halber als Checksumme des Inhalts vorstellen (und in der Tat besteht eine MÃ¶glichkeit den Etag zu generieren darin, z.B. die MD5 Summe des Inhalts zu berechnen).
</p>

<p>
Woher kommt der Etag? Beim Apache ist es Teil der KernfunktionalitÃ¤t, fÃ¼r eine angeforderte Ressource den Etag zu berechnen und mitzusenden, sowie entsprechend zu reagieren wenn ein Client den <em>If-None-Match</em> Header sendet. Alles out-of-the-box also, aber genau hier liegen fÃ¼r uns die Probleme:
</p>

<p>
<strong>Problem 1:</strong> DefaultmÃ¤ssig berechnet Apache den Etag fÃ¼r eine Ressource, indem eine Art Quersumme aus diesen Informationen generiert wird: I-Node-Nummer der angefragten Datei, letzter Ã„nderungszeitpunkt (mtime) der angefragten Datei, und GrÃ¶ÃŸe der angefragten Datei. Betreibt man eine Webseite auf nur einem Server, hat man kein Problem, denn wenn z.B. die Datei <em>/index.html</em> zwischen zwei Aufrufen nicht verÃ¤ndert wird, hat sie bei beiden Zugriffen denselben Etag, da keiner der drei Faktoren inode, mtime, size zwischenzeitlich verÃ¤ndert wurde.
</p>

<p>
Betreibt man aber einen Cluster aus mehreren Webservern, und besteht die MÃ¶glichkeit, dass ein Client bei zwei aufeinanderfolgenden Aufrufen derselben Ressource zuerst auf einem Webserver, beim zweiten Aufruf aber auf einem anderen landet, dann ist, auch wenn auf beiden Servern die exakt gleiche Datei liegt, der Etag beide Male ein anderer, denn selbst wenn letzter Ã„nderungszeitpunkt und GrÃ¶ÃŸe der Datei auf beiden Servern identisch sind: dass die I-Node-Nummer die gleiche ist, ist praktisch ausgeschlossen. Der Server wird also keine 304 Status senden, obwohl er es kÃ¶nnte.
</p>

<p>
Abhilfe ist zum GlÃ¼ck sehr einfach mÃ¶glich, und lohnt sich schon beim Wechseln von einem auf zwei Server: Man muss dem Apache mitteilen, dass er die I-Node-Nummer nicht mehr zur Berechnung des Etag heranziehen soll. Dies erledigt an zentraler Stelle die Anweisung <em>FileETag MTime Size</em>. Mehr dazu im <a href="http://httpd.apache.org/docs/2.2/mod/core.html#fileetag">Apache Handbuch</a>.
</p>

<p>
<strong>Problem 2:</strong> <em>mod_php</em> hebelt die Verwendung von Etag fÃ¼r PHP Skripte aus. Das macht ja prinzipiell auch Sinn: selbst wenn die Skriptdatei <em>/index.php</em> sich zwischen zwei Aufrufen inhaltlich Ã¼berhaupt nicht geÃ¤ndert hat, kann sie dennoch vÃ¶llig unterschiedliche Inhalte an den Client ausliefern &#8211; genau das ist ja Sinn und Zweck des Einsatzes von dynamischen Seiten.
</p>

<p>
Trotzdem kann es Sinn machen, dass der Server den Status 304 an einen Client sendet, wenn dieser dieselbe Ressource erneut anfragt. Zum Beispiel bei CSS Skripten, die von jeder Seite der Plattform eingebunden werden, und aus programmiertechnischen ErwÃ¤gungen als PHP Skripte realisiert sind, aber deren Inhalt sich trotzdem sehr selten Ã¤ndert. Jeder Seitenaufruf wÃ¼rde den Browser veranlassen, dies referenzierte CSS Datei anzufragen, und der Server wÃ¼rde jedes Mal den Inhalt senden, obwohl sich dieser seit dem letzten Aufruf nicht geÃ¤ndert hat. Das macht den Seitenaufbau im Client langsam, und ist zudem eine Ressourcenverschwendung.
</p>

<p>
Wie kann man nun sicherstellen, dass ein Client den 304 Status auch beim Abruf von PHP Skripten erhÃ¤lt, falls sich der Inhalt nicht verÃ¤ndert hat, aber auch auf keinen Fall einen 304 Status bekommt, falls der Inhalt sich geÃ¤ndert hat? Die LÃ¶sung ist leider nicht ganz so trivial wie beim ersten Problem, aber doch vergleichsweise einfach zu realisieren.
</p>

<p>
Da wie erwÃ¤hnt der Zustand der Skriptdatei selbst praktisch keine Rolle spielt, darf man nur mit dem von diesem Skript auszuliefernden Inhalt arbeiten. Eine LÃ¶sung wÃ¤re, bei den Skripten, fÃ¼r die man den Etag Mechanismus einsetzen mÃ¶chte, folgenden Code ans Ende anzuhÃ¤ngen (lÃ¤sst sich natÃ¼rlich einfach in eine zentrale Funktion kapseln):

<code>&lt;?php
$output = ob_get_clean(); // Gesamte Ausgabe, die an den Client gesendet werden soll, abfangen und zwischenspeichern

$etag = â€˜â€â€˜.sha1($output).â€™â€â€˜; // PrÃ¼fsumme der Ausgabe berechnen// Ist der Inhalt identisch mit dem, den der Client gecached hat?

if ($_SERVER['HTTP_IF_NONE_MATCH'] == $etag) // Wenn ja, dann sende nur den Status 304
{
    header(â€˜HTTP/1.x 304 Not Modifiedâ€™);
    header(â€˜Etag: â€˜.$etag);
    die();
}
else // Wenn nicht, dann sende den Inhalt inkl. des neuen Etags
{
    header(â€˜Etag: â€˜.$etag);
    echo $output;
    die();
}
?&gt;
</code>


</p>

<p>
Voraussetzung ist dafÃ¼r die Verwendung von <em>output buffering</em>.
</p>

<p>
Eines muss man ganz klar festhalten &#8211; fÃ¼r den Server fÃ¤llt exakt dieselbe Arbeit an, egal ob der User den Inhalt schlussendlich zugesendet bekommt oder nur die lapidare Meldung, er mÃ¶ge doch auf seinen Cache zurÃ¼ckgreifen. Es mag nach deutlich zuviel Overhead aussehen, PHP soviel Arbeit erledigen zu lassen, nur um das Ergebnis dieser Arbeit dann wegzuschmeissen; aber der Effekt auf die Lade- und damit Seitenaufbauzeiten beim Client ist wirklich beeindruckend, wenn man diesen Mechanismus geschickt einsetzt.
</p>]]></content>
		<link rel="replies" type="text/html" href="/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/#comments" thr:count="0"/>
		<link rel="replies" type="application/atom+xml" href="/2007/07/17/recycelter-artikel-my-hammer-das-fernsehen-und-die-serverlast/feed/atom/" thr:count="0"/>
		<thr:total>0</thr:total>
	</entry>
	</feed>
